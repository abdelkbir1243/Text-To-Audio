{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-zpjqjPUXpN",
        "outputId": "04ca4138-71fe-4fe3-851d-a83627c1c8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-text-to-audio'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 146 (delta 17), reused 146 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.42 MiB | 25.07 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "/content/torch-text-to-audio\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RChoukri03/torch-text-to-audio.git\n",
        "%cd torch-text-to-audio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# ID du fichier Google Drive\n",
        "file_id = \"1SbOCKLnebpHt8iW21LX7aWyGz5UxBK0X\"\n",
        "output = \"data.zip\"\n",
        "\n",
        "# Téléchargement\n",
        "gdown.download(f\"https://drive.google.com/uc?id=1SbOCKLnebpHt8iW21LX7aWyGz5UxBK0X\", \"data.zip\", quiet=False)\n",
        "\n",
        "# Décompression si c'est un zip\n",
        "!unzip -q data.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4con257sYxm",
        "outputId": "750b5113-ce11-4f64-8cc6-0ca91348376c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SbOCKLnebpHt8iW21LX7aWyGz5UxBK0X\n",
            "From (redirected): https://drive.google.com/uc?id=1SbOCKLnebpHt8iW21LX7aWyGz5UxBK0X&confirm=t&uuid=591d6f82-0ab7-4d88-9b6a-6fb4acb42ec4\n",
            "To: /content/torch-text-to-audio/data.zip\n",
            "100%|██████████| 2.37G/2.37G [00:40<00:00, 57.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1arcYIkUsXu",
        "outputId": "25e2aba5-15da-4521-e67c-28f63c262157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-25 23:55:03.393653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742946903.665578    1701 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742946903.740170    1701 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 23:55:03.799244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Created checkpoint_dir folder: checkpoints/exp_tc2_adv\n",
            "Caching datasets ...\n",
            "[======                        ] 20.8%invalid phonemes at line 376: \"ARA NORM  0378.wav\" \"w a l m u0 H aa f A Z A t U0 + E a l aa + l q A d r i0 + l k aa f ii0 + m i0 n a + nn a w m i0 .\"\n",
            "[==============================] 100.0%\n",
            "[==============================] 100.0%\n",
            "Epoch: 0\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.11937934160232544, grad_norm: 0.10763970762491226\n",
            "Training:   0% 1/226 [00:13<49:31, 13.21s/it]loss: 0.20451410114765167, grad_norm: 0.6510514616966248\n",
            "Training:   1% 2/226 [00:17<30:06,  8.06s/it]loss: 0.198780357837677, grad_norm: 0.3240123689174652\n",
            "Training:   1% 3/226 [00:20<20:57,  5.64s/it]loss: 0.14198218286037445, grad_norm: 0.4457414448261261\n",
            "Training:   2% 4/226 [00:27<23:23,  6.32s/it]loss: 0.1899583786725998, grad_norm: 0.4543125033378601\n",
            "Training:   2% 5/226 [00:32<20:43,  5.63s/it]loss: 0.19884057343006134, grad_norm: 0.3992846608161926\n",
            "Training:   3% 6/226 [00:36<19:04,  5.20s/it]loss: 0.18304572999477386, grad_norm: 0.2435823231935501\n",
            "Training:   3% 7/226 [00:41<18:12,  4.99s/it]loss: 0.2420758306980133, grad_norm: 0.3111170828342438\n",
            "Training:   4% 8/226 [00:46<18:43,  5.15s/it]loss: 0.20302625000476837, grad_norm: 0.3319247364997864\n",
            "Training:   4% 9/226 [00:50<17:13,  4.76s/it]loss: 0.20479847490787506, grad_norm: 0.3269510269165039\n",
            "Training:   4% 10/226 [00:53<14:47,  4.11s/it]loss: 0.1709308624267578, grad_norm: 0.2451525777578354\n",
            "Training:   5% 11/226 [00:58<16:27,  4.59s/it]loss: 0.19987107813358307, grad_norm: 0.22766925394535065\n",
            "Training:   5% 12/226 [01:02<15:42,  4.40s/it]loss: 0.2725304067134857, grad_norm: 0.29069602489471436\n",
            "Training:   6% 13/226 [01:05<13:26,  3.78s/it]loss: 0.19685478508472443, grad_norm: 0.2205408215522766\n",
            "Training:   6% 14/226 [01:08<13:23,  3.79s/it]loss: 0.12462373822927475, grad_norm: 0.2330026924610138\n",
            "Training:   7% 15/226 [01:19<20:04,  5.71s/it]loss: 0.17529048025608063, grad_norm: 0.2266339659690857\n",
            "Training:   7% 16/226 [01:22<17:57,  5.13s/it]loss: 0.1524437516927719, grad_norm: 0.1922386735677719\n",
            "Training:   8% 17/226 [01:28<18:16,  5.25s/it]loss: 0.23872767388820648, grad_norm: 0.2616213858127594\n",
            "Training:   8% 18/226 [01:31<15:44,  4.54s/it]loss: 0.21906492114067078, grad_norm: 0.23404383659362793\n",
            "Training:   8% 19/226 [01:35<15:11,  4.41s/it]loss: 0.19733040034770966, grad_norm: 0.23652233183383942\n",
            "Training:   9% 20/226 [01:39<14:55,  4.35s/it]loss: 0.19549813866615295, grad_norm: 0.21335376799106598\n",
            "Training:   9% 21/226 [01:45<16:43,  4.89s/it]loss: 0.14801329374313354, grad_norm: 0.20822188258171082\n",
            "Training:  10% 22/226 [01:50<16:54,  4.98s/it]loss: 0.2532130777835846, grad_norm: 0.23663997650146484\n",
            "Training:  10% 23/226 [01:53<14:01,  4.14s/it]loss: 0.12561237812042236, grad_norm: 0.14879772067070007\n",
            "Training:  11% 24/226 [02:02<19:27,  5.78s/it]loss: 0.20686471462249756, grad_norm: 0.19477267563343048\n",
            "Training:  11% 25/226 [02:07<18:45,  5.60s/it]loss: 0.17156286537647247, grad_norm: 0.16098150610923767\n",
            "Training:  12% 26/226 [02:17<22:28,  6.74s/it]loss: 0.239315465092659, grad_norm: 0.23216769099235535\n",
            "Training:  12% 27/226 [02:19<18:14,  5.50s/it]loss: 0.15132442116737366, grad_norm: 0.177363321185112\n",
            "Training:  12% 28/226 [02:25<18:36,  5.64s/it]loss: 0.1599484086036682, grad_norm: 0.17960037291049957\n",
            "Training:  13% 29/226 [02:30<17:04,  5.20s/it]loss: 0.17618408799171448, grad_norm: 0.19025059044361115\n",
            "Training:  13% 30/226 [02:34<16:37,  5.09s/it]loss: 0.14397865533828735, grad_norm: 0.1729966551065445\n",
            "Training:  14% 31/226 [02:41<17:41,  5.44s/it]loss: 0.16661810874938965, grad_norm: 0.16199888288974762\n",
            "Training:  14% 32/226 [02:45<16:45,  5.19s/it]loss: 0.2830457091331482, grad_norm: 0.2524871528148651\n",
            "Training:  15% 33/226 [02:48<14:42,  4.57s/it]loss: 0.19327695667743683, grad_norm: 0.16995196044445038\n",
            "Training:  15% 34/226 [02:52<13:59,  4.37s/it]loss: 0.24342752993106842, grad_norm: 0.22140508890151978\n",
            "Training:  15% 35/226 [02:55<12:05,  3.80s/it]loss: 0.18250581622123718, grad_norm: 0.20192961394786835\n",
            "Training:  16% 36/226 [02:58<11:03,  3.49s/it]loss: 0.16240417957305908, grad_norm: 0.18322977423667908\n",
            "Training:  16% 37/226 [03:02<12:01,  3.82s/it]loss: 0.16872458159923553, grad_norm: 0.20457346737384796\n",
            "Training:  17% 38/226 [03:07<12:43,  4.06s/it]loss: 0.1644730120897293, grad_norm: 0.1387764811515808\n",
            "Training:  17% 39/226 [03:11<13:13,  4.24s/it]loss: 0.27321097254753113, grad_norm: 0.2807943820953369\n",
            "Training:  18% 40/226 [03:14<11:11,  3.61s/it]loss: 0.1849449723958969, grad_norm: 0.1671496033668518\n",
            "Training:  18% 41/226 [03:19<12:55,  4.19s/it]loss: 0.1894698441028595, grad_norm: 0.20119057595729828\n",
            "Training:  19% 42/226 [03:23<12:42,  4.15s/it]loss: 0.2210560142993927, grad_norm: 0.1874391883611679\n",
            "Training:  19% 43/226 [03:28<12:57,  4.25s/it]loss: 0.1879173219203949, grad_norm: 0.16245007514953613\n",
            "Training:  19% 44/226 [03:32<12:30,  4.12s/it]loss: 0.1531989872455597, grad_norm: 0.16790279746055603\n",
            "Training:  20% 45/226 [03:38<14:52,  4.93s/it]loss: 0.1856866329908371, grad_norm: 0.17019203305244446\n",
            "Training:  20% 46/226 [03:42<13:46,  4.59s/it]loss: 0.12385622411966324, grad_norm: 0.1319912075996399\n",
            "Training:  21% 47/226 [03:49<15:26,  5.17s/it]loss: 0.18040788173675537, grad_norm: 0.19261181354522705\n",
            "Training:  21% 48/226 [03:52<13:39,  4.61s/it]loss: 0.19887763261795044, grad_norm: 0.1739531308412552\n",
            "Training:  22% 49/226 [03:56<13:24,  4.55s/it]loss: 0.20462878048419952, grad_norm: 0.1721537560224533\n",
            "Training:  22% 50/226 [04:01<13:36,  4.64s/it]loss: 0.1706686019897461, grad_norm: 0.1624125838279724\n",
            "Training:  23% 51/226 [04:07<14:15,  4.89s/it]loss: 0.19952501356601715, grad_norm: 0.1769171953201294\n",
            "Training:  23% 52/226 [04:12<14:50,  5.12s/it]loss: 0.1394031047821045, grad_norm: 0.1268758326768875\n",
            "Training:  23% 53/226 [04:22<18:30,  6.42s/it]loss: 0.17430496215820312, grad_norm: 0.18544220924377441\n",
            "Training:  24% 54/226 [04:26<16:32,  5.77s/it]loss: 0.15947195887565613, grad_norm: 0.16469861567020416\n",
            "Training:  24% 55/226 [04:30<15:08,  5.31s/it]loss: 0.15775245428085327, grad_norm: 0.14549988508224487\n",
            "Training:  25% 56/226 [04:36<15:23,  5.43s/it]loss: 0.14281484484672546, grad_norm: 0.13447853922843933\n",
            "Training:  25% 57/226 [04:43<16:26,  5.84s/it]loss: 0.1544991284608841, grad_norm: 0.13020049035549164\n",
            "Training:  26% 58/226 [04:49<16:34,  5.92s/it]loss: 0.15475142002105713, grad_norm: 0.13125082850456238\n",
            "Training:  26% 59/226 [04:54<15:38,  5.62s/it]loss: 0.27908268570899963, grad_norm: 0.2831524610519409\n",
            "Training:  27% 60/226 [04:56<12:21,  4.47s/it]loss: 0.18451686203479767, grad_norm: 0.2840624749660492\n",
            "Training:  27% 61/226 [05:04<15:27,  5.62s/it]loss: 0.14655067026615143, grad_norm: 0.16491417586803436\n",
            "Training:  27% 62/226 [05:09<14:41,  5.38s/it]loss: 0.20074667036533356, grad_norm: 0.18483984470367432\n",
            "Training:  28% 63/226 [05:12<12:59,  4.78s/it]loss: 0.14474603533744812, grad_norm: 0.12477289140224457\n",
            "Training:  28% 64/226 [05:20<15:39,  5.80s/it]loss: 0.18664707243442535, grad_norm: 0.16537494957447052\n",
            "Training:  29% 65/226 [05:24<13:40,  5.10s/it]loss: 0.19003607332706451, grad_norm: 0.16462476551532745\n",
            "Training:  29% 66/226 [05:27<12:27,  4.67s/it]loss: 0.19928579032421112, grad_norm: 0.15086644887924194\n",
            "Training:  30% 67/226 [05:31<11:36,  4.38s/it]loss: 0.31475159525871277, grad_norm: 0.35826942324638367\n",
            "Training:  30% 68/226 [05:33<09:23,  3.56s/it]loss: 0.2051917314529419, grad_norm: 0.20534195005893707\n",
            "Training:  31% 69/226 [05:36<08:50,  3.38s/it]loss: 0.1300075501203537, grad_norm: 0.14313584566116333\n",
            "Training:  31% 70/226 [05:44<12:34,  4.84s/it]loss: 0.18895043432712555, grad_norm: 0.18812991678714752\n",
            "Training:  31% 71/226 [05:48<12:08,  4.70s/it]loss: 0.19498802721500397, grad_norm: 0.17361746728420258\n",
            "Training:  32% 72/226 [05:53<11:48,  4.60s/it]loss: 0.22026927769184113, grad_norm: 0.18616928160190582\n",
            "Training:  32% 73/226 [05:56<10:48,  4.24s/it]loss: 0.16473828256130219, grad_norm: 0.17221300303936005\n",
            "Training:  33% 74/226 [06:01<11:24,  4.50s/it]loss: 0.28047046065330505, grad_norm: 0.20046834647655487\n",
            "Training:  33% 75/226 [06:05<11:07,  4.42s/it]loss: 0.16501107811927795, grad_norm: 0.17180661857128143\n",
            "Training:  34% 76/226 [06:12<12:18,  4.93s/it]loss: 0.25408613681793213, grad_norm: 0.26102498173713684\n",
            "Training:  34% 77/226 [06:14<10:10,  4.09s/it]loss: 0.16822241246700287, grad_norm: 0.14867018163204193\n",
            "Training:  35% 78/226 [06:20<11:37,  4.71s/it]loss: 0.165033757686615, grad_norm: 0.1798878014087677\n",
            "Training:  35% 79/226 [06:25<11:47,  4.81s/it]loss: 0.1717618703842163, grad_norm: 1.1530482769012451\n",
            "Training:  35% 80/226 [06:29<11:31,  4.74s/it]loss: 0.17432153224945068, grad_norm: 0.16021683812141418\n",
            "Training:  36% 81/226 [06:35<12:00,  4.97s/it]loss: 0.17706018686294556, grad_norm: 0.23218989372253418\n",
            "Training:  36% 82/226 [06:39<11:30,  4.80s/it]loss: 0.1331515908241272, grad_norm: 0.1464904099702835\n",
            "Training:  37% 83/226 [06:46<12:54,  5.42s/it]loss: 0.20205841958522797, grad_norm: 0.3119526207447052\n",
            "Training:  37% 84/226 [06:49<11:04,  4.68s/it]loss: 0.2728368639945984, grad_norm: 0.3198871314525604\n",
            "Training:  38% 85/226 [06:52<09:39,  4.11s/it]loss: 0.2590944468975067, grad_norm: 0.2751203775405884\n",
            "Training:  38% 86/226 [06:54<08:27,  3.63s/it]loss: 0.16785548627376556, grad_norm: 0.2595375180244446\n",
            "Training:  38% 87/226 [06:59<09:15,  4.00s/it]loss: 0.16961684823036194, grad_norm: 0.25690528750419617\n",
            "Training:  39% 88/226 [07:04<09:48,  4.26s/it]loss: 0.2787691354751587, grad_norm: 0.2760118544101715\n",
            "Training:  39% 89/226 [07:06<08:08,  3.57s/it]loss: 0.19407813251018524, grad_norm: 0.2002059519290924\n",
            "Training:  40% 90/226 [07:11<08:43,  3.85s/it]loss: 0.2195928692817688, grad_norm: 0.3052496612071991\n",
            "Training:  40% 91/226 [07:15<08:52,  3.94s/it]loss: 0.17484958469867706, grad_norm: 0.22257259488105774\n",
            "Training:  41% 92/226 [07:24<12:04,  5.41s/it]loss: 0.1775760054588318, grad_norm: 0.192942813038826\n",
            "Training:  41% 93/226 [07:28<11:13,  5.06s/it]loss: 0.1575227826833725, grad_norm: 0.2331146001815796\n",
            "Training:  42% 94/226 [07:33<11:19,  5.15s/it]loss: 0.11332159489393234, grad_norm: 0.18415570259094238\n",
            "Training:  42% 95/226 [07:41<12:36,  5.78s/it]loss: 0.21949437260627747, grad_norm: 0.211459681391716\n",
            "Training:  42% 96/226 [07:44<10:45,  4.96s/it]loss: 0.17747697234153748, grad_norm: 0.21988441050052643\n",
            "Training:  43% 97/226 [07:47<09:58,  4.64s/it]loss: 0.2696026861667633, grad_norm: 0.31759414076805115\n",
            "Training:  43% 98/226 [07:50<08:36,  4.03s/it]loss: 0.13870973885059357, grad_norm: 0.18193946778774261\n",
            "Training:  44% 99/226 [07:57<10:10,  4.81s/it]loss: 0.189030259847641, grad_norm: 0.1688070446252823\n",
            "Training:  44% 100/226 [08:01<09:34,  4.56s/it]loss: 0.23726801574230194, grad_norm: 0.2583237290382385\n",
            "Training:  45% 101/226 [08:05<09:14,  4.43s/it]loss: 0.19171668589115143, grad_norm: 0.21185055375099182\n",
            "Training:  45% 102/226 [08:10<09:33,  4.62s/it]loss: 0.16007845103740692, grad_norm: 0.1802409142255783\n",
            "Training:  46% 103/226 [08:14<09:07,  4.45s/it]loss: 0.21713201701641083, grad_norm: 0.20016677677631378\n",
            "Training:  46% 104/226 [08:17<08:20,  4.11s/it]loss: 0.21055473387241364, grad_norm: 0.18111568689346313\n",
            "Training:  46% 105/226 [08:20<07:39,  3.80s/it]loss: 0.24642011523246765, grad_norm: 0.21615497767925262\n",
            "Training:  47% 106/226 [08:23<06:43,  3.36s/it]loss: 0.21020418405532837, grad_norm: 0.18701593577861786\n",
            "Training:  47% 107/226 [08:27<07:21,  3.71s/it]loss: 0.1633765697479248, grad_norm: 0.1925530880689621\n",
            "Training:  48% 108/226 [08:34<08:57,  4.55s/it]loss: 0.18590770661830902, grad_norm: 0.24415551126003265\n",
            "Training:  48% 109/226 [08:37<08:17,  4.25s/it]loss: 0.15435561537742615, grad_norm: 0.13394714891910553\n",
            "Training:  49% 110/226 [08:44<09:50,  5.09s/it]loss: 0.13293370604515076, grad_norm: 0.15352240204811096\n",
            "Training:  49% 111/226 [08:52<11:05,  5.79s/it]loss: 0.09744825959205627, grad_norm: 0.14272911846637726\n",
            "Training:  50% 112/226 [09:01<13:03,  6.87s/it]loss: 0.2051009237766266, grad_norm: 0.32787683606147766\n",
            "Training:  50% 113/226 [09:05<11:01,  5.86s/it]loss: 0.2073720246553421, grad_norm: 0.18321284651756287\n",
            "Training:  50% 114/226 [09:09<09:52,  5.29s/it]loss: 0.20763666927814484, grad_norm: 0.29279839992523193\n",
            "Training:  51% 115/226 [09:11<08:28,  4.58s/it]loss: 0.15179100632667542, grad_norm: 0.27460426092147827\n",
            "Training:  51% 116/226 [09:16<08:26,  4.61s/it]loss: 0.20641428232192993, grad_norm: 2.642305850982666\n",
            "Training:  52% 117/226 [09:19<07:33,  4.16s/it]loss: 0.19994959235191345, grad_norm: 0.18157535791397095\n",
            "Training:  52% 118/226 [09:24<07:41,  4.27s/it]loss: 0.1770029515028, grad_norm: 0.2461794912815094\n",
            "Training:  53% 119/226 [09:29<08:06,  4.55s/it]loss: 0.2137889862060547, grad_norm: 0.23578470945358276\n",
            "Training:  53% 120/226 [09:33<07:49,  4.43s/it]loss: 0.1822170466184616, grad_norm: 0.1713448166847229\n",
            "Training:  54% 121/226 [09:41<09:26,  5.40s/it]loss: 0.20276713371276855, grad_norm: 0.2127918004989624\n",
            "Training:  54% 122/226 [09:45<08:34,  4.95s/it]loss: 0.16253061592578888, grad_norm: 0.2065044343471527\n",
            "Training:  54% 123/226 [09:50<08:32,  4.98s/it]loss: 0.17317692935466766, grad_norm: 0.18010446429252625\n",
            "Training:  55% 124/226 [09:56<08:56,  5.26s/it]loss: 0.19566115736961365, grad_norm: 0.17415060102939606\n",
            "Training:  55% 125/226 [10:00<08:10,  4.85s/it]loss: 0.12320340424776077, grad_norm: 0.14379288256168365\n",
            "Training:  56% 126/226 [10:07<09:25,  5.66s/it]loss: 0.2107095569372177, grad_norm: 0.19512520730495453\n",
            "Training:  56% 127/226 [10:14<09:50,  5.96s/it]loss: 0.26294973492622375, grad_norm: 0.3452473282814026\n",
            "Training:  57% 128/226 [10:17<08:10,  5.00s/it]loss: 0.1669091284275055, grad_norm: 0.16332651674747467\n",
            "Training:  57% 129/226 [10:21<07:53,  4.88s/it]loss: 0.18959780037403107, grad_norm: 0.23328471183776855\n",
            "Training:  58% 130/226 [10:25<07:04,  4.43s/it]loss: 0.19828017055988312, grad_norm: 0.20865190029144287\n",
            "Training:  58% 131/226 [10:32<08:21,  5.28s/it]loss: 0.21215054392814636, grad_norm: 0.19608159363269806\n",
            "Training:  58% 132/226 [10:34<07:00,  4.47s/it]loss: 0.2508513331413269, grad_norm: 0.3439861834049225\n",
            "Training:  59% 133/226 [10:37<06:06,  3.94s/it]loss: 0.2400849610567093, grad_norm: 0.22748412191867828\n",
            "Training:  59% 134/226 [10:41<06:08,  4.01s/it]loss: 0.2506033480167389, grad_norm: 0.23137322068214417\n",
            "Training:  60% 135/226 [10:46<06:13,  4.10s/it]loss: 0.16624630987644196, grad_norm: 0.2843772768974304\n",
            "Training:  60% 136/226 [10:50<06:07,  4.08s/it]loss: 0.14220649003982544, grad_norm: 0.195760115981102\n",
            "Training:  61% 137/226 [10:56<06:53,  4.64s/it]loss: 0.18317362666130066, grad_norm: 0.22866512835025787\n",
            "Training:  61% 138/226 [11:00<06:34,  4.48s/it]loss: 0.1815725713968277, grad_norm: 0.21428832411766052\n",
            "Training:  62% 139/226 [11:04<06:28,  4.47s/it]loss: 0.21985775232315063, grad_norm: 0.22394303977489471\n",
            "Training:  62% 140/226 [11:07<05:32,  3.87s/it]loss: 0.18773411214351654, grad_norm: 0.1866239756345749\n",
            "Training:  62% 141/226 [11:12<06:04,  4.29s/it]loss: 0.11637606471776962, grad_norm: 0.18106557428836823\n",
            "Training:  63% 142/226 [11:19<07:24,  5.29s/it]loss: 0.1292535811662674, grad_norm: 0.17919088900089264\n",
            "Training:  63% 143/226 [11:25<07:22,  5.33s/it]loss: 0.1841651350259781, grad_norm: 0.2133420705795288\n",
            "Training:  64% 144/226 [11:31<07:33,  5.53s/it]loss: 0.18144482374191284, grad_norm: 0.19406501948833466\n",
            "Training:  64% 145/226 [11:36<07:17,  5.40s/it]loss: 0.3090425729751587, grad_norm: 4.739266395568848\n",
            "Training:  65% 146/226 [11:38<06:01,  4.52s/it]loss: 0.2446417361497879, grad_norm: 0.3203946650028229\n",
            "Training:  65% 147/226 [11:41<05:17,  4.02s/it]loss: 0.18014399707317352, grad_norm: 0.21950651705265045\n",
            "Training:  65% 148/226 [11:47<05:43,  4.40s/it]loss: 0.23135153949260712, grad_norm: 0.29419100284576416\n",
            "Training:  66% 149/226 [11:51<05:45,  4.48s/it]loss: 0.20379412174224854, grad_norm: 0.2692662179470062\n",
            "Training:  66% 150/226 [11:55<05:14,  4.14s/it]loss: 0.24010951817035675, grad_norm: 0.3331703841686249\n",
            "Training:  67% 151/226 [12:00<05:45,  4.61s/it]loss: 0.22218820452690125, grad_norm: 0.21850883960723877\n",
            "Training:  67% 152/226 [12:05<05:36,  4.55s/it]loss: 0.1215415745973587, grad_norm: 0.17485611140727997\n",
            "Training:  68% 153/226 [12:12<06:28,  5.32s/it]loss: 0.2635241746902466, grad_norm: 0.3058443069458008\n",
            "Training:  68% 154/226 [12:15<05:29,  4.58s/it]loss: 0.13228072226047516, grad_norm: 0.18972431123256683\n",
            "Training:  69% 155/226 [12:21<06:07,  5.17s/it]loss: 0.1669120043516159, grad_norm: 0.26183032989501953\n",
            "Training:  69% 156/226 [12:25<05:32,  4.74s/it]loss: 0.1362859010696411, grad_norm: 0.16902711987495422\n",
            "Training:  69% 157/226 [12:31<05:55,  5.16s/it]loss: 0.1788836419582367, grad_norm: 0.19219544529914856\n",
            "Training:  70% 158/226 [12:34<05:14,  4.63s/it]loss: 0.20915068686008453, grad_norm: 0.24809300899505615\n",
            "Training:  70% 159/226 [12:37<04:30,  4.04s/it]loss: 0.20766568183898926, grad_norm: 0.2821788787841797\n",
            "Training:  71% 160/226 [12:40<04:10,  3.79s/it]loss: 0.13322165608406067, grad_norm: 0.18902894854545593\n",
            "Training:  71% 161/226 [12:50<05:57,  5.50s/it]loss: 0.18985189497470856, grad_norm: 0.23507563769817352\n",
            "Training:  72% 162/226 [12:54<05:18,  4.98s/it]loss: 0.19044378399848938, grad_norm: 0.18518950045108795\n",
            "Training:  72% 163/226 [13:00<05:34,  5.31s/it]loss: 0.22312067449092865, grad_norm: 0.25204023718833923\n",
            "Training:  73% 164/226 [13:03<04:58,  4.81s/it]loss: 0.17773497104644775, grad_norm: 0.8378316760063171\n",
            "Training:  73% 165/226 [13:08<04:51,  4.78s/it]loss: 0.17604167759418488, grad_norm: 0.21568496525287628\n",
            "Training:  73% 166/226 [13:12<04:27,  4.45s/it]loss: 0.16412098705768585, grad_norm: 0.17179067432880402\n",
            "Training:  74% 167/226 [13:17<04:30,  4.58s/it]loss: 0.18971680104732513, grad_norm: 0.2204204946756363\n",
            "Training:  74% 168/226 [13:20<04:07,  4.27s/it]loss: 0.1999044567346573, grad_norm: 0.2450525313615799\n",
            "Training:  75% 169/226 [13:24<04:02,  4.25s/it]loss: 0.25291356444358826, grad_norm: 0.28726959228515625\n",
            "Training:  75% 170/226 [13:27<03:29,  3.74s/it]loss: 0.24832206964492798, grad_norm: 0.22023196518421173\n",
            "Training:  76% 171/226 [13:31<03:35,  3.92s/it]loss: 0.20693933963775635, grad_norm: 0.23857486248016357\n",
            "Training:  76% 172/226 [13:36<03:49,  4.25s/it]loss: 0.1641482710838318, grad_norm: 0.19921381771564484\n",
            "Training:  77% 173/226 [13:41<03:58,  4.50s/it]loss: 0.21218055486679077, grad_norm: 0.24882397055625916\n",
            "Training:  77% 174/226 [13:45<03:39,  4.21s/it]loss: 0.25711488723754883, grad_norm: 0.6090642809867859\n",
            "Training:  77% 175/226 [13:49<03:30,  4.13s/it]loss: 0.20290571451187134, grad_norm: 0.18298755586147308\n",
            "Training:  78% 176/226 [13:54<03:45,  4.52s/it]loss: 0.2593253552913666, grad_norm: 0.3211006224155426\n",
            "Training:  78% 177/226 [13:57<03:16,  4.01s/it]loss: 0.24034111201763153, grad_norm: 0.22935575246810913\n",
            "Training:  79% 178/226 [14:01<03:17,  4.11s/it]loss: 0.21043512225151062, grad_norm: 0.2316480129957199\n",
            "Training:  79% 179/226 [14:05<03:11,  4.08s/it]loss: 0.2589869797229767, grad_norm: 0.6973018050193787\n",
            "Training:  80% 180/226 [14:08<02:43,  3.55s/it]loss: 0.19246137142181396, grad_norm: 0.665921688079834\n",
            "Training:  80% 181/226 [14:12<02:47,  3.72s/it]loss: 0.1594044417142868, grad_norm: 0.1636146456003189\n",
            "Training:  81% 182/226 [14:18<03:16,  4.47s/it]loss: 0.22356879711151123, grad_norm: 0.25383076071739197\n",
            "Training:  81% 183/226 [14:22<03:02,  4.25s/it]loss: 0.19222527742385864, grad_norm: 0.19653907418251038\n",
            "Training:  81% 184/226 [14:29<03:33,  5.09s/it]loss: 0.22763210535049438, grad_norm: 0.20711855590343475\n",
            "Training:  82% 185/226 [14:34<03:30,  5.14s/it]loss: 0.22685113549232483, grad_norm: 0.2731362581253052\n",
            "Training:  82% 186/226 [14:37<03:03,  4.59s/it]loss: 0.20930233597755432, grad_norm: 0.2205037921667099\n",
            "Training:  83% 187/226 [14:42<02:53,  4.45s/it]loss: 0.24863973259925842, grad_norm: 0.24249032139778137\n",
            "Training:  83% 188/226 [14:46<02:48,  4.43s/it]loss: 0.23903167247772217, grad_norm: 0.3670042157173157\n",
            "Training:  84% 189/226 [14:48<02:20,  3.81s/it]loss: 0.18616506457328796, grad_norm: 0.20045872032642365\n",
            "Training:  84% 190/226 [14:54<02:39,  4.44s/it]loss: 0.2052128165960312, grad_norm: 0.2598850131034851\n",
            "Training:  85% 191/226 [14:59<02:39,  4.55s/it]loss: 0.1999131590127945, grad_norm: 0.22686663269996643\n",
            "Training:  85% 192/226 [15:02<02:16,  4.01s/it]loss: 0.1816282570362091, grad_norm: 0.1823306828737259\n",
            "Training:  85% 193/226 [15:08<02:30,  4.57s/it]loss: 0.29171988368034363, grad_norm: 0.6796669363975525\n",
            "Training:  86% 194/226 [15:11<02:11,  4.12s/it]loss: 0.3030906021595001, grad_norm: 0.38876545429229736\n",
            "Training:  86% 195/226 [15:13<01:47,  3.48s/it]loss: 0.2016451507806778, grad_norm: 0.2140660285949707\n",
            "Training:  87% 196/226 [15:18<01:57,  3.92s/it]loss: 0.20280387997627258, grad_norm: 0.2660219669342041\n",
            "Training:  87% 197/226 [15:23<02:05,  4.32s/it]loss: 0.22774551808834076, grad_norm: 0.2599993348121643\n",
            "Training:  88% 198/226 [15:26<01:51,  3.99s/it]loss: 0.15403032302856445, grad_norm: 0.22731071710586548\n",
            "Training:  88% 199/226 [15:32<02:02,  4.53s/it]loss: 0.13978441059589386, grad_norm: 0.2104780226945877\n",
            "Training:  88% 200/226 [15:37<02:01,  4.68s/it]loss: 0.20624780654907227, grad_norm: 0.27164873480796814\n",
            "Training:  89% 201/226 [15:41<01:51,  4.45s/it]loss: 0.1907808631658554, grad_norm: 0.19291286170482635\n",
            "Training:  89% 202/226 [15:45<01:43,  4.33s/it]loss: 0.13684046268463135, grad_norm: 0.18858839571475983\n",
            "Training:  90% 203/226 [15:52<01:56,  5.09s/it]loss: 0.12218942493200302, grad_norm: 0.17117483913898468\n",
            "Training:  90% 204/226 [15:58<01:59,  5.43s/it]loss: 0.19896183907985687, grad_norm: 0.26915860176086426\n",
            "Training:  91% 205/226 [16:01<01:39,  4.73s/it]loss: 0.173943892121315, grad_norm: 0.18527615070343018\n",
            "Training:  91% 206/226 [16:06<01:34,  4.71s/it]loss: 0.17921631038188934, grad_norm: 0.32275426387786865\n",
            "Training:  92% 207/226 [16:09<01:22,  4.32s/it]loss: 0.23358161747455597, grad_norm: 0.2586112916469574\n",
            "Training:  92% 208/226 [16:12<01:12,  4.00s/it]loss: 0.18320834636688232, grad_norm: 0.1928950548171997\n",
            "Training:  92% 209/226 [16:18<01:13,  4.34s/it]loss: 0.2485213726758957, grad_norm: 0.2513740062713623\n",
            "Training:  93% 210/226 [16:20<01:01,  3.83s/it]loss: 0.1665487140417099, grad_norm: 0.24003002047538757\n",
            "Training:  93% 211/226 [16:25<01:00,  4.05s/it]loss: 0.25183936953544617, grad_norm: 0.21888870000839233\n",
            "Training:  94% 212/226 [16:28<00:51,  3.67s/it]loss: 0.2167433351278305, grad_norm: 0.23717506229877472\n",
            "Training:  94% 213/226 [16:33<00:55,  4.26s/it]loss: 0.2910989820957184, grad_norm: 0.31567662954330444\n",
            "Training:  95% 214/226 [16:35<00:43,  3.63s/it]loss: 0.2588048279285431, grad_norm: 0.2540460526943207\n",
            "Training:  95% 215/226 [16:38<00:37,  3.42s/it]loss: 0.2006041258573532, grad_norm: 0.21205872297286987\n",
            "Training:  96% 216/226 [16:42<00:36,  3.66s/it]loss: 0.21836437284946442, grad_norm: 0.21348534524440765\n",
            "Training:  96% 217/226 [16:46<00:33,  3.70s/it]loss: 0.15742278099060059, grad_norm: 0.1512778252363205\n",
            "Training:  96% 218/226 [16:54<00:40,  5.05s/it]loss: 0.1510535329580307, grad_norm: 0.15398314595222473\n",
            "Training:  97% 219/226 [17:01<00:38,  5.54s/it]loss: 0.19711317121982574, grad_norm: 0.15712890028953552\n",
            "Training:  97% 220/226 [17:10<00:38,  6.45s/it]loss: 0.17285548150539398, grad_norm: 0.14113076031208038\n",
            "Training:  98% 221/226 [17:16<00:32,  6.42s/it]loss: 0.2254999279975891, grad_norm: 0.1894182711839676\n",
            "Training:  98% 222/226 [17:22<00:24,  6.15s/it]loss: 0.18624110519886017, grad_norm: 0.16944639384746552\n",
            "Training:  99% 223/226 [17:25<00:16,  5.40s/it]loss: 0.22389179468154907, grad_norm: 0.25410735607147217\n",
            "Training:  99% 224/226 [17:28<00:09,  4.61s/it]loss: 0.1691609025001526, grad_norm: 0.16127042472362518\n",
            "Training: 100% 225/226 [17:33<00:04,  4.58s/it]loss: 0.18474428355693817, grad_norm: 0.15709036588668823\n",
            "Training: 100% 226/226 [17:36<00:00,  4.68s/it]\n",
            "Validation loss: 0.6745042324066162\n",
            "Epoch: 1\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.21522951126098633, grad_norm: 0.1987830102443695\n",
            "Training:   0% 1/226 [00:05<19:29,  5.20s/it]loss: 0.2028026580810547, grad_norm: 0.20954936742782593\n",
            "Training:   1% 2/226 [00:08<15:19,  4.10s/it]loss: 0.18494385480880737, grad_norm: 0.1958443969488144\n",
            "Training:   1% 3/226 [00:12<14:55,  4.01s/it]loss: 0.23941285908222198, grad_norm: 0.24765999615192413\n",
            "Training:   2% 4/226 [00:15<13:09,  3.55s/it]loss: 0.1478457897901535, grad_norm: 0.2615376114845276\n",
            "Training:   2% 5/226 [00:21<16:24,  4.45s/it]loss: 0.23285415768623352, grad_norm: 0.16596266627311707\n",
            "Training:   3% 6/226 [00:25<15:58,  4.36s/it]loss: 0.16285362839698792, grad_norm: 0.1537664830684662\n",
            "Training:   3% 7/226 [00:29<15:25,  4.23s/it]loss: 0.18492087721824646, grad_norm: 0.20177437365055084\n",
            "Training:   4% 8/226 [00:32<14:31,  4.00s/it]loss: 0.1762492060661316, grad_norm: 0.15863332152366638\n",
            "Training:   4% 9/226 [00:39<17:21,  4.80s/it]loss: 0.21147747337818146, grad_norm: 0.1817156970500946\n",
            "Training:   4% 10/226 [00:44<17:05,  4.75s/it]loss: 0.20373976230621338, grad_norm: 0.1965974122285843\n",
            "Training:   5% 11/226 [00:47<15:52,  4.43s/it]loss: 0.20795436203479767, grad_norm: 0.3984287977218628\n",
            "Training:   5% 12/226 [00:53<16:45,  4.70s/it]loss: 0.15601396560668945, grad_norm: 0.16348421573638916\n",
            "Training:   6% 13/226 [00:56<15:42,  4.42s/it]loss: 0.15317822992801666, grad_norm: 0.2617686986923218\n",
            "Training:   6% 14/226 [01:01<15:42,  4.44s/it]loss: 0.1862758845090866, grad_norm: 0.15271876752376556\n",
            "Training:   7% 15/226 [01:07<17:39,  5.02s/it]loss: 0.17850148677825928, grad_norm: 0.175688236951828\n",
            "Training:   7% 16/226 [01:13<17:52,  5.10s/it]loss: 0.20665813982486725, grad_norm: 0.24990089237689972\n",
            "Training:   8% 17/226 [01:17<16:54,  4.85s/it]loss: 0.19985738396644592, grad_norm: 0.20587189495563507\n",
            "Training:   8% 18/226 [01:24<18:43,  5.40s/it]loss: 0.20249046385288239, grad_norm: 0.1729777604341507\n",
            "Training:   8% 19/226 [01:27<17:01,  4.94s/it]loss: 0.11246860772371292, grad_norm: 0.13970810174942017\n",
            "Training:   9% 20/226 [01:36<20:46,  6.05s/it]loss: 0.219610333442688, grad_norm: 0.22047287225723267\n",
            "Training:   9% 21/226 [01:41<19:12,  5.62s/it]loss: 0.13323290646076202, grad_norm: 0.1718822866678238\n",
            "Training:  10% 22/226 [01:47<19:39,  5.78s/it]loss: 0.1130351573228836, grad_norm: 0.17260432243347168\n",
            "Training:  10% 23/226 [01:54<20:46,  6.14s/it]loss: 0.20907972753047943, grad_norm: 0.243279829621315\n",
            "Training:  11% 24/226 [01:58<18:29,  5.49s/it]loss: 0.17082788050174713, grad_norm: 0.1668875813484192\n",
            "Training:  11% 25/226 [02:03<18:19,  5.47s/it]loss: 0.15701888501644135, grad_norm: 0.13141785562038422\n",
            "Training:  12% 26/226 [02:10<19:47,  5.94s/it]loss: 0.20706352591514587, grad_norm: 0.17117856442928314\n",
            "Training:  12% 27/226 [02:15<18:19,  5.53s/it]loss: 0.15714053809642792, grad_norm: 0.1573156714439392\n",
            "Training:  12% 28/226 [02:21<18:25,  5.58s/it]loss: 0.2253398597240448, grad_norm: 0.2487158626317978\n",
            "Training:  13% 29/226 [02:23<15:01,  4.57s/it]loss: 0.1603405773639679, grad_norm: 0.15451738238334656\n",
            "Training:  13% 30/226 [02:30<17:39,  5.40s/it]loss: 0.20142273604869843, grad_norm: 0.1881362795829773\n",
            "Training:  14% 31/226 [02:34<16:28,  5.07s/it]loss: 0.2381969392299652, grad_norm: 0.29242900013923645\n",
            "Training:  14% 32/226 [02:37<14:04,  4.35s/it]loss: 0.21761547029018402, grad_norm: 0.17256781458854675\n",
            "Training:  15% 33/226 [02:45<17:52,  5.55s/it]loss: 0.1969984769821167, grad_norm: 0.18549175560474396\n",
            "Training:  15% 34/226 [02:51<17:57,  5.61s/it]loss: 0.23940986394882202, grad_norm: 0.23103757202625275\n",
            "Training:  15% 35/226 [03:01<21:27,  6.74s/it]loss: 0.1630786657333374, grad_norm: 0.14591415226459503\n",
            "Training:  16% 36/226 [03:07<21:01,  6.64s/it]loss: 0.20295077562332153, grad_norm: 0.24862487614154816\n",
            "Training:  16% 37/226 [03:11<18:16,  5.80s/it]loss: 0.23642194271087646, grad_norm: 0.25061044096946716\n",
            "Training:  17% 38/226 [03:14<15:18,  4.89s/it]loss: 0.20180973410606384, grad_norm: 0.1970752477645874\n",
            "Training:  17% 39/226 [03:18<14:53,  4.78s/it]loss: 0.1809682995080948, grad_norm: 0.17050126194953918\n",
            "Training:  18% 40/226 [03:24<15:52,  5.12s/it]loss: 0.14857923984527588, grad_norm: 0.17538291215896606\n",
            "Training:  18% 41/226 [03:29<15:25,  5.00s/it]loss: 0.21394920349121094, grad_norm: 0.20195506513118744\n",
            "Training:  19% 42/226 [03:32<14:10,  4.62s/it]loss: 0.1996927261352539, grad_norm: 0.27278828620910645\n",
            "Training:  19% 43/226 [03:37<14:25,  4.73s/it]loss: 0.21224069595336914, grad_norm: 0.1857275366783142\n",
            "Training:  19% 44/226 [03:40<12:07,  4.00s/it]loss: 0.18973582983016968, grad_norm: 0.5266649127006531\n",
            "Training:  20% 45/226 [03:50<18:07,  6.01s/it]loss: 0.26122206449508667, grad_norm: 0.2632428705692291\n",
            "Training:  20% 46/226 [03:53<15:17,  5.10s/it]loss: 0.1761437952518463, grad_norm: 0.15603531897068024\n",
            "Training:  21% 47/226 [03:59<15:44,  5.28s/it]loss: 0.15934331715106964, grad_norm: 0.19905102252960205\n",
            "Training:  21% 48/226 [04:03<14:12,  4.79s/it]loss: 0.14567705988883972, grad_norm: 0.18591730296611786\n",
            "Training:  22% 49/226 [04:07<13:31,  4.58s/it]loss: 0.11608204990625381, grad_norm: 0.13381586968898773\n",
            "Training:  22% 50/226 [04:14<16:01,  5.46s/it]loss: 0.17219600081443787, grad_norm: 0.20085248351097107\n",
            "Training:  23% 51/226 [04:18<14:03,  4.82s/it]loss: 0.12307046353816986, grad_norm: 0.1640365868806839\n",
            "Training:  23% 52/226 [04:28<18:24,  6.35s/it]loss: 0.22741568088531494, grad_norm: 0.23541602492332458\n",
            "Training:  23% 53/226 [04:31<15:27,  5.36s/it]loss: 0.15381088852882385, grad_norm: 0.1538708209991455\n",
            "Training:  24% 54/226 [04:36<15:45,  5.49s/it]loss: 0.21678771078586578, grad_norm: 0.4358275532722473\n",
            "Training:  24% 55/226 [04:42<15:39,  5.49s/it]loss: 0.1579805314540863, grad_norm: 0.18346461653709412\n",
            "Training:  25% 56/226 [04:47<15:21,  5.42s/it]loss: 0.16919763386249542, grad_norm: 0.16380728781223297\n",
            "Training:  25% 57/226 [04:53<15:59,  5.68s/it]loss: 0.19321975111961365, grad_norm: 0.15893793106079102\n",
            "Training:  26% 58/226 [04:58<14:57,  5.34s/it]loss: 0.16079546511173248, grad_norm: 0.16856378316879272\n",
            "Training:  26% 59/226 [05:04<15:00,  5.39s/it]loss: 0.2058742791414261, grad_norm: 1.3752975463867188\n",
            "Training:  27% 60/226 [05:08<14:25,  5.21s/it]loss: 0.16514696180820465, grad_norm: 0.14487901329994202\n",
            "Training:  27% 61/226 [05:14<14:44,  5.36s/it]loss: 0.22477024793624878, grad_norm: 0.27255651354789734\n",
            "Training:  27% 62/226 [05:16<12:14,  4.48s/it]loss: 0.17396090924739838, grad_norm: 0.31112122535705566\n",
            "Training:  28% 63/226 [05:21<12:23,  4.56s/it]loss: 0.1984223872423172, grad_norm: 0.24428313970565796\n",
            "Training:  28% 64/226 [05:27<12:57,  4.80s/it]loss: 0.21208882331848145, grad_norm: 0.2257365882396698\n",
            "Training:  29% 65/226 [05:34<15:16,  5.69s/it]loss: 0.14688976109027863, grad_norm: 0.3260233998298645\n",
            "Training:  29% 66/226 [05:45<18:56,  7.10s/it]loss: 0.16707730293273926, grad_norm: 0.18125174939632416\n",
            "Training:  30% 67/226 [05:49<16:33,  6.25s/it]loss: 0.2284863144159317, grad_norm: 0.3402769863605499\n",
            "Training:  30% 68/226 [05:52<14:16,  5.42s/it]loss: 0.15926283597946167, grad_norm: 0.15333323180675507\n",
            "Training:  31% 69/226 [06:03<17:57,  6.86s/it]loss: 0.27076268196105957, grad_norm: 0.8056760430335999\n",
            "Training:  31% 70/226 [06:05<14:00,  5.39s/it]loss: 0.15997879207134247, grad_norm: 0.18984825909137726\n",
            "Training:  31% 71/226 [06:09<13:28,  5.21s/it]loss: 0.17489762604236603, grad_norm: 0.17465245723724365\n",
            "Training:  32% 72/226 [06:15<13:48,  5.38s/it]loss: 0.25253167748451233, grad_norm: 0.29921507835388184\n",
            "Training:  32% 73/226 [06:18<11:32,  4.53s/it]loss: 0.21636375784873962, grad_norm: 0.24807560443878174\n",
            "Training:  33% 74/226 [06:21<10:50,  4.28s/it]loss: 0.14140619337558746, grad_norm: 0.20617790520191193\n",
            "Training:  33% 75/226 [06:30<13:45,  5.47s/it]loss: 0.1995576024055481, grad_norm: 0.19347625970840454\n",
            "Training:  34% 76/226 [06:35<13:18,  5.32s/it]loss: 0.3056468963623047, grad_norm: 0.3150808811187744\n",
            "Training:  34% 77/226 [06:37<10:37,  4.28s/it]loss: 0.15880730748176575, grad_norm: 0.17561748623847961\n",
            "Training:  35% 78/226 [06:42<11:25,  4.63s/it]loss: 0.23390445113182068, grad_norm: 0.24523913860321045\n",
            "Training:  35% 79/226 [06:45<10:18,  4.21s/it]loss: 0.16692113876342773, grad_norm: 0.19010549783706665\n",
            "Training:  35% 80/226 [06:50<10:56,  4.50s/it]loss: 0.22236040234565735, grad_norm: 0.2502273917198181\n",
            "Training:  36% 81/226 [06:53<09:14,  3.83s/it]loss: 0.2799036502838135, grad_norm: 0.47494828701019287\n",
            "Training:  36% 82/226 [06:55<08:17,  3.45s/it]loss: 0.2205066829919815, grad_norm: 0.20195268094539642\n",
            "Training:  37% 83/226 [06:58<07:59,  3.35s/it]loss: 0.180275559425354, grad_norm: 0.29383012652397156\n",
            "Training:  37% 84/226 [07:06<10:41,  4.52s/it]loss: 0.20487865805625916, grad_norm: 0.2597135603427887\n",
            "Training:  38% 85/226 [07:10<10:48,  4.60s/it]loss: 0.26458850502967834, grad_norm: 0.2866225242614746\n",
            "Training:  38% 86/226 [07:13<09:02,  3.87s/it]loss: 0.23702028393745422, grad_norm: 0.2686398923397064\n",
            "Training:  38% 87/226 [07:16<08:25,  3.63s/it]loss: 0.23424756526947021, grad_norm: 0.2856868803501129\n",
            "Training:  39% 88/226 [07:19<07:50,  3.41s/it]loss: 0.17220202088356018, grad_norm: 0.2033267319202423\n",
            "Training:  39% 89/226 [07:24<09:23,  4.11s/it]loss: 0.197416752576828, grad_norm: 0.16121242940425873\n",
            "Training:  40% 90/226 [07:28<08:54,  3.93s/it]loss: 0.1975412517786026, grad_norm: 0.19633492827415466\n",
            "Training:  40% 91/226 [07:32<09:20,  4.15s/it]loss: 0.2166980803012848, grad_norm: 0.24289663136005402\n",
            "Training:  41% 92/226 [07:35<08:18,  3.72s/it]loss: 0.2083572894334793, grad_norm: 0.18924371898174286\n",
            "Training:  41% 93/226 [07:38<07:58,  3.60s/it]loss: 0.16595549881458282, grad_norm: 0.1940932422876358\n",
            "Training:  42% 94/226 [07:44<09:23,  4.27s/it]loss: 0.18959468603134155, grad_norm: 0.19322632253170013\n",
            "Training:  42% 95/226 [07:52<11:32,  5.29s/it]loss: 0.16163226962089539, grad_norm: 0.15423540771007538\n",
            "Training:  42% 96/226 [07:57<11:08,  5.14s/it]loss: 0.1397416591644287, grad_norm: 0.1785491406917572\n",
            "Training:  43% 97/226 [08:01<10:44,  4.99s/it]loss: 0.19355672597885132, grad_norm: 0.19894228875637054\n",
            "Training:  43% 98/226 [08:05<09:37,  4.51s/it]loss: 0.2503744959831238, grad_norm: 0.30288589000701904\n",
            "Training:  44% 99/226 [08:07<08:21,  3.95s/it]loss: 0.25364136695861816, grad_norm: 0.31929102540016174\n",
            "Training:  44% 100/226 [08:10<07:33,  3.60s/it]loss: 0.16379530727863312, grad_norm: 0.27199694514274597\n",
            "Training:  45% 101/226 [08:16<09:03,  4.35s/it]loss: 0.22283567488193512, grad_norm: 0.23434126377105713\n",
            "Training:  45% 102/226 [08:20<08:25,  4.08s/it]loss: 0.19335098564624786, grad_norm: 0.19742104411125183\n",
            "Training:  46% 103/226 [08:24<08:44,  4.27s/it]loss: 0.2553384602069855, grad_norm: 0.23971977829933167\n",
            "Training:  46% 104/226 [08:28<08:14,  4.05s/it]loss: 0.26690673828125, grad_norm: 0.2988720238208771\n",
            "Training:  46% 105/226 [08:37<11:04,  5.49s/it]loss: 0.18707069754600525, grad_norm: 0.16701389849185944\n",
            "Training:  47% 106/226 [08:43<11:35,  5.79s/it]loss: 0.1942180097103119, grad_norm: 0.16827356815338135\n",
            "Training:  47% 107/226 [08:48<10:44,  5.41s/it]loss: 0.23332764208316803, grad_norm: 0.3110850155353546\n",
            "Training:  48% 108/226 [08:50<08:54,  4.53s/it]loss: 0.14836576581001282, grad_norm: 0.15844659507274628\n",
            "Training:  48% 109/226 [08:58<10:41,  5.49s/it]loss: 0.16058287024497986, grad_norm: 0.14932356774806976\n",
            "Training:  49% 110/226 [09:05<11:19,  5.86s/it]loss: 0.2899189889431, grad_norm: 0.3455159068107605\n",
            "Training:  49% 111/226 [09:07<08:56,  4.67s/it]loss: 0.17191524803638458, grad_norm: 0.18023014068603516\n",
            "Training:  50% 112/226 [09:11<08:50,  4.65s/it]loss: 0.17239563167095184, grad_norm: 0.18691523373126984\n",
            "Training:  50% 113/226 [09:15<08:16,  4.39s/it]loss: 0.22736625373363495, grad_norm: 0.19070157408714294\n",
            "Training:  50% 114/226 [09:18<07:24,  3.97s/it]loss: 0.17719480395317078, grad_norm: 0.2246391624212265\n",
            "Training:  51% 115/226 [09:30<11:46,  6.37s/it]loss: 0.21066522598266602, grad_norm: 0.21181584894657135\n",
            "Training:  51% 116/226 [09:35<10:45,  5.87s/it]loss: 0.27591463923454285, grad_norm: 0.22087977826595306\n",
            "Training:  52% 117/226 [09:38<09:05,  5.01s/it]loss: 0.1755571961402893, grad_norm: 0.18294768035411835\n",
            "Training:  52% 118/226 [09:43<08:52,  4.93s/it]loss: 0.15516297519207, grad_norm: 0.18945980072021484\n",
            "Training:  53% 119/226 [09:47<08:41,  4.88s/it]loss: 0.11318085342645645, grad_norm: 0.12568406760692596\n",
            "Training:  53% 120/226 [09:55<10:03,  5.69s/it]loss: 0.1529853343963623, grad_norm: 0.1792834997177124\n",
            "Training:  54% 121/226 [10:00<09:41,  5.54s/it]loss: 0.21602320671081543, grad_norm: 0.22645223140716553\n",
            "Training:  54% 122/226 [10:03<08:26,  4.87s/it]loss: 0.2739280164241791, grad_norm: 0.27267691493034363\n",
            "Training:  54% 123/226 [10:05<06:50,  3.99s/it]loss: 0.20927385985851288, grad_norm: 0.22733505070209503\n",
            "Training:  55% 124/226 [10:08<06:07,  3.61s/it]loss: 0.16408118605613708, grad_norm: 0.1718958616256714\n",
            "Training:  55% 125/226 [10:19<09:54,  5.89s/it]loss: 0.2793654799461365, grad_norm: 0.25563329458236694\n",
            "Training:  56% 126/226 [10:22<08:14,  4.94s/it]loss: 0.1974206566810608, grad_norm: 0.22210067510604858\n",
            "Training:  56% 127/226 [10:25<07:08,  4.33s/it]loss: 0.160810649394989, grad_norm: 0.16318263113498688\n",
            "Training:  57% 128/226 [10:31<07:55,  4.85s/it]loss: 0.21845169365406036, grad_norm: 0.23121608793735504\n",
            "Training:  57% 129/226 [10:35<07:18,  4.52s/it]loss: 0.1493656486272812, grad_norm: 0.16943705081939697\n",
            "Training:  58% 130/226 [10:40<07:44,  4.83s/it]loss: 0.16704057157039642, grad_norm: 0.16269934177398682\n",
            "Training:  58% 131/226 [10:44<07:18,  4.62s/it]loss: 0.1537671834230423, grad_norm: 0.13937218487262726\n",
            "Training:  58% 132/226 [10:51<08:04,  5.15s/it]loss: 0.17634259164333344, grad_norm: 0.16403235495090485\n",
            "Training:  59% 133/226 [10:56<07:53,  5.09s/it]loss: 0.31517237424850464, grad_norm: 0.2888716161251068\n",
            "Training:  59% 134/226 [10:57<06:18,  4.11s/it]loss: 0.1704963743686676, grad_norm: 0.16484038531780243\n",
            "Training:  60% 135/226 [11:06<08:23,  5.54s/it]loss: 0.1478101760149002, grad_norm: 0.1528673768043518\n",
            "Training:  60% 136/226 [11:12<08:29,  5.67s/it]loss: 0.1916617602109909, grad_norm: 0.18489693105220795\n",
            "Training:  61% 137/226 [11:16<07:36,  5.13s/it]loss: 0.15642817318439484, grad_norm: 0.1891803741455078\n",
            "Training:  61% 138/226 [11:21<07:26,  5.08s/it]loss: 0.16202761232852936, grad_norm: 0.1556324064731598\n",
            "Training:  62% 139/226 [11:26<07:11,  4.96s/it]loss: 0.19113825261592865, grad_norm: 0.15646229684352875\n",
            "Training:  62% 140/226 [11:29<06:31,  4.56s/it]loss: 0.18235106766223907, grad_norm: 0.2097262144088745\n",
            "Training:  62% 141/226 [11:36<07:27,  5.27s/it]loss: 0.14922893047332764, grad_norm: 0.14272285997867584\n",
            "Training:  63% 142/226 [11:42<07:26,  5.32s/it]loss: 0.16508907079696655, grad_norm: 0.1520451158285141\n",
            "Training:  63% 143/226 [11:48<07:52,  5.69s/it]loss: 0.13660478591918945, grad_norm: 0.15935471653938293\n",
            "Training:  64% 144/226 [11:53<07:26,  5.45s/it]loss: 0.147549107670784, grad_norm: 0.136050745844841\n",
            "Training:  64% 145/226 [12:01<08:05,  5.99s/it]loss: 0.19788922369480133, grad_norm: 0.17390058934688568\n",
            "Training:  65% 146/226 [12:04<07:06,  5.33s/it]loss: 0.17335397005081177, grad_norm: 0.1514049470424652\n",
            "Training:  65% 147/226 [12:09<06:40,  5.07s/it]loss: 0.21890759468078613, grad_norm: 0.2025872766971588\n",
            "Training:  65% 148/226 [12:13<06:06,  4.70s/it]loss: 0.1447938084602356, grad_norm: 0.13240249454975128\n",
            "Training:  66% 149/226 [12:21<07:21,  5.74s/it]loss: 0.2018166035413742, grad_norm: 0.16508211195468903\n",
            "Training:  66% 150/226 [12:26<07:14,  5.72s/it]loss: 0.15506282448768616, grad_norm: 0.16729289293289185\n",
            "Training:  67% 151/226 [12:31<06:51,  5.48s/it]loss: 0.1735323816537857, grad_norm: 0.15704062581062317\n",
            "Training:  67% 152/226 [12:38<07:05,  5.75s/it]loss: 0.2144988477230072, grad_norm: 0.2043142020702362\n",
            "Training:  68% 153/226 [12:41<05:54,  4.86s/it]loss: 0.2216733694076538, grad_norm: 0.17049212753772736\n",
            "Training:  68% 154/226 [12:44<05:30,  4.59s/it]loss: 0.23419524729251862, grad_norm: 0.207248255610466\n",
            "Training:  69% 155/226 [12:53<06:47,  5.74s/it]loss: 0.1784369796514511, grad_norm: 0.18071654438972473\n",
            "Training:  69% 156/226 [12:57<06:12,  5.32s/it]loss: 0.21553726494312286, grad_norm: 0.20772679150104523\n",
            "Training:  69% 157/226 [13:00<05:20,  4.65s/it]loss: 0.174479678273201, grad_norm: 0.2227741926908493\n",
            "Training:  70% 158/226 [13:04<04:53,  4.32s/it]loss: 0.10697002708911896, grad_norm: 0.13974422216415405\n",
            "Training:  70% 159/226 [13:13<06:18,  5.66s/it]loss: 0.12770996987819672, grad_norm: 0.12493640929460526\n",
            "Training:  71% 160/226 [13:21<07:15,  6.61s/it]loss: 0.13491350412368774, grad_norm: 0.1910233050584793\n",
            "Training:  71% 161/226 [13:30<07:45,  7.17s/it]loss: 0.15812820196151733, grad_norm: 0.16416989266872406\n",
            "Training:  72% 162/226 [13:36<07:10,  6.73s/it]loss: 0.19921110570430756, grad_norm: 0.16340263187885284\n",
            "Training:  72% 163/226 [13:41<06:41,  6.37s/it]loss: 0.18173018097877502, grad_norm: 0.16295327246189117\n",
            "Training:  73% 164/226 [13:45<05:51,  5.67s/it]loss: 0.1948148012161255, grad_norm: 0.18274952471256256\n",
            "Training:  73% 165/226 [13:52<06:03,  5.95s/it]loss: 0.13083931803703308, grad_norm: 0.14341652393341064\n",
            "Training:  73% 166/226 [13:57<05:46,  5.77s/it]loss: 0.16369879245758057, grad_norm: 0.15177839994430542\n",
            "Training:  74% 167/226 [14:03<05:39,  5.75s/it]loss: 0.2311994731426239, grad_norm: 0.25935521721839905\n",
            "Training:  74% 168/226 [14:06<04:42,  4.86s/it]loss: 0.11921657621860504, grad_norm: 0.13649998605251312\n",
            "Training:  75% 169/226 [14:14<05:42,  6.00s/it]loss: 0.1512439101934433, grad_norm: 0.13437262177467346\n",
            "Training:  75% 170/226 [14:19<05:20,  5.73s/it]loss: 0.14951899647712708, grad_norm: 0.1446811705827713\n",
            "Training:  76% 171/226 [14:24<04:58,  5.43s/it]loss: 0.19232489168643951, grad_norm: 0.17807775735855103\n",
            "Training:  76% 172/226 [14:28<04:33,  5.06s/it]loss: 0.24432572722434998, grad_norm: 0.24693524837493896\n",
            "Training:  77% 173/226 [14:31<03:49,  4.32s/it]loss: 0.12311442196369171, grad_norm: 0.13541123270988464\n",
            "Training:  77% 174/226 [14:38<04:23,  5.06s/it]loss: 0.18792428076267242, grad_norm: 0.18346036970615387\n",
            "Training:  77% 175/226 [14:43<04:27,  5.24s/it]loss: 0.17893871665000916, grad_norm: 0.16528216004371643\n",
            "Training:  78% 176/226 [14:48<04:14,  5.09s/it]loss: 0.1912171095609665, grad_norm: 0.13980542123317719\n",
            "Training:  78% 177/226 [14:52<03:58,  4.87s/it]loss: 0.2486717849969864, grad_norm: 0.26986077427864075\n",
            "Training:  79% 178/226 [14:55<03:18,  4.13s/it]loss: 0.2666564881801605, grad_norm: 0.3413122296333313\n",
            "Training:  79% 179/226 [14:57<02:48,  3.59s/it]loss: 0.18970535695552826, grad_norm: 0.1412106603384018\n",
            "Training:  80% 180/226 [15:03<03:10,  4.14s/it]loss: 0.16622132062911987, grad_norm: 0.14727553725242615\n",
            "Training:  80% 181/226 [15:07<03:10,  4.23s/it]loss: 0.21992753446102142, grad_norm: 0.16057290136814117\n",
            "Training:  81% 182/226 [15:10<02:52,  3.93s/it]loss: 0.1850212663412094, grad_norm: 0.17081265151500702\n",
            "Training:  81% 183/226 [15:15<02:57,  4.13s/it]loss: 0.1748369187116623, grad_norm: 0.14655937254428864\n",
            "Training:  81% 184/226 [15:21<03:14,  4.62s/it]loss: 0.17347094416618347, grad_norm: 0.17254841327667236\n",
            "Training:  82% 185/226 [15:29<03:51,  5.66s/it]loss: 0.20225146412849426, grad_norm: 0.21177226305007935\n",
            "Training:  82% 186/226 [15:31<03:06,  4.66s/it]loss: 0.18092729151248932, grad_norm: 0.16589653491973877\n",
            "Training:  83% 187/226 [15:35<02:50,  4.38s/it]loss: 0.2051389366388321, grad_norm: 0.17833633720874786\n",
            "Training:  83% 188/226 [15:39<02:42,  4.27s/it]loss: 0.1635493040084839, grad_norm: 0.15623997151851654\n",
            "Training:  84% 189/226 [15:43<02:31,  4.11s/it]loss: 0.2361796796321869, grad_norm: 0.29972878098487854\n",
            "Training:  84% 190/226 [15:45<02:07,  3.53s/it]loss: 0.15914587676525116, grad_norm: 0.14776118099689484\n",
            "Training:  85% 191/226 [15:50<02:18,  3.95s/it]loss: 0.16774038970470428, grad_norm: 0.1720273643732071\n",
            "Training:  85% 192/226 [15:54<02:21,  4.17s/it]loss: 0.3100518584251404, grad_norm: 0.3588108420372009\n",
            "Training:  85% 193/226 [15:56<01:54,  3.46s/it]loss: 0.2397485375404358, grad_norm: 0.19732044637203217\n",
            "Training:  86% 194/226 [16:00<01:50,  3.44s/it]loss: 0.13368383049964905, grad_norm: 0.17140315473079681\n",
            "Training:  86% 195/226 [16:09<02:46,  5.39s/it]loss: 0.1662793755531311, grad_norm: 0.16521593928337097\n",
            "Training:  87% 196/226 [16:15<02:38,  5.28s/it]loss: 0.16683731973171234, grad_norm: 0.21183671057224274\n",
            "Training:  87% 197/226 [16:20<02:31,  5.21s/it]loss: 0.1702043116092682, grad_norm: 0.174086332321167\n",
            "Training:  88% 198/226 [16:24<02:19,  4.97s/it]loss: 0.20131966471672058, grad_norm: 0.16221383213996887\n",
            "Training:  88% 199/226 [16:31<02:26,  5.44s/it]loss: 0.2068573534488678, grad_norm: 0.2357027381658554\n",
            "Training:  88% 200/226 [16:34<02:04,  4.78s/it]loss: 0.18439491093158722, grad_norm: 0.14744198322296143\n",
            "Training:  89% 201/226 [16:39<02:03,  4.93s/it]loss: 0.16234581172466278, grad_norm: 0.15746912360191345\n",
            "Training:  89% 202/226 [16:44<01:59,  4.99s/it]loss: 0.15714412927627563, grad_norm: 0.18294645845890045\n",
            "Training:  90% 203/226 [16:52<02:13,  5.79s/it]loss: 0.27856019139289856, grad_norm: 0.28596439957618713\n",
            "Training:  90% 204/226 [16:54<01:42,  4.65s/it]loss: 0.18842315673828125, grad_norm: 0.1852981448173523\n",
            "Training:  91% 205/226 [17:01<01:53,  5.42s/it]loss: 0.18540775775909424, grad_norm: 0.20161324739456177\n",
            "Training:  91% 206/226 [17:05<01:42,  5.12s/it]loss: 0.1645752638578415, grad_norm: 0.22324003279209137\n",
            "Training:  92% 207/226 [17:10<01:32,  4.88s/it]loss: 0.22988854348659515, grad_norm: 0.2251909226179123\n",
            "Training:  92% 208/226 [17:13<01:16,  4.25s/it]loss: 0.1854141503572464, grad_norm: 0.18401093780994415\n",
            "Training:  92% 209/226 [17:17<01:11,  4.19s/it]loss: 0.14759156107902527, grad_norm: 0.1419203132390976\n",
            "Training:  93% 210/226 [17:22<01:12,  4.54s/it]loss: 0.1859157383441925, grad_norm: 0.17481403052806854\n",
            "Training:  93% 211/226 [17:27<01:12,  4.83s/it]loss: 0.1575351059436798, grad_norm: 0.15452982485294342\n",
            "Training:  94% 212/226 [17:31<01:03,  4.54s/it]loss: 0.2014525830745697, grad_norm: 0.1881064474582672\n",
            "Training:  94% 213/226 [17:37<01:02,  4.79s/it]loss: 0.12087944149971008, grad_norm: 0.1429705172777176\n",
            "Training:  95% 214/226 [17:44<01:05,  5.46s/it]loss: 0.23397216200828552, grad_norm: 0.18956778943538666\n",
            "Training:  95% 215/226 [17:54<01:16,  6.96s/it]loss: 0.21803168952465057, grad_norm: 0.17368891835212708\n",
            "Training:  96% 216/226 [17:59<01:02,  6.21s/it]loss: 0.19986918568611145, grad_norm: 0.16752535104751587\n",
            "Training:  96% 217/226 [18:03<00:50,  5.59s/it]loss: 0.1472400575876236, grad_norm: 0.1527782827615738\n",
            "Training:  96% 218/226 [18:12<00:54,  6.78s/it]loss: 0.2726856470108032, grad_norm: 0.2632581889629364\n",
            "Training:  97% 219/226 [18:15<00:38,  5.50s/it]loss: 0.255668044090271, grad_norm: 0.3253774642944336\n",
            "Training:  97% 220/226 [18:18<00:28,  4.67s/it]loss: 0.15532547235488892, grad_norm: 0.1543477177619934\n",
            "Training:  98% 221/226 [18:24<00:25,  5.16s/it]loss: 0.2417047917842865, grad_norm: 0.26315417885780334\n",
            "Training:  98% 222/226 [18:27<00:17,  4.44s/it]loss: 0.14063912630081177, grad_norm: 0.12439257651567459\n",
            "Training:  99% 223/226 [18:33<00:15,  5.06s/it]loss: 0.15889054536819458, grad_norm: 0.1364177018404007\n",
            "Training:  99% 224/226 [18:39<00:10,  5.18s/it]loss: 0.16627687215805054, grad_norm: 0.14445734024047852\n",
            "Training: 100% 225/226 [18:50<00:06,  6.90s/it]loss: 0.20191411674022675, grad_norm: 0.17349651455879211\n",
            "Training: 100% 226/226 [18:53<00:00,  5.02s/it]\n",
            "Validation loss: 0.7665703868865967\n",
            "Epoch: 2\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.18591319024562836, grad_norm: 0.15660254657268524\n",
            "Training:   0% 1/226 [00:06<24:20,  6.49s/it]loss: 0.1823733001947403, grad_norm: 0.16012510657310486\n",
            "Training:   1% 2/226 [00:11<20:18,  5.44s/it]loss: 0.24059033393859863, grad_norm: 0.24541720747947693\n",
            "Training:   1% 3/226 [00:14<16:53,  4.55s/it]loss: 0.13232815265655518, grad_norm: 0.1607847958803177\n",
            "Training:   2% 4/226 [00:19<17:42,  4.79s/it]loss: 0.2188904583454132, grad_norm: 0.7060853242874146\n",
            "Training:   2% 5/226 [00:24<17:32,  4.76s/it]loss: 0.19069033861160278, grad_norm: 0.20127564668655396\n",
            "Training:   3% 6/226 [00:27<15:41,  4.28s/it]loss: 0.16128188371658325, grad_norm: 0.14754842221736908\n",
            "Training:   3% 7/226 [00:33<17:45,  4.86s/it]loss: 0.13855049014091492, grad_norm: 0.1843467801809311\n",
            "Training:   4% 8/226 [00:41<20:13,  5.57s/it]loss: 0.12214072048664093, grad_norm: 0.12366138398647308\n",
            "Training:   4% 9/226 [00:48<22:00,  6.08s/it]loss: 0.1874324381351471, grad_norm: 0.14994804561138153\n",
            "Training:   4% 10/226 [00:52<19:24,  5.39s/it]loss: 0.15662230551242828, grad_norm: 0.176535502076149\n",
            "Training:   5% 11/226 [00:58<20:42,  5.78s/it]loss: 0.13759100437164307, grad_norm: 0.1483229398727417\n",
            "Training:   5% 12/226 [01:04<20:49,  5.84s/it]loss: 0.13958343863487244, grad_norm: 0.1381627321243286\n",
            "Training:   6% 13/226 [01:11<21:28,  6.05s/it]loss: 0.1950719952583313, grad_norm: 0.2146182656288147\n",
            "Training:   6% 14/226 [01:14<18:10,  5.14s/it]loss: 0.2152535617351532, grad_norm: 0.17573058605194092\n",
            "Training:   7% 15/226 [01:18<16:54,  4.81s/it]loss: 0.19517889618873596, grad_norm: 0.27554816007614136\n",
            "Training:   7% 16/226 [01:21<15:31,  4.44s/it]loss: 0.20822881162166595, grad_norm: 0.390923410654068\n",
            "Training:   8% 17/226 [01:25<14:36,  4.19s/it]loss: 0.21672436594963074, grad_norm: 0.18169069290161133\n",
            "Training:   8% 18/226 [01:29<14:00,  4.04s/it]loss: 0.2465486079454422, grad_norm: 0.18564936518669128\n",
            "Training:   8% 19/226 [01:38<19:28,  5.65s/it]loss: 0.24343302845954895, grad_norm: 0.5025392770767212\n",
            "Training:   9% 20/226 [01:41<16:35,  4.83s/it]loss: 0.2687613368034363, grad_norm: 0.21694926917552948\n",
            "Training:   9% 21/226 [01:43<13:58,  4.09s/it]loss: 0.1675235778093338, grad_norm: 0.2130887657403946\n",
            "Training:  10% 22/226 [01:48<14:10,  4.17s/it]loss: 0.2235376536846161, grad_norm: 0.2310217022895813\n",
            "Training:  10% 23/226 [01:50<12:33,  3.71s/it]loss: 0.24398306012153625, grad_norm: 0.21228361129760742\n",
            "Training:  11% 24/226 [01:54<12:06,  3.60s/it]loss: 0.1888536661863327, grad_norm: 0.1576196849346161\n",
            "Training:  11% 25/226 [01:58<12:52,  3.84s/it]loss: 0.13366492092609406, grad_norm: 0.17165417969226837\n",
            "Training:  12% 26/226 [02:04<15:00,  4.50s/it]loss: 0.1667787730693817, grad_norm: 0.15894962847232819\n",
            "Training:  12% 27/226 [02:11<17:17,  5.21s/it]loss: 0.3023151159286499, grad_norm: 0.2568204402923584\n",
            "Training:  12% 28/226 [02:13<14:26,  4.37s/it]loss: 0.19957761466503143, grad_norm: 0.23778487741947174\n",
            "Training:  13% 29/226 [02:21<17:37,  5.37s/it]loss: 0.20155709981918335, grad_norm: 0.2176186740398407\n",
            "Training:  13% 30/226 [02:25<16:25,  5.03s/it]loss: 0.23166954517364502, grad_norm: 0.26772651076316833\n",
            "Training:  14% 31/226 [02:28<13:54,  4.28s/it]loss: 0.21397818624973297, grad_norm: 0.2436124086380005\n",
            "Training:  14% 32/226 [02:31<12:43,  3.93s/it]loss: 0.20780228078365326, grad_norm: 0.1710764318704605\n",
            "Training:  15% 33/226 [02:36<13:27,  4.18s/it]loss: 0.17522528767585754, grad_norm: 0.1945953667163849\n",
            "Training:  15% 34/226 [02:41<14:14,  4.45s/it]loss: 0.1879541277885437, grad_norm: 0.18871265649795532\n",
            "Training:  15% 35/226 [02:46<14:26,  4.53s/it]loss: 0.11977792531251907, grad_norm: 0.12382033467292786\n",
            "Training:  16% 36/226 [02:53<16:51,  5.33s/it]loss: 0.21988697350025177, grad_norm: 0.24272802472114563\n",
            "Training:  16% 37/226 [02:56<14:26,  4.59s/it]loss: 0.22402189671993256, grad_norm: 0.18118171393871307\n",
            "Training:  17% 38/226 [02:59<13:11,  4.21s/it]loss: 0.2897648215293884, grad_norm: 0.2919008135795593\n",
            "Training:  17% 39/226 [03:07<16:45,  5.38s/it]loss: 0.21667426824569702, grad_norm: 0.16051295399665833\n",
            "Training:  18% 40/226 [03:12<16:20,  5.27s/it]loss: 0.21470704674720764, grad_norm: 0.1902536153793335\n",
            "Training:  18% 41/226 [03:18<16:43,  5.42s/it]loss: 0.14426137506961823, grad_norm: 0.15762922167778015\n",
            "Training:  19% 42/226 [03:25<18:28,  6.02s/it]loss: 0.14786973595619202, grad_norm: 0.15015307068824768\n",
            "Training:  19% 43/226 [03:32<18:38,  6.11s/it]loss: 0.16530387103557587, grad_norm: 0.13868777453899384\n",
            "Training:  19% 44/226 [03:37<17:25,  5.74s/it]loss: 0.21243739128112793, grad_norm: 0.1649312973022461\n",
            "Training:  20% 45/226 [03:41<16:27,  5.46s/it]loss: 0.14324168860912323, grad_norm: 0.16533681750297546\n",
            "Training:  20% 46/226 [03:46<15:49,  5.27s/it]loss: 0.1894642859697342, grad_norm: 0.17413057386875153\n",
            "Training:  21% 47/226 [03:51<15:24,  5.16s/it]loss: 0.13447587192058563, grad_norm: 0.18697594106197357\n",
            "Training:  21% 48/226 [03:56<15:09,  5.11s/it]loss: 0.23663710057735443, grad_norm: 0.1800367683172226\n",
            "Training:  22% 49/226 [04:06<19:19,  6.55s/it]loss: 0.29566872119903564, grad_norm: 0.26766085624694824\n",
            "Training:  22% 50/226 [04:08<14:53,  5.08s/it]loss: 0.18406744301319122, grad_norm: 0.20340292155742645\n",
            "Training:  23% 51/226 [04:12<14:04,  4.82s/it]loss: 0.28219085931777954, grad_norm: 0.23236209154129028\n",
            "Training:  23% 52/226 [04:14<11:50,  4.08s/it]loss: 0.11792807281017303, grad_norm: 0.14745482802391052\n",
            "Training:  23% 53/226 [04:22<15:23,  5.34s/it]loss: 0.11975160986185074, grad_norm: 0.15880276262760162\n",
            "Training:  24% 54/226 [04:30<17:28,  6.09s/it]loss: 0.2906126081943512, grad_norm: 0.3257007896900177\n",
            "Training:  24% 55/226 [04:32<13:51,  4.86s/it]loss: 0.175405353307724, grad_norm: 0.1983492076396942\n",
            "Training:  25% 56/226 [04:36<12:57,  4.57s/it]loss: 0.26804041862487793, grad_norm: 0.2829447388648987\n",
            "Training:  25% 57/226 [04:38<10:44,  3.82s/it]loss: 0.18554526567459106, grad_norm: 0.18169084191322327\n",
            "Training:  26% 58/226 [04:43<11:11,  4.00s/it]loss: 0.1872073858976364, grad_norm: 0.1744857281446457\n",
            "Training:  26% 59/226 [04:55<18:10,  6.53s/it]loss: 0.19267688691616058, grad_norm: 0.16292628645896912\n",
            "Training:  27% 60/226 [04:59<15:49,  5.72s/it]loss: 0.14964739978313446, grad_norm: 0.14268313348293304\n",
            "Training:  27% 61/226 [05:04<15:29,  5.63s/it]loss: 0.14830218255519867, grad_norm: 0.16361652314662933\n",
            "Training:  27% 62/226 [05:09<14:10,  5.19s/it]loss: 0.24787595868110657, grad_norm: 0.2626285254955292\n",
            "Training:  28% 63/226 [05:11<11:57,  4.40s/it]loss: 0.17989078164100647, grad_norm: 0.17449919879436493\n",
            "Training:  28% 64/226 [05:14<10:57,  4.06s/it]loss: 0.12771311402320862, grad_norm: 0.15155093371868134\n",
            "Training:  29% 65/226 [05:23<14:13,  5.30s/it]loss: 0.14658363163471222, grad_norm: 0.12947528064250946\n",
            "Training:  29% 66/226 [05:31<16:29,  6.19s/it]loss: 0.21617484092712402, grad_norm: 0.18453799188137054\n",
            "Training:  30% 67/226 [05:34<14:06,  5.33s/it]loss: 0.22498776018619537, grad_norm: 0.19635091722011566\n",
            "Training:  30% 68/226 [05:39<13:20,  5.06s/it]loss: 0.14209863543510437, grad_norm: 0.19089661538600922\n",
            "Training:  31% 69/226 [05:50<18:37,  7.12s/it]loss: 0.2218022495508194, grad_norm: 0.19764402508735657\n",
            "Training:  31% 70/226 [05:53<15:17,  5.88s/it]loss: 0.17412443459033966, grad_norm: 0.43333157896995544\n",
            "Training:  31% 71/226 [05:57<13:42,  5.31s/it]loss: 0.18146906793117523, grad_norm: 0.20982122421264648\n",
            "Training:  32% 72/226 [06:02<12:47,  4.98s/it]loss: 0.14998167753219604, grad_norm: 0.14879342913627625\n",
            "Training:  32% 73/226 [06:08<13:42,  5.38s/it]loss: 0.13591338694095612, grad_norm: 0.1668391227722168\n",
            "Training:  33% 74/226 [06:13<13:34,  5.36s/it]loss: 0.24035857617855072, grad_norm: 0.23219463229179382\n",
            "Training:  33% 75/226 [06:16<11:34,  4.60s/it]loss: 0.17964833974838257, grad_norm: 0.14642296731472015\n",
            "Training:  34% 76/226 [06:20<11:10,  4.47s/it]loss: 0.22650454938411713, grad_norm: 0.24560923874378204\n",
            "Training:  34% 77/226 [06:23<09:55,  4.00s/it]loss: 0.15651866793632507, grad_norm: 0.12986184656620026\n",
            "Training:  35% 78/226 [06:28<10:15,  4.16s/it]loss: 0.20635095238685608, grad_norm: 0.17096848785877228\n",
            "Training:  35% 79/226 [06:38<14:25,  5.89s/it]loss: 0.158589705824852, grad_norm: 0.21262328326702118\n",
            "Training:  35% 80/226 [06:41<12:48,  5.27s/it]loss: 0.15891864895820618, grad_norm: 0.13289135694503784\n",
            "Training:  36% 81/226 [06:47<13:02,  5.39s/it]loss: 0.1369696706533432, grad_norm: 0.12443302571773529\n",
            "Training:  36% 82/226 [06:54<14:07,  5.88s/it]loss: 0.1571941077709198, grad_norm: 0.132019504904747\n",
            "Training:  37% 83/226 [06:59<13:06,  5.50s/it]loss: 0.13598790764808655, grad_norm: 0.11280667781829834\n",
            "Training:  37% 84/226 [07:09<16:18,  6.89s/it]loss: 0.18167608976364136, grad_norm: 0.15279586613178253\n",
            "Training:  38% 85/226 [07:15<15:39,  6.67s/it]loss: 0.26532626152038574, grad_norm: 0.21181564033031464\n",
            "Training:  38% 86/226 [07:18<12:49,  5.50s/it]loss: 0.1841658055782318, grad_norm: 0.192564457654953\n",
            "Training:  38% 87/226 [07:22<11:39,  5.03s/it]loss: 0.1399870216846466, grad_norm: 0.12519869208335876\n",
            "Training:  39% 88/226 [07:27<11:53,  5.17s/it]loss: 0.18587782979011536, grad_norm: 0.1760719269514084\n",
            "Training:  39% 89/226 [07:37<15:15,  6.68s/it]loss: 0.23650696873664856, grad_norm: 0.20874585211277008\n",
            "Training:  40% 90/226 [07:41<13:17,  5.87s/it]loss: 0.2084597647190094, grad_norm: 0.18414433300495148\n",
            "Training:  40% 91/226 [07:44<11:04,  4.92s/it]loss: 0.16629557311534882, grad_norm: 0.14833061397075653\n",
            "Training:  41% 92/226 [07:49<11:14,  5.03s/it]loss: 0.2219076305627823, grad_norm: 0.22125855088233948\n",
            "Training:  41% 93/226 [07:52<09:34,  4.32s/it]loss: 0.20015574991703033, grad_norm: 0.19226188957691193\n",
            "Training:  42% 94/226 [07:56<09:02,  4.11s/it]loss: 0.12275160104036331, grad_norm: 0.11037277430295944\n",
            "Training:  42% 95/226 [08:05<12:05,  5.54s/it]loss: 0.17432808876037598, grad_norm: 0.1474144458770752\n",
            "Training:  42% 96/226 [08:09<11:30,  5.31s/it]loss: 0.10810782015323639, grad_norm: 0.10308821499347687\n",
            "Training:  43% 97/226 [08:17<12:59,  6.04s/it]loss: 0.2014380693435669, grad_norm: 0.20450426638126373\n",
            "Training:  43% 98/226 [08:20<11:06,  5.20s/it]loss: 0.20571471750736237, grad_norm: 0.1767251342535019\n",
            "Training:  44% 99/226 [08:30<14:03,  6.64s/it]loss: 0.1852072924375534, grad_norm: 0.17677250504493713\n",
            "Training:  44% 100/226 [08:34<11:58,  5.71s/it]loss: 0.19111479818820953, grad_norm: 0.15750570595264435\n",
            "Training:  45% 101/226 [08:40<11:51,  5.69s/it]loss: 0.1922929286956787, grad_norm: 0.17385900020599365\n",
            "Training:  45% 102/226 [08:46<11:56,  5.78s/it]loss: 0.15936973690986633, grad_norm: 0.14773783087730408\n",
            "Training:  46% 103/226 [08:50<11:01,  5.38s/it]loss: 0.22031459212303162, grad_norm: 0.19907446205615997\n",
            "Training:  46% 104/226 [08:53<09:33,  4.70s/it]loss: 0.1911388635635376, grad_norm: 0.16225887835025787\n",
            "Training:  46% 105/226 [08:57<08:52,  4.40s/it]loss: 0.3426685035228729, grad_norm: 0.35431167483329773\n",
            "Training:  47% 106/226 [08:59<07:24,  3.70s/it]loss: 0.21643397212028503, grad_norm: 0.3004049062728882\n",
            "Training:  47% 107/226 [09:01<06:30,  3.28s/it]loss: 0.2251729667186737, grad_norm: 0.216406911611557\n",
            "Training:  48% 108/226 [09:03<05:42,  2.90s/it]loss: 0.17712947726249695, grad_norm: 0.15256303548812866\n",
            "Training:  48% 109/226 [09:07<06:20,  3.25s/it]loss: 0.2393023520708084, grad_norm: 0.2730821967124939\n",
            "Training:  49% 110/226 [09:10<05:43,  2.96s/it]loss: 0.16167649626731873, grad_norm: 0.19650880992412567\n",
            "Training:  49% 111/226 [09:16<07:26,  3.88s/it]loss: 0.10285881906747818, grad_norm: 0.1229131668806076\n",
            "Training:  50% 112/226 [09:26<11:05,  5.84s/it]loss: 0.22800928354263306, grad_norm: 0.3009251058101654\n",
            "Training:  50% 113/226 [09:29<09:10,  4.87s/it]loss: 0.2128576785326004, grad_norm: 0.18584272265434265\n",
            "Training:  50% 114/226 [09:33<08:34,  4.59s/it]loss: 0.22421953082084656, grad_norm: 0.24935947358608246\n",
            "Training:  51% 115/226 [09:36<07:56,  4.30s/it]loss: 0.27748632431030273, grad_norm: 0.2935493588447571\n",
            "Training:  51% 116/226 [09:38<06:41,  3.65s/it]loss: 0.15681251883506775, grad_norm: 0.15850947797298431\n",
            "Training:  52% 117/226 [09:45<08:08,  4.48s/it]loss: 0.20079153776168823, grad_norm: 0.16615784168243408\n",
            "Training:  52% 118/226 [09:51<08:56,  4.97s/it]loss: 0.1797386109828949, grad_norm: 0.18147967755794525\n",
            "Training:  53% 119/226 [10:00<11:20,  6.36s/it]loss: 0.2167784869670868, grad_norm: 0.19187887012958527\n",
            "Training:  53% 120/226 [10:04<09:55,  5.61s/it]loss: 0.20405565202236176, grad_norm: 0.2059924155473709\n",
            "Training:  54% 121/226 [10:07<08:23,  4.80s/it]loss: 0.14023874700069427, grad_norm: 0.12280157208442688\n",
            "Training:  54% 122/226 [10:13<08:55,  5.15s/it]loss: 0.13897207379341125, grad_norm: 0.13294664025306702\n",
            "Training:  54% 123/226 [10:20<09:55,  5.78s/it]loss: 0.1545952409505844, grad_norm: 0.3433993458747864\n",
            "Training:  55% 124/226 [10:27<10:15,  6.04s/it]loss: 0.1679295301437378, grad_norm: 0.15060877799987793\n",
            "Training:  55% 125/226 [10:31<09:19,  5.54s/it]loss: 0.27114248275756836, grad_norm: 0.25186270475387573\n",
            "Training:  56% 126/226 [10:34<07:50,  4.70s/it]loss: 0.1730192005634308, grad_norm: 0.15505827963352203\n",
            "Training:  56% 127/226 [10:40<08:22,  5.07s/it]loss: 0.2382093369960785, grad_norm: 0.29937463998794556\n",
            "Training:  57% 128/226 [10:43<07:12,  4.41s/it]loss: 0.2113780677318573, grad_norm: 0.17316386103630066\n",
            "Training:  57% 129/226 [10:52<09:15,  5.73s/it]loss: 0.22422830760478973, grad_norm: 0.19624824821949005\n",
            "Training:  58% 130/226 [10:55<08:00,  5.01s/it]loss: 0.19195395708084106, grad_norm: 0.18748779594898224\n",
            "Training:  58% 131/226 [11:00<07:58,  5.04s/it]loss: 0.1636217087507248, grad_norm: 0.14644910395145416\n",
            "Training:  58% 132/226 [11:06<08:17,  5.30s/it]loss: 0.2137734442949295, grad_norm: 0.4585352838039398\n",
            "Training:  59% 133/226 [11:12<08:37,  5.56s/it]loss: 0.15281370282173157, grad_norm: 0.16332286596298218\n",
            "Training:  59% 134/226 [11:19<09:07,  5.95s/it]loss: 0.14085471630096436, grad_norm: 0.182305708527565\n",
            "Training:  60% 135/226 [11:24<08:29,  5.59s/it]loss: 0.1537836343050003, grad_norm: 0.12434597313404083\n",
            "Training:  60% 136/226 [11:31<08:56,  5.96s/it]loss: 0.16104236245155334, grad_norm: 0.16319188475608826\n",
            "Training:  61% 137/226 [11:36<08:33,  5.77s/it]loss: 0.19707708060741425, grad_norm: 0.15663379430770874\n",
            "Training:  61% 138/226 [11:40<07:49,  5.34s/it]loss: 0.1700802743434906, grad_norm: 0.2080465406179428\n",
            "Training:  62% 139/226 [11:51<10:10,  7.02s/it]loss: 0.23442475497722626, grad_norm: 0.1934809684753418\n",
            "Training:  62% 140/226 [11:54<08:13,  5.74s/it]loss: 0.16030465066432953, grad_norm: 0.12338868528604507\n",
            "Training:  62% 141/226 [12:00<08:01,  5.66s/it]loss: 0.2428196668624878, grad_norm: 0.2247759848833084\n",
            "Training:  63% 142/226 [12:02<06:42,  4.79s/it]loss: 0.16283239424228668, grad_norm: 0.12459605932235718\n",
            "Training:  63% 143/226 [12:09<07:26,  5.37s/it]loss: 0.13577407598495483, grad_norm: 0.15489384531974792\n",
            "Training:  64% 144/226 [12:15<07:25,  5.43s/it]loss: 0.235352024435997, grad_norm: 0.2313711941242218\n",
            "Training:  64% 145/226 [12:17<06:09,  4.57s/it]loss: 0.1596229374408722, grad_norm: 0.22672708332538605\n",
            "Training:  65% 146/226 [12:21<05:51,  4.39s/it]loss: 0.22259318828582764, grad_norm: 0.18956252932548523\n",
            "Training:  65% 147/226 [12:25<05:27,  4.15s/it]loss: 0.25489699840545654, grad_norm: 0.18033646047115326\n",
            "Training:  65% 148/226 [12:28<04:56,  3.80s/it]loss: 0.2941964268684387, grad_norm: 0.25235459208488464\n",
            "Training:  66% 149/226 [12:36<06:39,  5.19s/it]loss: 0.20551888644695282, grad_norm: 0.1913064569234848\n",
            "Training:  66% 150/226 [12:40<05:59,  4.73s/it]loss: 0.1657111942768097, grad_norm: 0.1617036610841751\n",
            "Training:  67% 151/226 [12:46<06:28,  5.18s/it]loss: 0.14364202320575714, grad_norm: 0.1288706511259079\n",
            "Training:  67% 152/226 [12:56<08:00,  6.49s/it]loss: 0.25019046664237976, grad_norm: 0.20714372396469116\n",
            "Training:  68% 153/226 [12:59<06:36,  5.43s/it]loss: 0.2102959007024765, grad_norm: 0.20078371465206146\n",
            "Training:  68% 154/226 [13:01<05:29,  4.58s/it]loss: 0.22892433404922485, grad_norm: 0.272687703371048\n",
            "Training:  69% 155/226 [13:04<04:49,  4.07s/it]loss: 0.16927428543567657, grad_norm: 0.17194285988807678\n",
            "Training:  69% 156/226 [13:09<05:03,  4.34s/it]loss: 0.16905473172664642, grad_norm: 0.1403673142194748\n",
            "Training:  69% 157/226 [13:13<04:46,  4.15s/it]loss: 0.19613274931907654, grad_norm: 0.16264429688453674\n",
            "Training:  70% 158/226 [13:17<04:50,  4.28s/it]loss: 0.14270290732383728, grad_norm: 0.11887787282466888\n",
            "Training:  70% 159/226 [13:30<07:28,  6.70s/it]loss: 0.12089341878890991, grad_norm: 0.12735013663768768\n",
            "Training:  71% 160/226 [13:37<07:40,  6.98s/it]loss: 0.15904532372951508, grad_norm: 0.1222136840224266\n",
            "Training:  71% 161/226 [13:42<06:49,  6.30s/it]loss: 0.20742487907409668, grad_norm: 0.2585447132587433\n",
            "Training:  72% 162/226 [13:45<05:41,  5.33s/it]loss: 0.1773017793893814, grad_norm: 0.17032957077026367\n",
            "Training:  72% 163/226 [13:49<05:16,  5.02s/it]loss: 0.2195516675710678, grad_norm: 0.20520208775997162\n",
            "Training:  73% 164/226 [13:52<04:27,  4.31s/it]loss: 0.2199394702911377, grad_norm: 0.2146221101284027\n",
            "Training:  73% 165/226 [13:55<03:55,  3.86s/it]loss: 0.17790034413337708, grad_norm: 0.16154596209526062\n",
            "Training:  73% 166/226 [14:00<04:16,  4.27s/it]loss: 0.18218661844730377, grad_norm: 0.175912544131279\n",
            "Training:  74% 167/226 [14:04<04:11,  4.27s/it]loss: 0.16993246972560883, grad_norm: 0.14956308901309967\n",
            "Training:  74% 168/226 [14:10<04:33,  4.71s/it]loss: 0.2111019790172577, grad_norm: 0.18351955711841583\n",
            "Training:  75% 169/226 [14:21<06:12,  6.54s/it]loss: 0.21538153290748596, grad_norm: 0.21023480594158173\n",
            "Training:  75% 170/226 [14:24<05:11,  5.56s/it]loss: 0.20192356407642365, grad_norm: 0.1896059662103653\n",
            "Training:  76% 171/226 [14:31<05:22,  5.86s/it]loss: 0.11707345396280289, grad_norm: 0.1332198679447174\n",
            "Training:  76% 172/226 [14:38<05:48,  6.45s/it]loss: 0.2257140725851059, grad_norm: 0.2025357037782669\n",
            "Training:  77% 173/226 [14:43<05:12,  5.90s/it]loss: 0.22099435329437256, grad_norm: 0.2806578576564789\n",
            "Training:  77% 174/226 [14:48<04:56,  5.71s/it]loss: 0.16965660452842712, grad_norm: 0.1826886087656021\n",
            "Training:  77% 175/226 [14:55<05:04,  5.98s/it]loss: 0.18286336958408356, grad_norm: 0.1830197125673294\n",
            "Training:  78% 176/226 [15:00<04:47,  5.76s/it]loss: 0.19051945209503174, grad_norm: 0.1558295041322708\n",
            "Training:  78% 177/226 [15:04<04:13,  5.17s/it]loss: 0.24796724319458008, grad_norm: 0.21315354108810425\n",
            "Training:  79% 178/226 [15:06<03:26,  4.31s/it]loss: 0.20316144824028015, grad_norm: 0.1889420747756958\n",
            "Training:  79% 179/226 [15:12<03:37,  4.62s/it]loss: 0.17399217188358307, grad_norm: 0.15278419852256775\n",
            "Training:  80% 180/226 [15:16<03:31,  4.59s/it]loss: 0.22312286496162415, grad_norm: 0.1752406507730484\n",
            "Training:  80% 181/226 [15:21<03:24,  4.54s/it]loss: 0.2244722694158554, grad_norm: 0.16667045652866364\n",
            "Training:  81% 182/226 [15:25<03:13,  4.39s/it]loss: 0.22772088646888733, grad_norm: 0.20546811819076538\n",
            "Training:  81% 183/226 [15:27<02:44,  3.82s/it]loss: 0.16823801398277283, grad_norm: 0.15593624114990234\n",
            "Training:  81% 184/226 [15:32<02:47,  4.00s/it]loss: 0.21475574374198914, grad_norm: 0.189762145280838\n",
            "Training:  82% 185/226 [15:35<02:36,  3.82s/it]loss: 0.1759720891714096, grad_norm: 0.15100574493408203\n",
            "Training:  82% 186/226 [15:39<02:29,  3.74s/it]loss: 0.28948408365249634, grad_norm: 0.2518002986907959\n",
            "Training:  83% 187/226 [15:40<02:04,  3.19s/it]loss: 0.14332391321659088, grad_norm: 0.13273032009601593\n",
            "Training:  83% 188/226 [15:47<02:36,  4.12s/it]loss: 0.24877245724201202, grad_norm: 0.25155147910118103\n",
            "Training:  84% 189/226 [15:50<02:22,  3.85s/it]loss: 0.18363170325756073, grad_norm: 0.13863228261470795\n",
            "Training:  84% 190/226 [15:54<02:16,  3.80s/it]loss: 0.1258966326713562, grad_norm: 0.15626415610313416\n",
            "Training:  85% 191/226 [16:02<03:04,  5.28s/it]loss: 0.22824732959270477, grad_norm: 0.28479042649269104\n",
            "Training:  85% 192/226 [16:05<02:34,  4.53s/it]loss: 0.21125243604183197, grad_norm: 0.21955353021621704\n",
            "Training:  85% 193/226 [16:10<02:33,  4.65s/it]loss: 0.18084578216075897, grad_norm: 0.20112372934818268\n",
            "Training:  86% 194/226 [16:13<02:16,  4.28s/it]loss: 0.1455869823694229, grad_norm: 0.1631629467010498\n",
            "Training:  86% 195/226 [16:19<02:22,  4.59s/it]loss: 0.1807626336812973, grad_norm: 0.2166200578212738\n",
            "Training:  87% 196/226 [16:23<02:16,  4.56s/it]loss: 0.2085634171962738, grad_norm: 0.18672242760658264\n",
            "Training:  87% 197/226 [16:26<01:57,  4.05s/it]loss: 0.17549210786819458, grad_norm: 0.20435699820518494\n",
            "Training:  88% 198/226 [16:31<02:03,  4.41s/it]loss: 0.2271713763475418, grad_norm: 0.23411127924919128\n",
            "Training:  88% 199/226 [16:40<02:36,  5.78s/it]loss: 0.14728042483329773, grad_norm: 0.146224707365036\n",
            "Training:  88% 200/226 [16:45<02:19,  5.38s/it]loss: 0.2176414430141449, grad_norm: 0.1729506403207779\n",
            "Training:  89% 201/226 [16:49<02:04,  4.99s/it]loss: 0.11560531705617905, grad_norm: 0.1545054167509079\n",
            "Training:  89% 202/226 [16:56<02:14,  5.59s/it]loss: 0.16298554837703705, grad_norm: 0.19971983134746552\n",
            "Training:  90% 203/226 [17:00<02:00,  5.23s/it]loss: 0.17886781692504883, grad_norm: 0.15273430943489075\n",
            "Training:  90% 204/226 [17:05<01:51,  5.07s/it]loss: 0.16436852514743805, grad_norm: 0.19381782412528992\n",
            "Training:  91% 205/226 [17:10<01:47,  5.12s/it]loss: 0.2202625274658203, grad_norm: 0.17959924042224884\n",
            "Training:  91% 206/226 [17:14<01:32,  4.63s/it]loss: 0.20661334693431854, grad_norm: 0.1740492284297943\n",
            "Training:  92% 207/226 [17:17<01:19,  4.20s/it]loss: 0.13059528172016144, grad_norm: 0.13899171352386475\n",
            "Training:  92% 208/226 [17:23<01:26,  4.78s/it]loss: 0.19944025576114655, grad_norm: 0.2402377873659134\n",
            "Training:  92% 209/226 [17:29<01:28,  5.18s/it]loss: 0.13263389468193054, grad_norm: 0.27116161584854126\n",
            "Training:  93% 210/226 [17:35<01:24,  5.25s/it]loss: 0.19368340075016022, grad_norm: 0.19552892446517944\n",
            "Training:  93% 211/226 [17:38<01:08,  4.57s/it]loss: 0.19246850907802582, grad_norm: 0.19441236555576324\n",
            "Training:  94% 212/226 [17:41<01:01,  4.38s/it]loss: 0.10654958337545395, grad_norm: 0.11117949336767197\n",
            "Training:  94% 213/226 [17:51<01:15,  5.82s/it]loss: 0.33511441946029663, grad_norm: 0.5820366740226746\n",
            "Training:  95% 214/226 [17:52<00:54,  4.54s/it]loss: 0.17724093794822693, grad_norm: 0.18060167133808136\n",
            "Training:  95% 215/226 [17:56<00:48,  4.41s/it]loss: 0.21554966270923615, grad_norm: 0.2122570127248764\n",
            "Training:  96% 216/226 [18:02<00:47,  4.79s/it]loss: 0.1752513200044632, grad_norm: 0.18603669106960297\n",
            "Training:  96% 217/226 [18:06<00:41,  4.60s/it]loss: 0.22429223358631134, grad_norm: 0.28075751662254333\n",
            "Training:  96% 218/226 [18:09<00:31,  4.00s/it]loss: 0.1695406287908554, grad_norm: 0.16604115068912506\n",
            "Training:  97% 219/226 [18:19<00:40,  5.82s/it]loss: 0.24646404385566711, grad_norm: 0.24866452813148499\n",
            "Training:  97% 220/226 [18:22<00:30,  5.01s/it]loss: 0.1558733731508255, grad_norm: 0.14455321431159973\n",
            "Training:  98% 221/226 [18:31<00:31,  6.34s/it]loss: 0.22844240069389343, grad_norm: 0.2742871642112732\n",
            "Training:  98% 222/226 [18:34<00:20,  5.25s/it]loss: 0.19226238131523132, grad_norm: 0.18916471302509308\n",
            "Training:  99% 223/226 [18:39<00:15,  5.24s/it]loss: 0.23315368592739105, grad_norm: 0.29927462339401245\n",
            "Training:  99% 224/226 [18:42<00:08,  4.40s/it]loss: 0.16271193325519562, grad_norm: 0.3043079972267151\n",
            "Training: 100% 225/226 [18:46<00:04,  4.50s/it]loss: 0.1695185750722885, grad_norm: 0.17792551219463348\n",
            "Training: 100% 226/226 [18:51<00:00,  5.00s/it]\n",
            "Validation loss: 0.7134038996696472\n",
            "Epoch: 3\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.21783927083015442, grad_norm: 0.26997923851013184\n",
            "Training:   0% 1/226 [00:03<13:21,  3.56s/it]loss: 0.16191864013671875, grad_norm: 0.19763882458209991\n",
            "Training:   1% 2/226 [00:07<14:32,  3.90s/it]loss: 0.2319847196340561, grad_norm: 0.2569499611854553\n",
            "Training:   1% 3/226 [00:11<13:44,  3.70s/it]loss: 0.17324618995189667, grad_norm: 0.1727401465177536\n",
            "Training:   2% 4/226 [00:15<14:58,  4.05s/it]loss: 0.1477484405040741, grad_norm: 0.20760668814182281\n",
            "Training:   2% 5/226 [00:24<20:57,  5.69s/it]loss: 0.14087222516536713, grad_norm: 0.15121811628341675\n",
            "Training:   3% 6/226 [00:30<20:52,  5.69s/it]loss: 0.18659305572509766, grad_norm: 0.15765635669231415\n",
            "Training:   3% 7/226 [00:34<19:24,  5.32s/it]loss: 0.1909521520137787, grad_norm: 0.17425839602947235\n",
            "Training:   4% 8/226 [00:39<19:24,  5.34s/it]loss: 0.16137349605560303, grad_norm: 0.1867842972278595\n",
            "Training:   4% 9/226 [00:44<18:22,  5.08s/it]loss: 0.2099943459033966, grad_norm: 0.21576076745986938\n",
            "Training:   4% 10/226 [00:49<17:39,  4.91s/it]loss: 0.19909919798374176, grad_norm: 0.14362147450447083\n",
            "Training:   5% 11/226 [00:54<18:15,  5.10s/it]loss: 0.20453707873821259, grad_norm: 0.17075592279434204\n",
            "Training:   5% 12/226 [00:58<17:00,  4.77s/it]loss: 0.3038942813873291, grad_norm: 0.31602180004119873\n",
            "Training:   6% 13/226 [01:01<15:13,  4.29s/it]loss: 0.23812231421470642, grad_norm: 0.19120728969573975\n",
            "Training:   6% 14/226 [01:04<13:30,  3.82s/it]loss: 0.15611813962459564, grad_norm: 0.16759365797042847\n",
            "Training:   7% 15/226 [01:10<16:15,  4.62s/it]loss: 0.15880070626735687, grad_norm: 0.1779680848121643\n",
            "Training:   7% 16/226 [01:18<19:12,  5.49s/it]loss: 0.15173791348934174, grad_norm: 0.12605580687522888\n",
            "Training:   8% 17/226 [01:23<18:38,  5.35s/it]loss: 0.13032448291778564, grad_norm: 0.11994382739067078\n",
            "Training:   8% 18/226 [01:33<23:27,  6.77s/it]loss: 0.14911457896232605, grad_norm: 0.1314479559659958\n",
            "Training:   8% 19/226 [01:42<25:51,  7.50s/it]loss: 0.24155764281749725, grad_norm: 0.20953427255153656\n",
            "Training:   9% 20/226 [01:45<20:59,  6.11s/it]loss: 0.2156345099210739, grad_norm: 0.18924278020858765\n",
            "Training:   9% 21/226 [01:48<17:21,  5.08s/it]loss: 0.14379984140396118, grad_norm: 0.13564017415046692\n",
            "Training:  10% 22/226 [01:53<17:02,  5.01s/it]loss: 0.19188791513442993, grad_norm: 0.18734219670295715\n",
            "Training:  10% 23/226 [01:59<18:37,  5.50s/it]loss: 0.14576378464698792, grad_norm: 0.15570330619812012\n",
            "Training:  11% 24/226 [02:08<21:26,  6.37s/it]loss: 0.1275288611650467, grad_norm: 0.15074145793914795\n",
            "Training:  11% 25/226 [02:14<21:38,  6.46s/it]loss: 0.15269315242767334, grad_norm: 0.15728867053985596\n",
            "Training:  12% 26/226 [02:18<19:10,  5.75s/it]loss: 0.12195257097482681, grad_norm: 0.12625043094158173\n",
            "Training:  12% 27/226 [02:26<21:00,  6.33s/it]loss: 0.17871946096420288, grad_norm: 0.20452633500099182\n",
            "Training:  12% 28/226 [02:30<18:35,  5.63s/it]loss: 0.19301730394363403, grad_norm: 0.21625551581382751\n",
            "Training:  13% 29/226 [02:33<15:52,  4.83s/it]loss: 0.2380353957414627, grad_norm: 0.27198800444602966\n",
            "Training:  13% 30/226 [02:36<14:02,  4.30s/it]loss: 0.13596093654632568, grad_norm: 0.1736460030078888\n",
            "Training:  14% 31/226 [02:41<14:15,  4.39s/it]loss: 0.1568039506673813, grad_norm: 0.1369992196559906\n",
            "Training:  14% 32/226 [02:46<14:45,  4.57s/it]loss: 0.1529088169336319, grad_norm: 0.1340676099061966\n",
            "Training:  15% 33/226 [02:58<21:47,  6.78s/it]loss: 0.18284757435321808, grad_norm: 0.17013604938983917\n",
            "Training:  15% 34/226 [03:03<20:16,  6.34s/it]loss: 0.21599823236465454, grad_norm: 0.23514483869075775\n",
            "Training:  15% 35/226 [03:05<16:20,  5.13s/it]loss: 0.1660001426935196, grad_norm: 0.14582771062850952\n",
            "Training:  16% 36/226 [03:10<15:24,  4.86s/it]loss: 0.15708304941654205, grad_norm: 0.14678031206130981\n",
            "Training:  16% 37/226 [03:14<14:40,  4.66s/it]loss: 0.1322121024131775, grad_norm: 0.13446125388145447\n",
            "Training:  17% 38/226 [03:19<15:37,  4.99s/it]loss: 0.1256287544965744, grad_norm: 0.18191763758659363\n",
            "Training:  17% 39/226 [03:25<16:17,  5.23s/it]loss: 0.1707036793231964, grad_norm: 0.12605020403862\n",
            "Training:  18% 40/226 [03:31<16:21,  5.28s/it]loss: 0.22876618802547455, grad_norm: 0.2144872099161148\n",
            "Training:  18% 41/226 [03:33<13:53,  4.50s/it]loss: 0.19414767622947693, grad_norm: 0.158966064453125\n",
            "Training:  19% 42/226 [03:38<14:02,  4.58s/it]loss: 0.18278361856937408, grad_norm: 0.16735053062438965\n",
            "Training:  19% 43/226 [03:45<15:53,  5.21s/it]loss: 0.14827139675617218, grad_norm: 0.11186874657869339\n",
            "Training:  19% 44/226 [03:52<17:52,  5.89s/it]loss: 0.14770524203777313, grad_norm: 0.13038140535354614\n",
            "Training:  20% 45/226 [03:57<16:28,  5.46s/it]loss: 0.16980913281440735, grad_norm: 0.1478533148765564\n",
            "Training:  20% 46/226 [04:01<15:05,  5.03s/it]loss: 0.1559615582227707, grad_norm: 0.13226716220378876\n",
            "Training:  21% 47/226 [04:07<15:45,  5.28s/it]loss: 0.1463470757007599, grad_norm: 0.11299711465835571\n",
            "Training:  21% 48/226 [04:16<19:13,  6.48s/it]loss: 0.23965132236480713, grad_norm: 0.1705484390258789\n",
            "Training:  22% 49/226 [04:19<16:12,  5.50s/it]loss: 0.22134530544281006, grad_norm: 0.2079743891954422\n",
            "Training:  22% 50/226 [04:24<15:09,  5.17s/it]loss: 0.22899016737937927, grad_norm: 0.24654486775398254\n",
            "Training:  23% 51/226 [04:27<13:53,  4.76s/it]loss: 0.24739938974380493, grad_norm: 0.21004809439182281\n",
            "Training:  23% 52/226 [04:30<11:33,  3.99s/it]loss: 0.17972928285598755, grad_norm: 0.1580265462398529\n",
            "Training:  23% 53/226 [04:40<17:14,  5.98s/it]loss: 0.14662915468215942, grad_norm: 0.1501154750585556\n",
            "Training:  24% 54/226 [04:47<17:27,  6.09s/it]loss: 0.18268106877803802, grad_norm: 0.14746399223804474\n",
            "Training:  24% 55/226 [04:52<17:03,  5.98s/it]loss: 0.16456125676631927, grad_norm: 0.12810173630714417\n",
            "Training:  25% 56/226 [04:58<16:24,  5.79s/it]loss: 0.18751563131809235, grad_norm: 0.15703831613063812\n",
            "Training:  25% 57/226 [05:02<15:27,  5.49s/it]loss: 0.15604761242866516, grad_norm: 0.13187141716480255\n",
            "Training:  26% 58/226 [05:09<16:24,  5.86s/it]loss: 0.17366008460521698, grad_norm: 0.1460932344198227\n",
            "Training:  26% 59/226 [05:14<15:13,  5.47s/it]loss: 0.18831369280815125, grad_norm: 0.22862908244132996\n",
            "Training:  27% 60/226 [05:17<13:19,  4.81s/it]loss: 0.21624967455863953, grad_norm: 0.19159814715385437\n",
            "Training:  27% 61/226 [05:20<11:41,  4.25s/it]loss: 0.16457019746303558, grad_norm: 0.1458895057439804\n",
            "Training:  27% 62/226 [05:24<11:13,  4.11s/it]loss: 0.16225706040859222, grad_norm: 0.18117739260196686\n",
            "Training:  28% 63/226 [05:34<15:57,  5.88s/it]loss: 0.2001798152923584, grad_norm: 0.15015186369419098\n",
            "Training:  28% 64/226 [05:38<14:41,  5.44s/it]loss: 0.21754471957683563, grad_norm: 0.2797670066356659\n",
            "Training:  29% 65/226 [05:41<12:46,  4.76s/it]loss: 0.12172552943229675, grad_norm: 0.12132933735847473\n",
            "Training:  29% 66/226 [05:50<16:11,  6.07s/it]loss: 0.25278475880622864, grad_norm: 0.2557835280895233\n",
            "Training:  30% 67/226 [05:53<13:05,  4.94s/it]loss: 0.23112739622592926, grad_norm: 0.27452024817466736\n",
            "Training:  30% 68/226 [05:56<11:31,  4.38s/it]loss: 0.16889406740665436, grad_norm: 0.16298307478427887\n",
            "Training:  31% 69/226 [06:01<11:48,  4.51s/it]loss: 0.19613583385944366, grad_norm: 0.17877638339996338\n",
            "Training:  31% 70/226 [06:03<10:29,  4.03s/it]loss: 0.12289131432771683, grad_norm: 0.13434907793998718\n",
            "Training:  31% 71/226 [06:10<12:43,  4.93s/it]loss: 0.24355104565620422, grad_norm: 0.2312740534543991\n",
            "Training:  32% 72/226 [06:13<11:03,  4.31s/it]loss: 0.2339753657579422, grad_norm: 0.21726509928703308\n",
            "Training:  32% 73/226 [06:19<12:00,  4.71s/it]loss: 0.16382700204849243, grad_norm: 0.17069664597511292\n",
            "Training:  33% 74/226 [06:23<11:31,  4.55s/it]loss: 0.2416042685508728, grad_norm: 0.19513845443725586\n",
            "Training:  33% 75/226 [06:28<11:39,  4.63s/it]loss: 0.15919160842895508, grad_norm: 0.15073877573013306\n",
            "Training:  34% 76/226 [06:32<11:06,  4.44s/it]loss: 0.259538471698761, grad_norm: 0.2563471794128418\n",
            "Training:  34% 77/226 [06:34<09:08,  3.68s/it]loss: 0.17224407196044922, grad_norm: 0.14002923667430878\n",
            "Training:  35% 78/226 [06:39<10:12,  4.14s/it]loss: 0.21772485971450806, grad_norm: 0.20033863186836243\n",
            "Training:  35% 79/226 [06:42<09:09,  3.74s/it]loss: 0.1758154034614563, grad_norm: 0.14990204572677612\n",
            "Training:  35% 80/226 [06:46<09:16,  3.81s/it]loss: 0.15676282346248627, grad_norm: 0.15606744587421417\n",
            "Training:  36% 81/226 [06:50<09:40,  4.01s/it]loss: 0.14959068596363068, grad_norm: 0.13486215472221375\n",
            "Training:  36% 82/226 [06:56<10:50,  4.52s/it]loss: 0.19023993611335754, grad_norm: 0.14981110394001007\n",
            "Training:  37% 83/226 [07:08<15:45,  6.61s/it]loss: 0.20082087814807892, grad_norm: 0.1818259060382843\n",
            "Training:  37% 84/226 [07:10<12:51,  5.44s/it]loss: 0.24101847410202026, grad_norm: 0.34971702098846436\n",
            "Training:  38% 85/226 [07:12<10:29,  4.47s/it]loss: 0.19190450012683868, grad_norm: 0.15375226736068726\n",
            "Training:  38% 86/226 [07:17<10:19,  4.43s/it]loss: 0.18352791666984558, grad_norm: 0.14364346861839294\n",
            "Training:  38% 87/226 [07:23<11:39,  5.03s/it]loss: 0.22783242166042328, grad_norm: 0.2097899615764618\n",
            "Training:  39% 88/226 [07:27<10:27,  4.54s/it]loss: 0.16715538501739502, grad_norm: 0.15563327074050903\n",
            "Training:  39% 89/226 [07:30<09:50,  4.31s/it]loss: 0.15391850471496582, grad_norm: 0.2141757756471634\n",
            "Training:  40% 90/226 [07:34<09:23,  4.15s/it]loss: 0.11603692919015884, grad_norm: 0.2172728180885315\n",
            "Training:  40% 91/226 [07:41<11:00,  4.89s/it]loss: 0.22272710502147675, grad_norm: 0.2035924345254898\n",
            "Training:  41% 92/226 [07:44<09:35,  4.30s/it]loss: 0.17018643021583557, grad_norm: 0.16647100448608398\n",
            "Training:  41% 93/226 [07:50<11:05,  5.00s/it]loss: 0.1276707500219345, grad_norm: 0.14529815316200256\n",
            "Training:  42% 94/226 [07:57<11:51,  5.39s/it]loss: 0.20310728251934052, grad_norm: 0.21024483442306519\n",
            "Training:  42% 95/226 [08:00<10:10,  4.66s/it]loss: 0.16262778639793396, grad_norm: 0.11974872648715973\n",
            "Training:  42% 96/226 [08:08<12:46,  5.90s/it]loss: 0.1225830465555191, grad_norm: 0.1306254267692566\n",
            "Training:  43% 97/226 [08:16<13:30,  6.28s/it]loss: 0.17107625305652618, grad_norm: 0.13832640647888184\n",
            "Training:  43% 98/226 [08:20<12:07,  5.69s/it]loss: 0.18535761535167694, grad_norm: 0.20167651772499084\n",
            "Training:  44% 99/226 [08:24<10:59,  5.19s/it]loss: 0.1472737193107605, grad_norm: 0.15830761194229126\n",
            "Training:  44% 100/226 [08:28<10:21,  4.94s/it]loss: 0.30072006583213806, grad_norm: 0.36765119433403015\n",
            "Training:  45% 101/226 [08:30<08:24,  4.03s/it]loss: 0.18013277649879456, grad_norm: 0.17828308045864105\n",
            "Training:  45% 102/226 [08:36<09:35,  4.64s/it]loss: 0.22569003701210022, grad_norm: 0.20043286681175232\n",
            "Training:  46% 103/226 [08:46<12:35,  6.14s/it]loss: 0.12181486189365387, grad_norm: 0.1304643750190735\n",
            "Training:  46% 104/226 [08:56<14:47,  7.28s/it]loss: 0.24943894147872925, grad_norm: 0.26333993673324585\n",
            "Training:  46% 105/226 [08:58<11:41,  5.80s/it]loss: 0.17285311222076416, grad_norm: 0.17283746600151062\n",
            "Training:  47% 106/226 [09:03<10:53,  5.44s/it]loss: 0.1870918869972229, grad_norm: 0.13181667029857635\n",
            "Training:  47% 107/226 [09:08<10:40,  5.38s/it]loss: 0.17788627743721008, grad_norm: 0.1584472358226776\n",
            "Training:  48% 108/226 [09:13<10:04,  5.12s/it]loss: 0.18004806339740753, grad_norm: 0.2030872404575348\n",
            "Training:  48% 109/226 [09:17<09:24,  4.82s/it]loss: 0.1917954683303833, grad_norm: 0.1921709030866623\n",
            "Training:  49% 110/226 [09:20<08:18,  4.30s/it]loss: 0.12509611248970032, grad_norm: 0.12268612533807755\n",
            "Training:  49% 111/226 [09:29<10:57,  5.72s/it]loss: 0.21320253610610962, grad_norm: 0.1820579171180725\n",
            "Training:  50% 112/226 [09:33<09:47,  5.15s/it]loss: 0.2548225224018097, grad_norm: 0.20697417855262756\n",
            "Training:  50% 113/226 [09:41<11:42,  6.22s/it]loss: 0.2097693681716919, grad_norm: 1.5346158742904663\n",
            "Training:  50% 114/226 [09:47<11:28,  6.15s/it]loss: 0.2733437716960907, grad_norm: 0.24596014618873596\n",
            "Training:  51% 115/226 [09:50<09:25,  5.10s/it]loss: 0.204381063580513, grad_norm: 0.2530420124530792\n",
            "Training:  51% 116/226 [09:54<08:40,  4.73s/it]loss: 0.2354130744934082, grad_norm: 1.9650120735168457\n",
            "Training:  52% 117/226 [09:58<08:06,  4.47s/it]loss: 0.1859947144985199, grad_norm: 0.1948353350162506\n",
            "Training:  52% 118/226 [10:02<08:08,  4.53s/it]loss: 0.1713746041059494, grad_norm: 0.1528741717338562\n",
            "Training:  53% 119/226 [10:08<08:52,  4.97s/it]loss: 0.15235890448093414, grad_norm: 0.15538164973258972\n",
            "Training:  53% 120/226 [10:13<08:50,  5.01s/it]loss: 0.18296244740486145, grad_norm: 0.2433733195066452\n",
            "Training:  54% 121/226 [10:17<08:11,  4.68s/it]loss: 0.1354673057794571, grad_norm: 0.18387733399868011\n",
            "Training:  54% 122/226 [10:23<08:48,  5.08s/it]loss: 0.31840693950653076, grad_norm: 0.4638887643814087\n",
            "Training:  54% 123/226 [10:27<07:53,  4.60s/it]loss: 0.1929393857717514, grad_norm: 0.2244069129228592\n",
            "Training:  55% 124/226 [10:31<07:50,  4.62s/it]loss: 0.1647532731294632, grad_norm: 0.2146233171224594\n",
            "Training:  55% 125/226 [10:36<07:33,  4.49s/it]loss: 0.1652933657169342, grad_norm: 0.17118492722511292\n",
            "Training:  56% 126/226 [10:42<08:16,  4.96s/it]loss: 0.136725515127182, grad_norm: 0.19644306600093842\n",
            "Training:  56% 127/226 [10:49<09:20,  5.66s/it]loss: 0.2241588979959488, grad_norm: 0.2717376947402954\n",
            "Training:  57% 128/226 [10:52<07:50,  4.80s/it]loss: 0.2061847299337387, grad_norm: 0.2696520984172821\n",
            "Training:  57% 129/226 [10:57<07:51,  4.86s/it]loss: 0.2136746197938919, grad_norm: 0.2849592864513397\n",
            "Training:  58% 130/226 [10:59<06:41,  4.18s/it]loss: 0.15896376967430115, grad_norm: 0.17298491299152374\n",
            "Training:  58% 131/226 [11:05<07:15,  4.59s/it]loss: 0.2055160105228424, grad_norm: 0.23696433007717133\n",
            "Training:  58% 132/226 [11:08<06:33,  4.18s/it]loss: 0.15300291776657104, grad_norm: 0.23646804690361023\n",
            "Training:  59% 133/226 [11:14<07:19,  4.73s/it]loss: 0.24108560383319855, grad_norm: 0.24612639844417572\n",
            "Training:  59% 134/226 [11:17<06:15,  4.08s/it]loss: 0.19744989275932312, grad_norm: 0.18754810094833374\n",
            "Training:  60% 135/226 [11:22<06:40,  4.40s/it]loss: 0.1706872284412384, grad_norm: 0.2032383233308792\n",
            "Training:  60% 136/226 [11:27<06:51,  4.57s/it]loss: 0.18477682769298553, grad_norm: 0.252608984708786\n",
            "Training:  61% 137/226 [11:32<06:56,  4.68s/it]loss: 0.22015875577926636, grad_norm: 0.2082173377275467\n",
            "Training:  61% 138/226 [11:35<06:16,  4.28s/it]loss: 0.1692585051059723, grad_norm: 0.20474666357040405\n",
            "Training:  62% 139/226 [11:40<06:36,  4.56s/it]loss: 0.20716363191604614, grad_norm: 0.23263019323349\n",
            "Training:  62% 140/226 [11:44<05:55,  4.14s/it]loss: 0.24636365473270416, grad_norm: 0.2384653091430664\n",
            "Training:  62% 141/226 [11:46<05:07,  3.62s/it]loss: 0.15332895517349243, grad_norm: 0.17373445630073547\n",
            "Training:  63% 142/226 [11:50<05:27,  3.90s/it]loss: 0.3324095606803894, grad_norm: 0.3284841775894165\n",
            "Training:  63% 143/226 [11:53<04:57,  3.58s/it]loss: 0.1472896933555603, grad_norm: 0.21658240258693695\n",
            "Training:  64% 144/226 [11:58<05:12,  3.81s/it]loss: 0.14574620127677917, grad_norm: 0.18042951822280884\n",
            "Training:  64% 145/226 [12:04<06:17,  4.66s/it]loss: 0.17370593547821045, grad_norm: 0.14428812265396118\n",
            "Training:  65% 146/226 [12:10<06:34,  4.93s/it]loss: 0.24303163588047028, grad_norm: 0.3912239968776703\n",
            "Training:  65% 147/226 [12:13<05:45,  4.38s/it]loss: 0.2611131966114044, grad_norm: 0.2350434511899948\n",
            "Training:  65% 148/226 [12:16<04:59,  3.85s/it]loss: 0.21089842915534973, grad_norm: 0.23946219682693481\n",
            "Training:  66% 149/226 [12:20<05:05,  3.97s/it]loss: 0.1993730068206787, grad_norm: 0.20724806189537048\n",
            "Training:  66% 150/226 [12:23<04:42,  3.72s/it]loss: 0.15157845616340637, grad_norm: 0.15864266455173492\n",
            "Training:  67% 151/226 [12:30<06:00,  4.81s/it]loss: 0.15805864334106445, grad_norm: 0.1882753074169159\n",
            "Training:  67% 152/226 [12:35<05:58,  4.85s/it]loss: 0.17774894833564758, grad_norm: 0.20344232022762299\n",
            "Training:  68% 153/226 [12:43<06:46,  5.58s/it]loss: 0.2122257798910141, grad_norm: 0.25407204031944275\n",
            "Training:  68% 154/226 [12:46<06:00,  5.01s/it]loss: 0.22433315217494965, grad_norm: 0.4708417057991028\n",
            "Training:  69% 155/226 [12:49<05:02,  4.25s/it]loss: 0.2124946266412735, grad_norm: 0.26247429847717285\n",
            "Training:  69% 156/226 [12:52<04:43,  4.05s/it]loss: 0.1756865382194519, grad_norm: 0.18418706953525543\n",
            "Training:  69% 157/226 [12:56<04:33,  3.97s/it]loss: 0.22207115590572357, grad_norm: 0.22298577427864075\n",
            "Training:  70% 158/226 [13:01<04:53,  4.31s/it]loss: 0.19512036442756653, grad_norm: 0.2505076825618744\n",
            "Training:  70% 159/226 [13:06<04:52,  4.37s/it]loss: 0.21802131831645966, grad_norm: 0.4504784345626831\n",
            "Training:  71% 160/226 [13:10<04:39,  4.23s/it]loss: 0.18961507081985474, grad_norm: 0.20949287712574005\n",
            "Training:  71% 161/226 [13:13<04:23,  4.05s/it]loss: 0.2673242390155792, grad_norm: 0.2922567129135132\n",
            "Training:  72% 162/226 [13:15<03:39,  3.43s/it]loss: 0.1916724294424057, grad_norm: 0.23364923894405365\n",
            "Training:  72% 163/226 [13:22<04:31,  4.30s/it]loss: 0.22857743501663208, grad_norm: 0.21180199086666107\n",
            "Training:  73% 164/226 [13:26<04:27,  4.31s/it]loss: 0.16007240116596222, grad_norm: 0.14478428661823273\n",
            "Training:  73% 165/226 [13:31<04:41,  4.61s/it]loss: 0.2342245876789093, grad_norm: 0.21290388703346252\n",
            "Training:  73% 166/226 [13:34<04:13,  4.22s/it]loss: 0.22708100080490112, grad_norm: 0.30339378118515015\n",
            "Training:  74% 167/226 [13:37<03:44,  3.81s/it]loss: 0.2150711566209793, grad_norm: 0.21925701200962067\n",
            "Training:  74% 168/226 [13:42<04:01,  4.17s/it]loss: 0.19552335143089294, grad_norm: 0.17515580356121063\n",
            "Training:  75% 169/226 [13:48<04:18,  4.54s/it]loss: 0.18911761045455933, grad_norm: 0.2548474669456482\n",
            "Training:  75% 170/226 [13:52<04:08,  4.44s/it]loss: 0.20775282382965088, grad_norm: 0.20553328096866608\n",
            "Training:  76% 171/226 [13:57<04:09,  4.54s/it]loss: 0.2872066795825958, grad_norm: 0.2769666314125061\n",
            "Training:  76% 172/226 [13:59<03:27,  3.83s/it]loss: 0.24933397769927979, grad_norm: 0.26059430837631226\n",
            "Training:  77% 173/226 [14:03<03:21,  3.80s/it]loss: 0.18263626098632812, grad_norm: 0.20376193523406982\n",
            "Training:  77% 174/226 [14:07<03:31,  4.06s/it]loss: 0.20575256645679474, grad_norm: 0.2070269137620926\n",
            "Training:  77% 175/226 [14:11<03:29,  4.11s/it]loss: 0.13916729390621185, grad_norm: 0.13166655600070953\n",
            "Training:  78% 176/226 [14:19<04:11,  5.03s/it]loss: 0.13840274512767792, grad_norm: 0.19645096361637115\n",
            "Training:  78% 177/226 [14:23<03:54,  4.79s/it]loss: 0.1596386432647705, grad_norm: 0.20823310315608978\n",
            "Training:  79% 178/226 [14:28<03:59,  5.00s/it]loss: 0.1181735098361969, grad_norm: 0.13340754806995392\n",
            "Training:  79% 179/226 [14:39<05:12,  6.64s/it]loss: 0.1830468624830246, grad_norm: 0.16748520731925964\n",
            "Training:  80% 180/226 [14:44<04:48,  6.28s/it]loss: 0.16368597745895386, grad_norm: 0.20235396921634674\n",
            "Training:  80% 181/226 [14:50<04:40,  6.23s/it]loss: 0.20703192055225372, grad_norm: 0.19132614135742188\n",
            "Training:  81% 182/226 [14:54<03:57,  5.40s/it]loss: 0.181142196059227, grad_norm: 0.18767036497592926\n",
            "Training:  81% 183/226 [15:05<05:08,  7.17s/it]loss: 0.21433784067630768, grad_norm: 0.19666945934295654\n",
            "Training:  81% 184/226 [15:08<04:12,  6.01s/it]loss: 0.2886790335178375, grad_norm: 0.2872407138347626\n",
            "Training:  82% 185/226 [15:10<03:16,  4.80s/it]loss: 0.17607073485851288, grad_norm: 0.1553625613451004\n",
            "Training:  82% 186/226 [15:15<03:11,  4.78s/it]loss: 0.19070447981357574, grad_norm: 0.27525192499160767\n",
            "Training:  83% 187/226 [15:20<03:07,  4.82s/it]loss: 0.22565478086471558, grad_norm: 0.19439764320850372\n",
            "Training:  83% 188/226 [15:23<02:40,  4.22s/it]loss: 0.15232530236244202, grad_norm: 0.16809417307376862\n",
            "Training:  84% 189/226 [15:29<02:52,  4.66s/it]loss: 0.20185615122318268, grad_norm: 0.20925866067409515\n",
            "Training:  84% 190/226 [15:31<02:25,  4.05s/it]loss: 0.13745088875293732, grad_norm: 0.13936151564121246\n",
            "Training:  85% 191/226 [15:37<02:39,  4.56s/it]loss: 0.15454892814159393, grad_norm: 0.13841530680656433\n",
            "Training:  85% 192/226 [15:42<02:43,  4.80s/it]loss: 0.14110012352466583, grad_norm: 0.14954037964344025\n",
            "Training:  85% 193/226 [15:55<03:53,  7.06s/it]loss: 0.24054360389709473, grad_norm: 0.28761789202690125\n",
            "Training:  86% 194/226 [15:57<03:03,  5.74s/it]loss: 0.16995690762996674, grad_norm: 0.15434518456459045\n",
            "Training:  86% 195/226 [16:03<02:55,  5.67s/it]loss: 0.2063576877117157, grad_norm: 0.25480109453201294\n",
            "Training:  87% 196/226 [16:06<02:27,  4.92s/it]loss: 0.1549636423587799, grad_norm: 0.14912965893745422\n",
            "Training:  87% 197/226 [16:10<02:17,  4.72s/it]loss: 0.1759013682603836, grad_norm: 0.15397566556930542\n",
            "Training:  88% 198/226 [16:14<02:07,  4.55s/it]loss: 0.2444005161523819, grad_norm: 0.2010565549135208\n",
            "Training:  88% 199/226 [16:17<01:48,  4.04s/it]loss: 0.21556419134140015, grad_norm: 0.15862725675106049\n",
            "Training:  88% 200/226 [16:21<01:44,  4.02s/it]loss: 0.22406136989593506, grad_norm: 0.1934327781200409\n",
            "Training:  89% 201/226 [16:24<01:31,  3.65s/it]loss: 0.18752145767211914, grad_norm: 0.16985774040222168\n",
            "Training:  89% 202/226 [16:28<01:32,  3.84s/it]loss: 0.32721132040023804, grad_norm: 0.41311365365982056\n",
            "Training:  90% 203/226 [16:37<02:04,  5.39s/it]loss: 0.2295595407485962, grad_norm: 0.20119182765483856\n",
            "Training:  90% 204/226 [16:40<01:38,  4.46s/it]loss: 0.28096309304237366, grad_norm: 0.3313285708427429\n",
            "Training:  91% 205/226 [16:42<01:17,  3.69s/it]loss: 0.17425119876861572, grad_norm: 0.17285838723182678\n",
            "Training:  91% 206/226 [16:49<01:36,  4.83s/it]loss: 0.20687827467918396, grad_norm: 0.21157699823379517\n",
            "Training:  92% 207/226 [16:54<01:33,  4.91s/it]loss: 0.24508953094482422, grad_norm: 0.26989850401878357\n",
            "Training:  92% 208/226 [16:56<01:13,  4.06s/it]loss: 0.1237492561340332, grad_norm: 0.1345030516386032\n",
            "Training:  92% 209/226 [17:03<01:22,  4.87s/it]loss: 0.13844034075737, grad_norm: 0.14335274696350098\n",
            "Training:  93% 210/226 [17:10<01:29,  5.57s/it]loss: 0.2294318825006485, grad_norm: 0.6128079891204834\n",
            "Training:  93% 211/226 [17:13<01:11,  4.76s/it]loss: 0.14532478153705597, grad_norm: 0.13277100026607513\n",
            "Training:  94% 212/226 [17:18<01:06,  4.75s/it]loss: 0.21245789527893066, grad_norm: 0.18547719717025757\n",
            "Training:  94% 213/226 [17:24<01:06,  5.15s/it]loss: 0.18494001030921936, grad_norm: 0.17379750311374664\n",
            "Training:  95% 214/226 [17:29<01:00,  5.05s/it]loss: 0.24287724494934082, grad_norm: 0.24148628115653992\n",
            "Training:  95% 215/226 [17:32<00:49,  4.46s/it]loss: 0.23023530840873718, grad_norm: 0.1896391659975052\n",
            "Training:  96% 216/226 [17:35<00:40,  4.10s/it]loss: 0.19114989042282104, grad_norm: 0.16866157948970795\n",
            "Training:  96% 217/226 [17:41<00:43,  4.82s/it]loss: 0.2013072371482849, grad_norm: 0.1943846195936203\n",
            "Training:  96% 218/226 [17:45<00:34,  4.36s/it]loss: 0.21417196094989777, grad_norm: 0.2401123195886612\n",
            "Training:  97% 219/226 [17:48<00:27,  3.90s/it]loss: 0.1681058406829834, grad_norm: 0.14109712839126587\n",
            "Training:  97% 220/226 [17:54<00:27,  4.61s/it]loss: 0.18188726902008057, grad_norm: 0.21195970475673676\n",
            "Training:  98% 221/226 [17:57<00:20,  4.12s/it]loss: 0.162857323884964, grad_norm: 0.1550767719745636\n",
            "Training:  98% 222/226 [18:02<00:18,  4.50s/it]loss: 0.18767493963241577, grad_norm: 0.14621292054653168\n",
            "Training:  99% 223/226 [18:14<00:20,  6.83s/it]loss: 0.2496861219406128, grad_norm: 0.3018997311592102\n",
            "Training:  99% 224/226 [18:17<00:10,  5.48s/it]loss: 0.16545173525810242, grad_norm: 0.16259944438934326\n",
            "Training: 100% 225/226 [18:21<00:05,  5.04s/it]loss: 0.1600513905286789, grad_norm: 0.14974172413349152\n",
            "Training: 100% 226/226 [18:30<00:00,  4.91s/it]\n",
            "Validation loss: 0.7125454473495484\n",
            "Epoch: 4\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.20130778849124908, grad_norm: 0.24167867004871368\n",
            "Training:   0% 1/226 [00:03<13:15,  3.54s/it]loss: 0.21671634912490845, grad_norm: 0.17298611998558044\n",
            "Training:   1% 2/226 [00:06<11:52,  3.18s/it]loss: 0.1480739265680313, grad_norm: 0.17452086508274078\n",
            "Training:   1% 3/226 [00:13<17:32,  4.72s/it]loss: 0.22117576003074646, grad_norm: 0.22331449389457703\n",
            "Training:   2% 4/226 [00:15<13:40,  3.70s/it]loss: 0.13662396371364594, grad_norm: 0.16496606171131134\n",
            "Training:   2% 5/226 [00:21<17:24,  4.72s/it]loss: 0.11784302443265915, grad_norm: 0.12197748571634293\n",
            "Training:   3% 6/226 [00:30<21:47,  5.94s/it]loss: 0.16685961186885834, grad_norm: 0.144490048289299\n",
            "Training:   3% 7/226 [00:37<23:55,  6.56s/it]loss: 0.1353168785572052, grad_norm: 0.1285010129213333\n",
            "Training:   4% 8/226 [00:43<22:16,  6.13s/it]loss: 0.2229972779750824, grad_norm: 0.19968685507774353\n",
            "Training:   4% 9/226 [00:46<18:55,  5.23s/it]loss: 0.15042082965373993, grad_norm: 0.14226983487606049\n",
            "Training:   4% 10/226 [00:49<17:05,  4.75s/it]loss: 0.17988424003124237, grad_norm: 0.15031275153160095\n",
            "Training:   5% 11/226 [00:54<16:38,  4.64s/it]loss: 0.26537853479385376, grad_norm: 0.29012569785118103\n",
            "Training:   5% 12/226 [00:57<14:28,  4.06s/it]loss: 0.14939822256565094, grad_norm: 0.14796704053878784\n",
            "Training:   6% 13/226 [01:01<15:00,  4.23s/it]loss: 0.16780324280261993, grad_norm: 0.16644322872161865\n",
            "Training:   6% 14/226 [01:05<13:58,  3.95s/it]loss: 0.18601809442043304, grad_norm: 0.18435460329055786\n",
            "Training:   7% 15/226 [01:10<15:27,  4.40s/it]loss: 0.16307269036769867, grad_norm: 0.1390390396118164\n",
            "Training:   7% 16/226 [01:15<15:58,  4.56s/it]loss: 0.14032836258411407, grad_norm: 0.132035031914711\n",
            "Training:   8% 17/226 [01:21<17:57,  5.16s/it]loss: 0.18605005741119385, grad_norm: 0.1750669777393341\n",
            "Training:   8% 18/226 [01:26<17:08,  4.95s/it]loss: 0.13959093391895294, grad_norm: 0.11610749363899231\n",
            "Training:   8% 19/226 [01:31<17:22,  5.04s/it]loss: 0.18635019659996033, grad_norm: 0.11959250271320343\n",
            "Training:   9% 20/226 [01:37<17:56,  5.23s/it]loss: 0.2134438008069992, grad_norm: 0.16252592206001282\n",
            "Training:   9% 21/226 [01:42<17:55,  5.25s/it]loss: 0.2371853142976761, grad_norm: 0.22233395278453827\n",
            "Training:  10% 22/226 [01:45<15:47,  4.64s/it]loss: 0.1507391780614853, grad_norm: 0.11608260869979858\n",
            "Training:  10% 23/226 [01:51<16:40,  4.93s/it]loss: 0.15692642331123352, grad_norm: 0.15237842500209808\n",
            "Training:  11% 24/226 [01:56<16:29,  4.90s/it]loss: 0.1805122196674347, grad_norm: 0.13230940699577332\n",
            "Training:  11% 25/226 [02:01<16:44,  5.00s/it]loss: 0.16935944557189941, grad_norm: 0.18478825688362122\n",
            "Training:  12% 26/226 [02:04<15:03,  4.52s/it]loss: 0.27864521741867065, grad_norm: 0.30138450860977173\n",
            "Training:  12% 27/226 [02:07<13:33,  4.09s/it]loss: 0.22644278407096863, grad_norm: 0.25002986192703247\n",
            "Training:  12% 28/226 [02:11<12:56,  3.92s/it]loss: 0.15705254673957825, grad_norm: 0.15241338312625885\n",
            "Training:  13% 29/226 [02:17<14:40,  4.47s/it]loss: 0.1782362163066864, grad_norm: 0.1464378833770752\n",
            "Training:  13% 30/226 [02:21<14:46,  4.52s/it]loss: 0.18176624178886414, grad_norm: 0.15151594579219818\n",
            "Training:  14% 31/226 [02:25<14:09,  4.36s/it]loss: 0.21682322025299072, grad_norm: 0.19790291786193848\n",
            "Training:  14% 32/226 [02:28<12:24,  3.84s/it]loss: 0.2748422622680664, grad_norm: 0.24490250647068024\n",
            "Training:  15% 33/226 [02:30<10:58,  3.41s/it]loss: 0.20847474038600922, grad_norm: 0.17291270196437836\n",
            "Training:  15% 34/226 [02:36<12:39,  3.96s/it]loss: 0.15785349905490875, grad_norm: 0.17256899178028107\n",
            "Training:  15% 35/226 [02:40<13:15,  4.17s/it]loss: 0.13957636058330536, grad_norm: 0.11124708503484726\n",
            "Training:  16% 36/226 [02:46<14:15,  4.50s/it]loss: 0.13477779924869537, grad_norm: 0.10953143984079361\n",
            "Training:  16% 37/226 [02:58<21:20,  6.78s/it]loss: 0.2574211061000824, grad_norm: 0.20320124924182892\n",
            "Training:  17% 38/226 [03:01<17:49,  5.69s/it]loss: 0.25299564003944397, grad_norm: 0.2859104871749878\n",
            "Training:  17% 39/226 [03:03<14:22,  4.61s/it]loss: 0.1158917024731636, grad_norm: 0.1155816987156868\n",
            "Training:  18% 40/226 [03:10<17:02,  5.50s/it]loss: 0.11934991925954819, grad_norm: 0.09874425083398819\n",
            "Training:  18% 41/226 [03:20<20:45,  6.73s/it]loss: 0.12734881043434143, grad_norm: 0.14774876832962036\n",
            "Training:  19% 42/226 [03:26<20:20,  6.63s/it]loss: 0.25292348861694336, grad_norm: 0.2403489649295807\n",
            "Training:  19% 43/226 [03:29<16:54,  5.55s/it]loss: 0.15283720195293427, grad_norm: 0.1559848040342331\n",
            "Training:  19% 44/226 [03:35<16:28,  5.43s/it]loss: 0.2537912130355835, grad_norm: 0.24353598058223724\n",
            "Training:  20% 45/226 [03:37<13:26,  4.46s/it]loss: 0.1686985045671463, grad_norm: 0.13970358669757843\n",
            "Training:  20% 46/226 [03:41<13:29,  4.50s/it]loss: 0.19840240478515625, grad_norm: 0.20456728339195251\n",
            "Training:  21% 47/226 [03:51<17:51,  5.99s/it]loss: 0.1518690139055252, grad_norm: 0.1237376257777214\n",
            "Training:  21% 48/226 [03:56<17:16,  5.82s/it]loss: 0.1755952686071396, grad_norm: 0.12817202508449554\n",
            "Training:  22% 49/226 [04:05<19:27,  6.60s/it]loss: 0.2363588660955429, grad_norm: 0.2407124787569046\n",
            "Training:  22% 50/226 [04:07<15:16,  5.21s/it]loss: 0.27329328656196594, grad_norm: 0.4101254343986511\n",
            "Training:  23% 51/226 [04:09<12:21,  4.24s/it]loss: 0.19862252473831177, grad_norm: 0.18216027319431305\n",
            "Training:  23% 52/226 [04:13<12:32,  4.33s/it]loss: 0.19894041121006012, grad_norm: 0.15699103474617004\n",
            "Training:  23% 53/226 [04:16<11:21,  3.94s/it]loss: 0.23595985770225525, grad_norm: 0.2534753382205963\n",
            "Training:  24% 54/226 [04:19<09:50,  3.43s/it]loss: 0.15632058680057526, grad_norm: 0.36990898847579956\n",
            "Training:  24% 55/226 [04:24<11:26,  4.01s/it]loss: 0.19368834793567657, grad_norm: 0.1998758763074875\n",
            "Training:  25% 56/226 [04:26<10:07,  3.57s/it]loss: 0.196050226688385, grad_norm: 0.28403836488723755\n",
            "Training:  25% 57/226 [04:34<13:21,  4.74s/it]loss: 0.25300559401512146, grad_norm: 0.23091906309127808\n",
            "Training:  26% 58/226 [04:37<12:02,  4.30s/it]loss: 0.2195010632276535, grad_norm: 0.2022293359041214\n",
            "Training:  26% 59/226 [04:40<10:44,  3.86s/it]loss: 0.18863435089588165, grad_norm: 0.20535381138324738\n",
            "Training:  27% 60/226 [04:44<10:58,  3.97s/it]loss: 0.1482747495174408, grad_norm: 0.14596758782863617\n",
            "Training:  27% 61/226 [04:49<11:45,  4.28s/it]loss: 0.21857482194900513, grad_norm: 0.1746922880411148\n",
            "Training:  27% 62/226 [04:53<11:24,  4.17s/it]loss: 0.14856502413749695, grad_norm: 0.12270151823759079\n",
            "Training:  28% 63/226 [05:00<13:20,  4.91s/it]loss: 0.2747819423675537, grad_norm: 0.2340233027935028\n",
            "Training:  28% 64/226 [05:02<11:19,  4.20s/it]loss: 0.12421239912509918, grad_norm: 0.12433051317930222\n",
            "Training:  29% 65/226 [05:10<13:42,  5.11s/it]loss: 0.13927792012691498, grad_norm: 0.2007325440645218\n",
            "Training:  29% 66/226 [05:14<13:15,  4.97s/it]loss: 0.16340334713459015, grad_norm: 0.13670732080936432\n",
            "Training:  30% 67/226 [05:24<16:44,  6.32s/it]loss: 0.18426603078842163, grad_norm: 0.16164252161979675\n",
            "Training:  30% 68/226 [05:29<15:52,  6.03s/it]loss: 0.15705417096614838, grad_norm: 0.15102127194404602\n",
            "Training:  31% 69/226 [05:34<14:55,  5.71s/it]loss: 0.15415966510772705, grad_norm: 0.13478633761405945\n",
            "Training:  31% 70/226 [05:38<13:52,  5.34s/it]loss: 0.32792478799819946, grad_norm: 0.4240301549434662\n",
            "Training:  31% 71/226 [05:40<10:53,  4.22s/it]loss: 0.14632035791873932, grad_norm: 0.15438933670520782\n",
            "Training:  32% 72/226 [05:47<12:54,  5.03s/it]loss: 0.19552257657051086, grad_norm: 0.18948084115982056\n",
            "Training:  32% 73/226 [05:53<13:47,  5.41s/it]loss: 0.25774946808815, grad_norm: 0.2441474348306656\n",
            "Training:  33% 74/226 [05:56<11:52,  4.69s/it]loss: 0.20129983127117157, grad_norm: 0.20282189548015594\n",
            "Training:  33% 75/226 [06:01<11:42,  4.65s/it]loss: 0.18322713673114777, grad_norm: 0.173053577542305\n",
            "Training:  34% 76/226 [06:05<11:00,  4.41s/it]loss: 0.18068350851535797, grad_norm: 0.17844392359256744\n",
            "Training:  34% 77/226 [06:14<14:49,  5.97s/it]loss: 0.2988627254962921, grad_norm: 0.2471436709165573\n",
            "Training:  35% 78/226 [06:17<12:03,  4.89s/it]loss: 0.2195158749818802, grad_norm: 0.20320990681648254\n",
            "Training:  35% 79/226 [06:19<10:27,  4.27s/it]loss: 0.2299010008573532, grad_norm: 0.19181644916534424\n",
            "Training:  35% 80/226 [06:24<10:41,  4.39s/it]loss: 0.16726580262184143, grad_norm: 0.13433697819709778\n",
            "Training:  36% 81/226 [06:30<11:32,  4.78s/it]loss: 0.14938193559646606, grad_norm: 0.12721963226795197\n",
            "Training:  36% 82/226 [06:35<11:27,  4.77s/it]loss: 0.14056825637817383, grad_norm: 0.1720103919506073\n",
            "Training:  37% 83/226 [06:40<11:30,  4.83s/it]loss: 0.14543558657169342, grad_norm: 0.1430673897266388\n",
            "Training:  37% 84/226 [06:44<10:48,  4.57s/it]loss: 0.23340673744678497, grad_norm: 0.20746168494224548\n",
            "Training:  38% 85/226 [06:48<10:37,  4.52s/it]loss: 0.2069852352142334, grad_norm: 0.23373714089393616\n",
            "Training:  38% 86/226 [06:50<09:02,  3.88s/it]loss: 0.20121215283870697, grad_norm: 0.21147064864635468\n",
            "Training:  38% 87/226 [07:03<15:25,  6.66s/it]loss: 0.15660889446735382, grad_norm: 0.15850412845611572\n",
            "Training:  39% 88/226 [07:09<14:22,  6.25s/it]loss: 0.1650836318731308, grad_norm: 0.15758833289146423\n",
            "Training:  39% 89/226 [07:14<13:48,  6.05s/it]loss: 0.13102255761623383, grad_norm: 0.12429747730493546\n",
            "Training:  40% 90/226 [07:20<13:42,  6.05s/it]loss: 0.15111874043941498, grad_norm: 0.18009015917778015\n",
            "Training:  40% 91/226 [07:25<12:27,  5.54s/it]loss: 0.19253484904766083, grad_norm: 0.19136297702789307\n",
            "Training:  41% 92/226 [07:28<11:10,  5.00s/it]loss: 0.1433565616607666, grad_norm: 0.15398502349853516\n",
            "Training:  41% 93/226 [07:33<10:50,  4.89s/it]loss: 0.10671067237854004, grad_norm: 0.10799236595630646\n",
            "Training:  42% 94/226 [07:43<14:12,  6.46s/it]loss: 0.2243937999010086, grad_norm: 0.22639508545398712\n",
            "Training:  42% 95/226 [07:48<13:01,  5.97s/it]loss: 0.18687058985233307, grad_norm: 0.17163699865341187\n",
            "Training:  42% 96/226 [07:53<12:27,  5.75s/it]loss: 0.19453047215938568, grad_norm: 0.18222831189632416\n",
            "Training:  43% 97/226 [08:04<15:41,  7.30s/it]loss: 0.2105715423822403, grad_norm: 0.1790228635072708\n",
            "Training:  43% 98/226 [08:09<14:00,  6.57s/it]loss: 0.2969686686992645, grad_norm: 0.2969607710838318\n",
            "Training:  44% 99/226 [08:11<10:51,  5.13s/it]loss: 0.12564539909362793, grad_norm: 0.13013413548469543\n",
            "Training:  44% 100/226 [08:20<13:19,  6.34s/it]loss: 0.15433959662914276, grad_norm: 0.13443155586719513\n",
            "Training:  45% 101/226 [08:26<13:18,  6.39s/it]loss: 0.1693332940340042, grad_norm: 0.1766810417175293\n",
            "Training:  45% 102/226 [08:31<12:05,  5.85s/it]loss: 0.15755924582481384, grad_norm: 0.16571691632270813\n",
            "Training:  46% 103/226 [08:36<11:35,  5.65s/it]loss: 0.17574335634708405, grad_norm: 0.1548164337873459\n",
            "Training:  46% 104/226 [08:41<11:07,  5.47s/it]loss: 0.16868877410888672, grad_norm: 0.16801103949546814\n",
            "Training:  46% 105/226 [08:46<10:25,  5.17s/it]loss: 0.22174163162708282, grad_norm: 0.18630869686603546\n",
            "Training:  47% 106/226 [08:49<09:08,  4.57s/it]loss: 0.30131858587265015, grad_norm: 0.3063793182373047\n",
            "Training:  47% 107/226 [08:52<07:51,  3.97s/it]loss: 0.23343360424041748, grad_norm: 0.19914890825748444\n",
            "Training:  48% 108/226 [08:54<07:02,  3.58s/it]loss: 0.21377119421958923, grad_norm: 0.16740529239177704\n",
            "Training:  48% 109/226 [09:00<08:16,  4.25s/it]loss: 0.20175540447235107, grad_norm: 0.1809142380952835\n",
            "Training:  49% 110/226 [09:04<07:57,  4.12s/it]loss: 0.1483052372932434, grad_norm: 0.15505605936050415\n",
            "Training:  49% 111/226 [09:09<08:39,  4.52s/it]loss: 0.17954668402671814, grad_norm: 0.15943218767642975\n",
            "Training:  50% 112/226 [09:14<08:29,  4.47s/it]loss: 0.1385945975780487, grad_norm: 0.1270696222782135\n",
            "Training:  50% 113/226 [09:20<09:18,  4.94s/it]loss: 0.16933761537075043, grad_norm: 0.15208382904529572\n",
            "Training:  50% 114/226 [09:26<10:11,  5.46s/it]loss: 0.19857646524906158, grad_norm: 0.16029326617717743\n",
            "Training:  51% 115/226 [09:30<09:04,  4.91s/it]loss: 0.16924604773521423, grad_norm: 0.14574700593948364\n",
            "Training:  51% 116/226 [09:34<08:34,  4.67s/it]loss: 0.155109703540802, grad_norm: 0.14006078243255615\n",
            "Training:  52% 117/226 [09:45<12:05,  6.65s/it]loss: 0.13915742933750153, grad_norm: 0.12212958186864853\n",
            "Training:  52% 118/226 [09:52<11:56,  6.64s/it]loss: 0.19085118174552917, grad_norm: 0.17629119753837585\n",
            "Training:  53% 119/226 [09:56<10:36,  5.94s/it]loss: 0.14836660027503967, grad_norm: 0.16051340103149414\n",
            "Training:  53% 120/226 [10:01<09:57,  5.64s/it]loss: 0.17763926088809967, grad_norm: 0.1834518164396286\n",
            "Training:  54% 121/226 [10:05<08:52,  5.07s/it]loss: 0.1764620691537857, grad_norm: 0.13530607521533966\n",
            "Training:  54% 122/226 [10:10<08:42,  5.02s/it]loss: 0.19613632559776306, grad_norm: 0.3425547778606415\n",
            "Training:  54% 123/226 [10:13<07:36,  4.43s/it]loss: 0.18615008890628815, grad_norm: 0.1861858367919922\n",
            "Training:  55% 124/226 [10:17<07:09,  4.21s/it]loss: 0.17360819876194, grad_norm: 0.1625930219888687\n",
            "Training:  55% 125/226 [10:21<07:08,  4.24s/it]loss: 0.185219869017601, grad_norm: 0.18848969042301178\n",
            "Training:  56% 126/226 [10:24<06:33,  3.93s/it]loss: 0.20116382837295532, grad_norm: 0.17051514983177185\n",
            "Training:  56% 127/226 [10:37<10:44,  6.51s/it]loss: 0.16569434106349945, grad_norm: 0.14025264978408813\n",
            "Training:  57% 128/226 [10:42<10:09,  6.22s/it]loss: 0.12574641406536102, grad_norm: 0.13156674802303314\n",
            "Training:  57% 129/226 [10:49<10:20,  6.40s/it]loss: 0.17887282371520996, grad_norm: 0.14240765571594238\n",
            "Training:  58% 130/226 [10:53<09:14,  5.78s/it]loss: 0.18882346153259277, grad_norm: 0.3117969334125519\n",
            "Training:  58% 131/226 [10:58<08:34,  5.41s/it]loss: 0.22919833660125732, grad_norm: 0.21237148344516754\n",
            "Training:  58% 132/226 [11:01<07:09,  4.57s/it]loss: 0.2235955446958542, grad_norm: 0.2179240882396698\n",
            "Training:  59% 133/226 [11:03<06:17,  4.06s/it]loss: 0.1359557956457138, grad_norm: 0.17093577980995178\n",
            "Training:  59% 134/226 [11:08<06:23,  4.16s/it]loss: 0.14561457931995392, grad_norm: 0.1423993557691574\n",
            "Training:  60% 135/226 [11:12<06:28,  4.27s/it]loss: 0.2489819973707199, grad_norm: 0.250895619392395\n",
            "Training:  60% 136/226 [11:15<05:54,  3.94s/it]loss: 0.1839570552110672, grad_norm: 0.16976530849933624\n",
            "Training:  61% 137/226 [11:24<07:48,  5.27s/it]loss: 0.17865395545959473, grad_norm: 0.1924174576997757\n",
            "Training:  61% 138/226 [11:27<06:42,  4.57s/it]loss: 0.2770538330078125, grad_norm: 0.26273027062416077\n",
            "Training:  62% 139/226 [11:30<05:53,  4.07s/it]loss: 0.25871747732162476, grad_norm: 0.25963911414146423\n",
            "Training:  62% 140/226 [11:32<05:15,  3.66s/it]loss: 0.14308106899261475, grad_norm: 0.15881769359111786\n",
            "Training:  62% 141/226 [11:36<05:16,  3.73s/it]loss: 0.20708240568637848, grad_norm: 0.18278929591178894\n",
            "Training:  63% 142/226 [11:42<06:11,  4.42s/it]loss: 0.13114525377750397, grad_norm: 0.12062333524227142\n",
            "Training:  63% 143/226 [11:52<08:06,  5.87s/it]loss: 0.12667781114578247, grad_norm: 0.13560478389263153\n",
            "Training:  64% 144/226 [11:59<08:41,  6.36s/it]loss: 0.16926521062850952, grad_norm: 0.16047103703022003\n",
            "Training:  64% 145/226 [12:03<07:44,  5.74s/it]loss: 0.23884476721286774, grad_norm: 0.2171253114938736\n",
            "Training:  65% 146/226 [12:06<06:21,  4.77s/it]loss: 0.11867308616638184, grad_norm: 0.1332864612340927\n",
            "Training:  65% 147/226 [12:19<09:42,  7.38s/it]loss: 0.24294689297676086, grad_norm: 0.28866133093833923\n",
            "Training:  65% 148/226 [12:22<07:51,  6.05s/it]loss: 0.21919313073158264, grad_norm: 0.1968255639076233\n",
            "Training:  66% 149/226 [12:25<06:31,  5.09s/it]loss: 0.14193642139434814, grad_norm: 0.13558441400527954\n",
            "Training:  66% 150/226 [12:32<07:04,  5.58s/it]loss: 0.2505219280719757, grad_norm: 0.19388139247894287\n",
            "Training:  67% 151/226 [12:35<06:10,  4.94s/it]loss: 0.22371481359004974, grad_norm: 0.20487867295742035\n",
            "Training:  67% 152/226 [12:38<05:23,  4.37s/it]loss: 0.20805928111076355, grad_norm: 0.17326578497886658\n",
            "Training:  68% 153/226 [12:43<05:29,  4.51s/it]loss: 0.21724118292331696, grad_norm: 0.22422955930233002\n",
            "Training:  68% 154/226 [12:48<05:34,  4.65s/it]loss: 0.16026440262794495, grad_norm: 0.13748352229595184\n",
            "Training:  69% 155/226 [12:57<06:55,  5.86s/it]loss: 0.2108519822359085, grad_norm: 0.1555211991071701\n",
            "Training:  69% 156/226 [13:00<06:01,  5.17s/it]loss: 0.24864879250526428, grad_norm: 0.18504880368709564\n",
            "Training:  69% 157/226 [13:12<08:16,  7.20s/it]loss: 0.12189562618732452, grad_norm: 0.11636090278625488\n",
            "Training:  70% 158/226 [13:20<08:14,  7.27s/it]loss: 0.16878260672092438, grad_norm: 0.14085088670253754\n",
            "Training:  70% 159/226 [13:24<06:56,  6.22s/it]loss: 0.20878657698631287, grad_norm: 0.22618743777275085\n",
            "Training:  71% 160/226 [13:27<05:50,  5.31s/it]loss: 0.19522228837013245, grad_norm: 0.13083013892173767\n",
            "Training:  71% 161/226 [13:31<05:29,  5.06s/it]loss: 0.2293846756219864, grad_norm: 0.2320498377084732\n",
            "Training:  72% 162/226 [13:34<04:44,  4.44s/it]loss: 0.16086040437221527, grad_norm: 0.17487257719039917\n",
            "Training:  72% 163/226 [13:40<04:57,  4.72s/it]loss: 0.218052938580513, grad_norm: 0.18490846455097198\n",
            "Training:  73% 164/226 [13:44<04:52,  4.72s/it]loss: 0.17988404631614685, grad_norm: 0.1362529993057251\n",
            "Training:  73% 165/226 [13:48<04:35,  4.51s/it]loss: 0.22812172770500183, grad_norm: 0.22327159345149994\n",
            "Training:  73% 166/226 [13:51<03:57,  3.97s/it]loss: 0.2196122705936432, grad_norm: 0.16224561631679535\n",
            "Training:  74% 167/226 [14:04<06:26,  6.55s/it]loss: 0.2420206218957901, grad_norm: 0.1988806128501892\n",
            "Training:  74% 168/226 [14:07<05:26,  5.62s/it]loss: 0.14542199671268463, grad_norm: 0.14452368021011353\n",
            "Training:  75% 169/226 [14:13<05:28,  5.77s/it]loss: 0.12806794047355652, grad_norm: 0.12925677001476288\n",
            "Training:  75% 170/226 [14:20<05:36,  6.01s/it]loss: 0.2259751707315445, grad_norm: 0.20393604040145874\n",
            "Training:  76% 171/226 [14:24<04:58,  5.42s/it]loss: 0.14169098436832428, grad_norm: 0.11903417110443115\n",
            "Training:  76% 172/226 [14:33<05:55,  6.58s/it]loss: 0.17604537308216095, grad_norm: 0.15619096159934998\n",
            "Training:  77% 173/226 [14:40<05:49,  6.60s/it]loss: 0.17353981733322144, grad_norm: 0.1262771636247635\n",
            "Training:  77% 174/226 [14:45<05:24,  6.24s/it]loss: 0.23628388345241547, grad_norm: 0.2943076193332672\n",
            "Training:  77% 175/226 [14:48<04:21,  5.13s/it]loss: 0.20336872339248657, grad_norm: 0.1553022563457489\n",
            "Training:  78% 176/226 [14:52<03:59,  4.79s/it]loss: 0.18779516220092773, grad_norm: 0.18514488637447357\n",
            "Training:  78% 177/226 [14:58<04:13,  5.17s/it]loss: 0.17473675310611725, grad_norm: 0.14300639927387238\n",
            "Training:  79% 178/226 [15:02<03:49,  4.79s/it]loss: 0.11566781997680664, grad_norm: 0.10472097992897034\n",
            "Training:  79% 179/226 [15:10<04:41,  6.00s/it]loss: 0.2671317458152771, grad_norm: 0.22607119381427765\n",
            "Training:  80% 180/226 [15:13<03:45,  4.90s/it]loss: 0.12977781891822815, grad_norm: 0.18887531757354736\n",
            "Training:  80% 181/226 [15:20<04:06,  5.48s/it]loss: 0.1488790512084961, grad_norm: 0.12536406517028809\n",
            "Training:  81% 182/226 [15:25<04:06,  5.59s/it]loss: 0.17544639110565186, grad_norm: 0.1534530073404312\n",
            "Training:  81% 183/226 [15:30<03:46,  5.27s/it]loss: 0.11911211907863617, grad_norm: 0.11192836612462997\n",
            "Training:  81% 184/226 [15:39<04:31,  6.46s/it]loss: 0.1416049748659134, grad_norm: 0.1528148651123047\n",
            "Training:  82% 185/226 [15:45<04:13,  6.19s/it]loss: 0.15137629210948944, grad_norm: 0.13227908313274384\n",
            "Training:  82% 186/226 [15:55<04:50,  7.27s/it]loss: 0.2389553189277649, grad_norm: 0.19792897999286652\n",
            "Training:  83% 187/226 [16:04<05:14,  8.06s/it]loss: 0.15594005584716797, grad_norm: 0.15010127425193787\n",
            "Training:  83% 188/226 [16:09<04:30,  7.13s/it]loss: 0.18467018008232117, grad_norm: 0.18971720337867737\n",
            "Training:  84% 189/226 [16:13<03:48,  6.16s/it]loss: 0.131906196475029, grad_norm: 0.14521729946136475\n",
            "Training:  84% 190/226 [16:19<03:33,  5.93s/it]loss: 0.16602128744125366, grad_norm: 0.16748467087745667\n",
            "Training:  85% 191/226 [16:24<03:25,  5.86s/it]loss: 0.3054717481136322, grad_norm: 0.3356361389160156\n",
            "Training:  85% 192/226 [16:26<02:37,  4.62s/it]loss: 0.23016442358493805, grad_norm: 0.2643777132034302\n",
            "Training:  85% 193/226 [16:29<02:17,  4.16s/it]loss: 0.1511750966310501, grad_norm: 0.15685422718524933\n",
            "Training:  86% 194/226 [16:35<02:24,  4.51s/it]loss: 0.21526405215263367, grad_norm: 0.1996246725320816\n",
            "Training:  86% 195/226 [16:38<02:07,  4.12s/it]loss: 0.20713728666305542, grad_norm: 0.18825507164001465\n",
            "Training:  87% 196/226 [16:41<01:52,  3.76s/it]loss: 0.1697360724210739, grad_norm: 0.15304023027420044\n",
            "Training:  87% 197/226 [16:52<02:52,  5.95s/it]loss: 0.1798766851425171, grad_norm: 0.18642309308052063\n",
            "Training:  88% 198/226 [16:56<02:36,  5.59s/it]loss: 0.1782686859369278, grad_norm: 0.18617312610149384\n",
            "Training:  88% 199/226 [17:00<02:14,  5.00s/it]loss: 0.3199862837791443, grad_norm: 0.3426336348056793\n",
            "Training:  88% 200/226 [17:02<01:43,  3.99s/it]loss: 0.20560137927532196, grad_norm: 0.1548394113779068\n",
            "Training:  89% 201/226 [17:06<01:41,  4.05s/it]loss: 0.13374827802181244, grad_norm: 0.16364380717277527\n",
            "Training:  89% 202/226 [17:12<01:51,  4.63s/it]loss: 0.18762360513210297, grad_norm: 0.20472246408462524\n",
            "Training:  90% 203/226 [17:17<01:48,  4.72s/it]loss: 0.22114671766757965, grad_norm: 0.19746175408363342\n",
            "Training:  90% 204/226 [17:20<01:32,  4.19s/it]loss: 0.1973685473203659, grad_norm: 0.16343015432357788\n",
            "Training:  91% 205/226 [17:23<01:23,  3.96s/it]loss: 0.21080495417118073, grad_norm: 0.28017178177833557\n",
            "Training:  91% 206/226 [17:26<01:10,  3.52s/it]loss: 0.21139229834079742, grad_norm: 0.22913849353790283\n",
            "Training:  92% 207/226 [17:37<01:50,  5.79s/it]loss: 0.14830105006694794, grad_norm: 0.11928805708885193\n",
            "Training:  92% 208/226 [17:44<01:54,  6.34s/it]loss: 0.24711395800113678, grad_norm: 0.17626546323299408\n",
            "Training:  92% 209/226 [17:50<01:41,  5.97s/it]loss: 0.20455336570739746, grad_norm: 0.2743978500366211\n",
            "Training:  93% 210/226 [17:52<01:19,  4.99s/it]loss: 0.19252045452594757, grad_norm: 0.16727091372013092\n",
            "Training:  93% 211/226 [17:56<01:08,  4.59s/it]loss: 0.16725528240203857, grad_norm: 0.141824409365654\n",
            "Training:  94% 212/226 [18:00<01:02,  4.50s/it]loss: 0.2535177171230316, grad_norm: 0.26455050706863403\n",
            "Training:  94% 213/226 [18:03<00:51,  3.93s/it]loss: 0.1418365240097046, grad_norm: 0.13411560654640198\n",
            "Training:  95% 214/226 [18:08<00:52,  4.38s/it]loss: 0.260417640209198, grad_norm: 0.2353798747062683\n",
            "Training:  95% 215/226 [18:10<00:41,  3.73s/it]loss: 0.22105060517787933, grad_norm: 0.17617885768413544\n",
            "Training:  96% 216/226 [18:15<00:39,  3.94s/it]loss: 0.24556301534175873, grad_norm: 0.23973512649536133\n",
            "Training:  96% 217/226 [18:24<00:49,  5.48s/it]loss: 0.1405138373374939, grad_norm: 0.13509772717952728\n",
            "Training:  96% 218/226 [18:27<00:39,  4.91s/it]loss: 0.2748397886753082, grad_norm: 0.2902197539806366\n",
            "Training:  97% 219/226 [18:29<00:28,  4.01s/it]loss: 0.1752726137638092, grad_norm: 0.18481306731700897\n",
            "Training:  97% 220/226 [18:34<00:25,  4.29s/it]loss: 0.2093256115913391, grad_norm: 0.17005619406700134\n",
            "Training:  98% 221/226 [18:38<00:20,  4.15s/it]loss: 0.2562985420227051, grad_norm: 0.22295740246772766\n",
            "Training:  98% 222/226 [18:40<00:13,  3.48s/it]loss: 0.2160823494195938, grad_norm: 0.22246992588043213\n",
            "Training:  99% 223/226 [18:45<00:11,  3.85s/it]loss: 0.20470823347568512, grad_norm: 0.19365987181663513\n",
            "Training:  99% 224/226 [18:48<00:07,  3.75s/it]loss: 0.17621304094791412, grad_norm: 0.16047851741313934\n",
            "Training: 100% 225/226 [18:53<00:03,  3.95s/it]loss: 0.20430096983909607, grad_norm: 0.19156384468078613\n",
            "Training: 100% 226/226 [18:56<00:00,  5.03s/it]\n",
            "Validation loss: 0.7296499633789062\n",
            "Epoch: 5\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.1577971875667572, grad_norm: 0.14371143281459808\n",
            "Training:   0% 1/226 [00:06<24:43,  6.59s/it]loss: 0.16230760514736176, grad_norm: 0.14077402651309967\n",
            "Training:   1% 2/226 [00:11<21:26,  5.74s/it]loss: 0.17166219651699066, grad_norm: 0.1956987828016281\n",
            "Training:   1% 3/226 [00:15<18:13,  4.90s/it]loss: 0.20939472317695618, grad_norm: 0.14305974543094635\n",
            "Training:   2% 4/226 [00:22<20:20,  5.50s/it]loss: 0.18729008734226227, grad_norm: 0.18813522160053253\n",
            "Training:   2% 5/226 [00:25<17:35,  4.78s/it]loss: 0.1614742875099182, grad_norm: 0.1627591848373413\n",
            "Training:   3% 6/226 [00:29<16:06,  4.39s/it]loss: 0.16631482541561127, grad_norm: 0.16700254380702972\n",
            "Training:   3% 7/226 [00:33<16:30,  4.52s/it]loss: 0.25167611241340637, grad_norm: 0.2123909443616867\n",
            "Training:   4% 8/226 [00:36<14:09,  3.90s/it]loss: 0.22336092591285706, grad_norm: 0.6564357876777649\n",
            "Training:   4% 9/226 [00:38<12:15,  3.39s/it]loss: 0.15079376101493835, grad_norm: 0.1518702507019043\n",
            "Training:   4% 10/226 [00:43<13:50,  3.84s/it]loss: 0.242890402674675, grad_norm: 0.24238914251327515\n",
            "Training:   5% 11/226 [00:47<14:15,  3.98s/it]loss: 0.27965056896209717, grad_norm: 0.27197587490081787\n",
            "Training:   5% 12/226 [00:49<11:48,  3.31s/it]loss: 0.20567166805267334, grad_norm: 0.15150868892669678\n",
            "Training:   6% 13/226 [00:54<13:16,  3.74s/it]loss: 0.2477429062128067, grad_norm: 0.23838220536708832\n",
            "Training:   6% 14/226 [00:57<11:57,  3.39s/it]loss: 0.19548934698104858, grad_norm: 0.20062629878520966\n",
            "Training:   7% 15/226 [01:00<12:18,  3.50s/it]loss: 0.2009306699037552, grad_norm: 0.1842694729566574\n",
            "Training:   7% 16/226 [01:04<12:38,  3.61s/it]loss: 0.12123623490333557, grad_norm: 0.13279218971729279\n",
            "Training:   8% 17/226 [01:15<19:58,  5.74s/it]loss: 0.1641254872083664, grad_norm: 0.18048842251300812\n",
            "Training:   8% 18/226 [01:19<17:46,  5.13s/it]loss: 0.22612543404102325, grad_norm: 0.2147100865840912\n",
            "Training:   8% 19/226 [01:21<14:58,  4.34s/it]loss: 0.24389544129371643, grad_norm: 0.2471439391374588\n",
            "Training:   9% 20/226 [01:24<12:58,  3.78s/it]loss: 0.20418213307857513, grad_norm: 0.17391856014728546\n",
            "Training:   9% 21/226 [01:30<15:28,  4.53s/it]loss: 0.17771638929843903, grad_norm: 0.15188293159008026\n",
            "Training:  10% 22/226 [01:34<14:50,  4.36s/it]loss: 0.12830260396003723, grad_norm: 0.12521269917488098\n",
            "Training:  10% 23/226 [01:39<15:51,  4.69s/it]loss: 0.25055739283561707, grad_norm: 0.2095886617898941\n",
            "Training:  11% 24/226 [01:42<14:10,  4.21s/it]loss: 0.17800793051719666, grad_norm: 0.2392835170030594\n",
            "Training:  11% 25/226 [01:48<15:41,  4.69s/it]loss: 0.2060869187116623, grad_norm: 0.1482507586479187\n",
            "Training:  12% 26/226 [01:55<18:02,  5.41s/it]loss: 0.2930881381034851, grad_norm: 0.3287636339664459\n",
            "Training:  12% 27/226 [01:57<14:19,  4.32s/it]loss: 0.19958917796611786, grad_norm: 0.2729943096637726\n",
            "Training:  12% 28/226 [02:02<14:41,  4.45s/it]loss: 0.15376494824886322, grad_norm: 0.1463368535041809\n",
            "Training:  13% 29/226 [02:07<15:16,  4.65s/it]loss: 0.15018533170223236, grad_norm: 0.11905693262815475\n",
            "Training:  13% 30/226 [02:14<17:34,  5.38s/it]loss: 0.15022709965705872, grad_norm: 0.14981742203235626\n",
            "Training:  14% 31/226 [02:25<22:51,  7.03s/it]loss: 0.26609644293785095, grad_norm: 0.23791532218456268\n",
            "Training:  14% 32/226 [02:28<18:42,  5.79s/it]loss: 0.26090529561042786, grad_norm: 0.28939080238342285\n",
            "Training:  15% 33/226 [02:31<15:51,  4.93s/it]loss: 0.10720707476139069, grad_norm: 0.1411617547273636\n",
            "Training:  15% 34/226 [02:39<18:49,  5.88s/it]loss: 0.172479048371315, grad_norm: 0.20721465349197388\n",
            "Training:  15% 35/226 [02:44<17:47,  5.59s/it]loss: 0.16968412697315216, grad_norm: 0.14514411985874176\n",
            "Training:  16% 36/226 [02:48<16:37,  5.25s/it]loss: 0.15916511416435242, grad_norm: 0.14962148666381836\n",
            "Training:  16% 37/226 [02:53<16:16,  5.17s/it]loss: 0.2595958113670349, grad_norm: 0.2590954005718231\n",
            "Training:  17% 38/226 [02:55<13:11,  4.21s/it]loss: 0.23517081141471863, grad_norm: 0.24807032942771912\n",
            "Training:  17% 39/226 [02:59<12:37,  4.05s/it]loss: 0.14475542306900024, grad_norm: 0.1549835503101349\n",
            "Training:  18% 40/226 [03:03<12:55,  4.17s/it]loss: 0.15530847012996674, grad_norm: 0.15244384109973907\n",
            "Training:  18% 41/226 [03:14<19:01,  6.17s/it]loss: 0.16432714462280273, grad_norm: 0.17197968065738678\n",
            "Training:  19% 42/226 [03:19<17:43,  5.78s/it]loss: 0.1540473848581314, grad_norm: 0.11955329030752182\n",
            "Training:  19% 43/226 [03:25<17:45,  5.82s/it]loss: 0.1408761441707611, grad_norm: 0.1385328769683838\n",
            "Training:  19% 44/226 [03:30<16:57,  5.59s/it]loss: 0.206590473651886, grad_norm: 0.18316668272018433\n",
            "Training:  20% 45/226 [03:35<16:05,  5.34s/it]loss: 0.16954714059829712, grad_norm: 0.1437569260597229\n",
            "Training:  20% 46/226 [03:44<19:34,  6.52s/it]loss: 0.20135289430618286, grad_norm: 0.18011006712913513\n",
            "Training:  21% 47/226 [03:50<18:45,  6.29s/it]loss: 0.176217719912529, grad_norm: 0.14259187877178192\n",
            "Training:  21% 48/226 [03:55<17:35,  5.93s/it]loss: 0.18755103647708893, grad_norm: 0.1350880116224289\n",
            "Training:  22% 49/226 [03:58<15:12,  5.15s/it]loss: 0.14707207679748535, grad_norm: 0.14647546410560608\n",
            "Training:  22% 50/226 [04:02<14:22,  4.90s/it]loss: 0.2156641036272049, grad_norm: 0.24128985404968262\n",
            "Training:  23% 51/226 [04:11<17:49,  6.11s/it]loss: 0.20018523931503296, grad_norm: 0.16482822597026825\n",
            "Training:  23% 52/226 [04:15<15:13,  5.25s/it]loss: 0.3281826972961426, grad_norm: 0.32349491119384766\n",
            "Training:  23% 53/226 [04:16<11:58,  4.15s/it]loss: 0.1426316201686859, grad_norm: 0.12710751593112946\n",
            "Training:  24% 54/226 [04:24<14:51,  5.19s/it]loss: 0.18968553841114044, grad_norm: 0.16836076974868774\n",
            "Training:  24% 55/226 [04:28<13:43,  4.81s/it]loss: 0.1501377373933792, grad_norm: 0.14213691651821136\n",
            "Training:  25% 56/226 [04:33<14:17,  5.05s/it]loss: 0.226225346326828, grad_norm: 0.20191708207130432\n",
            "Training:  25% 57/226 [04:37<13:05,  4.65s/it]loss: 0.16512706875801086, grad_norm: 0.1655062437057495\n",
            "Training:  26% 58/226 [04:42<13:05,  4.68s/it]loss: 0.20453697443008423, grad_norm: 0.17264407873153687\n",
            "Training:  26% 59/226 [04:46<12:40,  4.55s/it]loss: 0.159603089094162, grad_norm: 0.1970425248146057\n",
            "Training:  27% 60/226 [04:51<13:18,  4.81s/it]loss: 0.2372720092535019, grad_norm: 0.2180025726556778\n",
            "Training:  27% 61/226 [05:03<18:25,  6.70s/it]loss: 0.19787853956222534, grad_norm: 0.38392171263694763\n",
            "Training:  27% 62/226 [05:06<16:01,  5.86s/it]loss: 0.19062989950180054, grad_norm: 0.20012831687927246\n",
            "Training:  28% 63/226 [05:11<14:37,  5.38s/it]loss: 0.13535724580287933, grad_norm: 0.18417590856552124\n",
            "Training:  28% 64/226 [05:16<14:04,  5.22s/it]loss: 0.1747865378856659, grad_norm: 0.16395829617977142\n",
            "Training:  29% 65/226 [05:20<13:23,  4.99s/it]loss: 0.15247748792171478, grad_norm: 0.14374133944511414\n",
            "Training:  29% 66/226 [05:25<13:04,  4.90s/it]loss: 0.17999456822872162, grad_norm: 0.15446366369724274\n",
            "Training:  30% 67/226 [05:29<12:38,  4.77s/it]loss: 0.1131930723786354, grad_norm: 0.14298592507839203\n",
            "Training:  30% 68/226 [05:38<15:30,  5.89s/it]loss: 0.1409410983324051, grad_norm: 0.12251578271389008\n",
            "Training:  31% 69/226 [05:44<15:57,  6.10s/it]loss: 0.1665254831314087, grad_norm: 0.2669868469238281\n",
            "Training:  31% 70/226 [05:50<15:41,  6.03s/it]loss: 0.25667324662208557, grad_norm: 0.2182985544204712\n",
            "Training:  31% 71/226 [06:00<18:38,  7.21s/it]loss: 0.23971056938171387, grad_norm: 0.2006986141204834\n",
            "Training:  32% 72/226 [06:03<14:58,  5.83s/it]loss: 0.21516332030296326, grad_norm: 0.21950246393680573\n",
            "Training:  32% 73/226 [06:06<12:55,  5.07s/it]loss: 0.14057429134845734, grad_norm: 0.15398375689983368\n",
            "Training:  33% 74/226 [06:10<12:17,  4.85s/it]loss: 0.1609298437833786, grad_norm: 0.13960371911525726\n",
            "Training:  33% 75/226 [06:18<14:16,  5.67s/it]loss: 0.21080872416496277, grad_norm: 0.45806339383125305\n",
            "Training:  34% 76/226 [06:22<13:01,  5.21s/it]loss: 0.13095621764659882, grad_norm: 0.19137734174728394\n",
            "Training:  34% 77/226 [06:29<13:53,  5.59s/it]loss: 0.2420315146446228, grad_norm: 0.28526607155799866\n",
            "Training:  35% 78/226 [06:31<11:23,  4.62s/it]loss: 0.26093629002571106, grad_norm: 0.4475603699684143\n",
            "Training:  35% 79/226 [06:33<09:36,  3.92s/it]loss: 0.1455645114183426, grad_norm: 0.17472943663597107\n",
            "Training:  35% 80/226 [06:37<09:42,  3.99s/it]loss: 0.19645793735980988, grad_norm: 0.15241093933582306\n",
            "Training:  36% 81/226 [06:51<16:53,  6.99s/it]loss: 0.1597839742898941, grad_norm: 0.12258853018283844\n",
            "Training:  36% 82/226 [06:58<16:27,  6.86s/it]loss: 0.20342615246772766, grad_norm: 0.20869740843772888\n",
            "Training:  37% 83/226 [07:03<14:49,  6.22s/it]loss: 0.17100273072719574, grad_norm: 0.19175775349140167\n",
            "Training:  37% 84/226 [07:07<13:03,  5.52s/it]loss: 0.2967628538608551, grad_norm: 4.669503211975098\n",
            "Training:  38% 85/226 [07:09<10:45,  4.58s/it]loss: 0.16095182299613953, grad_norm: 0.17419856786727905\n",
            "Training:  38% 86/226 [07:13<10:13,  4.38s/it]loss: 0.18219254910945892, grad_norm: 0.32227644324302673\n",
            "Training:  38% 87/226 [07:17<09:46,  4.22s/it]loss: 0.19744865596294403, grad_norm: 0.3959909975528717\n",
            "Training:  39% 88/226 [07:20<08:51,  3.85s/it]loss: 0.19667360186576843, grad_norm: 0.29522472620010376\n",
            "Training:  39% 89/226 [07:24<08:51,  3.88s/it]loss: 0.21334241330623627, grad_norm: 0.20299707353115082\n",
            "Training:  40% 90/226 [07:27<08:15,  3.65s/it]loss: 0.19848674535751343, grad_norm: 0.189388245344162\n",
            "Training:  40% 91/226 [07:32<09:35,  4.27s/it]loss: 0.20763233304023743, grad_norm: 0.27016115188598633\n",
            "Training:  41% 92/226 [07:36<09:20,  4.19s/it]loss: 0.21400530636310577, grad_norm: 0.27071407437324524\n",
            "Training:  41% 93/226 [07:40<08:44,  3.94s/it]loss: 0.19840869307518005, grad_norm: 0.2621248662471771\n",
            "Training:  42% 94/226 [07:45<09:12,  4.19s/it]loss: 0.24791090190410614, grad_norm: 0.3290224075317383\n",
            "Training:  42% 95/226 [07:48<08:37,  3.95s/it]loss: 0.13246499001979828, grad_norm: 0.18425115942955017\n",
            "Training:  42% 96/226 [07:55<10:22,  4.79s/it]loss: 0.23597201704978943, grad_norm: 0.36651697754859924\n",
            "Training:  43% 97/226 [07:58<09:07,  4.25s/it]loss: 0.2552148997783661, grad_norm: 0.4131091237068176\n",
            "Training:  43% 98/226 [08:00<07:46,  3.64s/it]loss: 0.19452379643917084, grad_norm: 0.2076202929019928\n",
            "Training:  44% 99/226 [08:03<07:28,  3.53s/it]loss: 0.1913984715938568, grad_norm: 0.17924858629703522\n",
            "Training:  44% 100/226 [08:08<08:30,  4.05s/it]loss: 0.24032732844352722, grad_norm: 0.2457534670829773\n",
            "Training:  45% 101/226 [08:20<12:54,  6.20s/it]loss: 0.19524972140789032, grad_norm: 0.1866564005613327\n",
            "Training:  45% 102/226 [08:23<11:17,  5.47s/it]loss: 0.149569571018219, grad_norm: 0.17748552560806274\n",
            "Training:  46% 103/226 [08:29<11:18,  5.52s/it]loss: 0.27753692865371704, grad_norm: 0.312757670879364\n",
            "Training:  46% 104/226 [08:32<09:43,  4.78s/it]loss: 0.17119821906089783, grad_norm: 0.1823703646659851\n",
            "Training:  46% 105/226 [08:37<09:55,  4.92s/it]loss: 0.1996195763349533, grad_norm: 0.17326366901397705\n",
            "Training:  47% 106/226 [08:42<09:34,  4.79s/it]loss: 0.2424527257680893, grad_norm: 0.5983271598815918\n",
            "Training:  47% 107/226 [08:45<08:28,  4.27s/it]loss: 0.176426500082016, grad_norm: 0.189775288105011\n",
            "Training:  48% 108/226 [08:48<07:59,  4.06s/it]loss: 0.21898135542869568, grad_norm: 0.2379058599472046\n",
            "Training:  48% 109/226 [08:51<07:14,  3.71s/it]loss: 0.25705552101135254, grad_norm: 0.29794496297836304\n",
            "Training:  49% 110/226 [08:54<06:16,  3.25s/it]loss: 0.14046205580234528, grad_norm: 0.17870353162288666\n",
            "Training:  49% 111/226 [09:07<11:51,  6.19s/it]loss: 0.17709587514400482, grad_norm: 0.1897079050540924\n",
            "Training:  50% 112/226 [09:11<11:01,  5.80s/it]loss: 0.23694758117198944, grad_norm: 0.2948860228061676\n",
            "Training:  50% 113/226 [09:14<09:01,  4.79s/it]loss: 0.3025582432746887, grad_norm: 0.37968015670776367\n",
            "Training:  50% 114/226 [09:15<07:06,  3.81s/it]loss: 0.13650374114513397, grad_norm: 0.15311385691165924\n",
            "Training:  51% 115/226 [09:25<09:59,  5.40s/it]loss: 0.18628928065299988, grad_norm: 0.2842491567134857\n",
            "Training:  51% 116/226 [09:30<09:52,  5.39s/it]loss: 0.21686728298664093, grad_norm: 0.262113094329834\n",
            "Training:  52% 117/226 [09:35<09:28,  5.22s/it]loss: 0.34418630599975586, grad_norm: 0.4068247973918915\n",
            "Training:  52% 118/226 [09:36<07:29,  4.16s/it]loss: 0.22255203127861023, grad_norm: 0.31308218836784363\n",
            "Training:  53% 119/226 [09:39<06:47,  3.81s/it]loss: 0.14689013361930847, grad_norm: 0.19026480615139008\n",
            "Training:  53% 120/226 [09:45<07:42,  4.37s/it]loss: 0.12068889290094376, grad_norm: 0.12650801241397858\n",
            "Training:  54% 121/226 [10:02<14:12,  8.12s/it]loss: 0.19693782925605774, grad_norm: 0.22130490839481354\n",
            "Training:  54% 122/226 [10:06<11:53,  6.86s/it]loss: 0.14270378649234772, grad_norm: 0.15033310651779175\n",
            "Training:  54% 123/226 [10:12<11:15,  6.55s/it]loss: 0.2227391004562378, grad_norm: 0.25703829526901245\n",
            "Training:  55% 124/226 [10:14<09:04,  5.34s/it]loss: 0.25875940918922424, grad_norm: 0.30654966831207275\n",
            "Training:  55% 125/226 [10:16<07:24,  4.40s/it]loss: 0.217410147190094, grad_norm: 0.21596796810626984\n",
            "Training:  56% 126/226 [10:20<06:47,  4.07s/it]loss: 0.21820393204689026, grad_norm: 0.33090099692344666\n",
            "Training:  56% 127/226 [10:23<06:13,  3.77s/it]loss: 0.24600648880004883, grad_norm: 0.2210417538881302\n",
            "Training:  57% 128/226 [10:25<05:37,  3.44s/it]loss: 0.20141905546188354, grad_norm: 0.16198204457759857\n",
            "Training:  57% 129/226 [10:30<05:55,  3.66s/it]loss: 0.12702812254428864, grad_norm: 0.11686388403177261\n",
            "Training:  58% 130/226 [10:38<07:59,  4.99s/it]loss: 0.18081246316432953, grad_norm: 0.15225856006145477\n",
            "Training:  58% 131/226 [10:51<11:54,  7.52s/it]loss: 0.11493011564016342, grad_norm: 0.21778278052806854\n",
            "Training:  58% 132/226 [10:58<11:17,  7.21s/it]loss: 0.13140413165092468, grad_norm: 0.13810180127620697\n",
            "Training:  59% 133/226 [11:06<11:50,  7.65s/it]loss: 0.24361197650432587, grad_norm: 0.2876991927623749\n",
            "Training:  59% 134/226 [11:09<09:30,  6.20s/it]loss: 0.23202109336853027, grad_norm: 0.20080985128879547\n",
            "Training:  60% 135/226 [11:13<08:06,  5.35s/it]loss: 0.17295436561107635, grad_norm: 0.17222033441066742\n",
            "Training:  60% 136/226 [11:18<08:01,  5.35s/it]loss: 0.21652019023895264, grad_norm: 0.2159155160188675\n",
            "Training:  61% 137/226 [11:21<06:51,  4.63s/it]loss: 0.23417608439922333, grad_norm: 0.20573197305202484\n",
            "Training:  61% 138/226 [11:23<05:51,  4.00s/it]loss: 0.19256317615509033, grad_norm: 0.1643831580877304\n",
            "Training:  62% 139/226 [11:29<06:31,  4.50s/it]loss: 0.23729799687862396, grad_norm: 0.24917128682136536\n",
            "Training:  62% 140/226 [11:31<05:29,  3.83s/it]loss: 0.21527133882045746, grad_norm: 0.2725996971130371\n",
            "Training:  62% 141/226 [11:38<06:50,  4.83s/it]loss: 0.23071379959583282, grad_norm: 0.22159233689308167\n",
            "Training:  63% 142/226 [11:42<06:07,  4.37s/it]loss: 0.15173986554145813, grad_norm: 0.1879601925611496\n",
            "Training:  63% 143/226 [11:46<06:02,  4.37s/it]loss: 0.20512031018733978, grad_norm: 0.1579248160123825\n",
            "Training:  64% 144/226 [11:50<05:43,  4.20s/it]loss: 0.16573598980903625, grad_norm: 0.16584965586662292\n",
            "Training:  64% 145/226 [11:55<06:07,  4.54s/it]loss: 0.17635926604270935, grad_norm: 0.18534286320209503\n",
            "Training:  65% 146/226 [12:00<05:59,  4.49s/it]loss: 0.14690464735031128, grad_norm: 0.20815855264663696\n",
            "Training:  65% 147/226 [12:05<06:23,  4.86s/it]loss: 0.16370342671871185, grad_norm: 0.22748559713363647\n",
            "Training:  65% 148/226 [12:09<05:44,  4.41s/it]loss: 0.13197553157806396, grad_norm: 0.10174792259931564\n",
            "Training:  66% 149/226 [12:18<07:36,  5.92s/it]loss: 0.2019100934267044, grad_norm: 0.3434552550315857\n",
            "Training:  66% 150/226 [12:22<06:37,  5.23s/it]loss: 0.22754845023155212, grad_norm: 0.23421165347099304\n",
            "Training:  67% 151/226 [12:32<08:23,  6.71s/it]loss: 0.19242152571678162, grad_norm: 0.1977035403251648\n",
            "Training:  67% 152/226 [12:35<06:52,  5.58s/it]loss: 0.1786811649799347, grad_norm: 0.1777220517396927\n",
            "Training:  68% 153/226 [12:39<06:21,  5.23s/it]loss: 0.1981651782989502, grad_norm: 0.16778817772865295\n",
            "Training:  68% 154/226 [12:44<06:09,  5.13s/it]loss: 0.2564331591129303, grad_norm: 0.2307373583316803\n",
            "Training:  69% 155/226 [12:47<05:09,  4.36s/it]loss: 0.19480271637439728, grad_norm: 0.15729792416095734\n",
            "Training:  69% 156/226 [12:52<05:18,  4.55s/it]loss: 0.1847894936800003, grad_norm: 0.15793775022029877\n",
            "Training:  69% 157/226 [12:56<05:14,  4.56s/it]loss: 0.13765981793403625, grad_norm: 0.12866102159023285\n",
            "Training:  70% 158/226 [13:05<06:32,  5.77s/it]loss: 0.1899954229593277, grad_norm: 0.14980405569076538\n",
            "Training:  70% 159/226 [13:11<06:42,  6.01s/it]loss: 0.24223771691322327, grad_norm: 0.21449433267116547\n",
            "Training:  71% 160/226 [13:16<06:12,  5.65s/it]loss: 0.19464118778705597, grad_norm: 0.16812269389629364\n",
            "Training:  71% 161/226 [13:27<07:52,  7.27s/it]loss: 0.23549668490886688, grad_norm: 0.16970060765743256\n",
            "Training:  72% 162/226 [13:31<06:26,  6.04s/it]loss: 0.17060451209545135, grad_norm: 0.14875534176826477\n",
            "Training:  72% 163/226 [13:37<06:36,  6.30s/it]loss: 0.24543188512325287, grad_norm: 0.18897826969623566\n",
            "Training:  73% 164/226 [13:40<05:24,  5.23s/it]loss: 0.1635204553604126, grad_norm: 0.14664271473884583\n",
            "Training:  73% 165/226 [13:49<06:29,  6.39s/it]loss: 0.3086022436618805, grad_norm: 0.2945694625377655\n",
            "Training:  73% 166/226 [13:51<05:01,  5.02s/it]loss: 0.23618239164352417, grad_norm: 0.2074761986732483\n",
            "Training:  74% 167/226 [13:55<04:36,  4.69s/it]loss: 0.2530125081539154, grad_norm: 0.20012681186199188\n",
            "Training:  74% 168/226 [14:00<04:36,  4.76s/it]loss: 0.20363017916679382, grad_norm: 0.20763705670833588\n",
            "Training:  75% 169/226 [14:03<04:09,  4.38s/it]loss: 0.21876400709152222, grad_norm: 0.20263253152370453\n",
            "Training:  75% 170/226 [14:07<03:50,  4.12s/it]loss: 0.18429774045944214, grad_norm: 1.0706367492675781\n",
            "Training:  76% 171/226 [14:21<06:24,  6.99s/it]loss: 0.15426357090473175, grad_norm: 0.22088000178337097\n",
            "Training:  76% 172/226 [14:27<06:05,  6.77s/it]loss: 0.22625039517879486, grad_norm: 0.24411851167678833\n",
            "Training:  77% 173/226 [14:30<04:55,  5.58s/it]loss: 0.16550704836845398, grad_norm: 0.2294599413871765\n",
            "Training:  77% 174/226 [14:35<04:40,  5.39s/it]loss: 0.1630922555923462, grad_norm: 0.17724449932575226\n",
            "Training:  77% 175/226 [14:39<04:14,  4.99s/it]loss: 0.1838787943124771, grad_norm: 0.1867460012435913\n",
            "Training:  78% 176/226 [14:43<04:06,  4.94s/it]loss: 0.15757040679454803, grad_norm: 0.16401980817317963\n",
            "Training:  78% 177/226 [14:49<04:13,  5.17s/it]loss: 0.2486335188150406, grad_norm: 0.26111817359924316\n",
            "Training:  79% 178/226 [14:52<03:37,  4.53s/it]loss: 0.21671700477600098, grad_norm: 0.1812290996313095\n",
            "Training:  79% 179/226 [14:56<03:28,  4.43s/it]loss: 0.209757462143898, grad_norm: 0.19723284244537354\n",
            "Training:  80% 180/226 [15:01<03:27,  4.51s/it]loss: 0.14998307824134827, grad_norm: 0.13849686086177826\n",
            "Training:  80% 181/226 [15:16<05:41,  7.59s/it]loss: 0.16626478731632233, grad_norm: 0.150046244263649\n",
            "Training:  81% 182/226 [15:23<05:21,  7.32s/it]loss: 0.20728038251399994, grad_norm: 0.20403869450092316\n",
            "Training:  81% 183/226 [15:26<04:20,  6.07s/it]loss: 0.16218040883541107, grad_norm: 0.16117113828659058\n",
            "Training:  81% 184/226 [15:31<04:01,  5.74s/it]loss: 0.24333907663822174, grad_norm: 0.22201167047023773\n",
            "Training:  82% 185/226 [15:35<03:37,  5.30s/it]loss: 0.1904064565896988, grad_norm: 0.15758657455444336\n",
            "Training:  82% 186/226 [15:42<03:54,  5.86s/it]loss: 0.14812611043453217, grad_norm: 0.17440085113048553\n",
            "Training:  83% 187/226 [15:47<03:33,  5.47s/it]loss: 0.10639255493879318, grad_norm: 0.11156629770994186\n",
            "Training:  83% 188/226 [15:55<04:04,  6.43s/it]loss: 0.15366075932979584, grad_norm: 0.13846267759799957\n",
            "Training:  84% 189/226 [16:00<03:36,  5.86s/it]loss: 0.21697595715522766, grad_norm: 0.264363169670105\n",
            "Training:  84% 190/226 [16:03<02:56,  4.90s/it]loss: 0.13354291021823883, grad_norm: 0.1383758932352066\n",
            "Training:  85% 191/226 [16:17<04:36,  7.90s/it]loss: 0.1846867948770523, grad_norm: 0.15896634757518768\n",
            "Training:  85% 192/226 [16:24<04:12,  7.43s/it]loss: 0.15463076531887054, grad_norm: 0.12625809013843536\n",
            "Training:  85% 193/226 [16:28<03:35,  6.52s/it]loss: 0.23097720742225647, grad_norm: 0.2127627730369568\n",
            "Training:  86% 194/226 [16:32<02:59,  5.61s/it]loss: 0.25496506690979004, grad_norm: 0.23068922758102417\n",
            "Training:  86% 195/226 [16:34<02:24,  4.67s/it]loss: 0.19866631925106049, grad_norm: 0.1832222193479538\n",
            "Training:  87% 196/226 [16:38<02:07,  4.26s/it]loss: 0.2037908434867859, grad_norm: 0.14531627297401428\n",
            "Training:  87% 197/226 [16:42<02:04,  4.28s/it]loss: 0.2199530154466629, grad_norm: 0.14837512373924255\n",
            "Training:  88% 198/226 [16:46<02:02,  4.38s/it]loss: 0.17761225998401642, grad_norm: 1.0718797445297241\n",
            "Training:  88% 199/226 [16:50<01:54,  4.25s/it]loss: 0.25803232192993164, grad_norm: 0.2253851294517517\n",
            "Training:  88% 200/226 [16:53<01:40,  3.86s/it]loss: 0.18364466726779938, grad_norm: 0.20544283092021942\n",
            "Training:  89% 201/226 [16:59<01:46,  4.28s/it]loss: 0.16499264538288116, grad_norm: 0.15575717389583588\n",
            "Training:  89% 202/226 [17:04<01:48,  4.50s/it]loss: 0.20143002271652222, grad_norm: 0.17123551666736603\n",
            "Training:  90% 203/226 [17:10<01:55,  5.02s/it]loss: 0.1642920821905136, grad_norm: 0.17313800752162933\n",
            "Training:  90% 204/226 [17:16<01:56,  5.31s/it]loss: 0.2488807737827301, grad_norm: 0.24221143126487732\n",
            "Training:  91% 205/226 [17:18<01:32,  4.42s/it]loss: 0.2484320104122162, grad_norm: 0.29735592007637024\n",
            "Training:  91% 206/226 [17:21<01:19,  3.98s/it]loss: 0.21204815804958344, grad_norm: 0.19424059987068176\n",
            "Training:  92% 207/226 [17:26<01:23,  4.37s/it]loss: 0.12583814561367035, grad_norm: 0.13734477758407593\n",
            "Training:  92% 208/226 [17:33<01:32,  5.11s/it]loss: 0.15940764546394348, grad_norm: 0.17864751815795898\n",
            "Training:  92% 209/226 [17:38<01:25,  5.04s/it]loss: 0.14972852170467377, grad_norm: 0.13587415218353271\n",
            "Training:  93% 210/226 [17:44<01:26,  5.39s/it]loss: 0.13245531916618347, grad_norm: 0.13556677103042603\n",
            "Training:  93% 211/226 [17:58<01:57,  7.83s/it]loss: 0.24420596659183502, grad_norm: 0.3818146288394928\n",
            "Training:  94% 212/226 [18:00<01:26,  6.16s/it]loss: 0.18076921999454498, grad_norm: 0.15036730468273163\n",
            "Training:  94% 213/226 [18:05<01:15,  5.84s/it]loss: 0.10118310153484344, grad_norm: 0.0999978557229042\n",
            "Training:  95% 214/226 [18:15<01:22,  6.89s/it]loss: 0.1998271495103836, grad_norm: 0.1710495799779892\n",
            "Training:  95% 215/226 [18:19<01:06,  6.02s/it]loss: 0.15484526753425598, grad_norm: 0.1360926628112793\n",
            "Training:  96% 216/226 [18:25<01:00,  6.06s/it]loss: 0.14038456976413727, grad_norm: 0.12830737233161926\n",
            "Training:  96% 217/226 [18:30<00:53,  5.93s/it]loss: 0.21314123272895813, grad_norm: 0.2783774733543396\n",
            "Training:  96% 218/226 [18:34<00:41,  5.22s/it]loss: 0.20223307609558105, grad_norm: 0.18906289339065552\n",
            "Training:  97% 219/226 [18:37<00:32,  4.60s/it]loss: 0.160862997174263, grad_norm: 0.1350804716348648\n",
            "Training:  97% 220/226 [18:41<00:26,  4.42s/it]loss: 0.20969060063362122, grad_norm: 0.24174298346042633\n",
            "Training:  98% 221/226 [18:48<00:25,  5.06s/it]loss: 0.18741697072982788, grad_norm: 0.13728560507297516\n",
            "Training:  98% 222/226 [18:52<00:19,  4.88s/it]loss: 0.3555438220500946, grad_norm: 1.7919284105300903\n",
            "Training:  99% 223/226 [18:54<00:11,  3.99s/it]loss: 0.2661988139152527, grad_norm: 0.38824528455734253\n",
            "Training:  99% 224/226 [18:57<00:07,  3.61s/it]loss: 0.2542435824871063, grad_norm: 0.23297518491744995\n",
            "Training: 100% 225/226 [18:59<00:03,  3.26s/it]loss: 0.18684488534927368, grad_norm: 0.17195896804332733\n",
            "Training: 100% 226/226 [19:03<00:00,  5.06s/it]\n",
            "Validation loss: 0.7628030848503112\n",
            "Epoch: 6\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.20823870599269867, grad_norm: 1.0594292879104614\n",
            "Training:   0% 1/226 [00:03<11:26,  3.05s/it]loss: 0.22866876423358917, grad_norm: 0.1861954778432846\n",
            "Training:   1% 2/226 [00:06<12:30,  3.35s/it]loss: 0.2181405872106552, grad_norm: 0.19505064189434052\n",
            "Training:   1% 3/226 [00:10<13:46,  3.71s/it]loss: 0.12239327281713486, grad_norm: 0.13887198269367218\n",
            "Training:   2% 4/226 [00:19<20:47,  5.62s/it]loss: 0.18505720794200897, grad_norm: 0.2559402287006378\n",
            "Training:   2% 5/226 [00:25<22:06,  6.00s/it]loss: 0.2745600938796997, grad_norm: 0.38220134377479553\n",
            "Training:   3% 6/226 [00:28<17:11,  4.69s/it]loss: 0.16594552993774414, grad_norm: 0.17736943066120148\n",
            "Training:   3% 7/226 [00:33<17:38,  4.83s/it]loss: 0.2555607557296753, grad_norm: 0.2752964496612549\n",
            "Training:   4% 8/226 [00:38<18:28,  5.09s/it]loss: 0.20541809499263763, grad_norm: 0.25547900795936584\n",
            "Training:   4% 9/226 [00:42<16:44,  4.63s/it]loss: 0.22585926949977875, grad_norm: 0.2868140935897827\n",
            "Training:   4% 10/226 [00:47<16:34,  4.60s/it]loss: 0.16903384029865265, grad_norm: 0.20005249977111816\n",
            "Training:   5% 11/226 [00:51<15:54,  4.44s/it]loss: 0.19315138459205627, grad_norm: 0.18299072980880737\n",
            "Training:   5% 12/226 [00:56<17:01,  4.77s/it]loss: 0.1251453012228012, grad_norm: 0.11839418113231659\n",
            "Training:   6% 13/226 [01:04<19:53,  5.60s/it]loss: 0.1817563772201538, grad_norm: 0.2337971031665802\n",
            "Training:   6% 14/226 [01:09<19:03,  5.40s/it]loss: 0.22767896950244904, grad_norm: 0.2113545536994934\n",
            "Training:   7% 15/226 [01:18<23:34,  6.70s/it]loss: 0.20642340183258057, grad_norm: 0.4501151442527771\n",
            "Training:   7% 16/226 [01:21<19:35,  5.60s/it]loss: 0.18980543315410614, grad_norm: 0.27155205607414246\n",
            "Training:   8% 17/226 [01:26<18:09,  5.21s/it]loss: 0.11634119600057602, grad_norm: 0.15844149887561798\n",
            "Training:   8% 18/226 [01:33<20:31,  5.92s/it]loss: 0.17734648287296295, grad_norm: 0.1544475108385086\n",
            "Training:   8% 19/226 [01:37<18:34,  5.39s/it]loss: 0.17547687888145447, grad_norm: 0.1875905692577362\n",
            "Training:   9% 20/226 [01:42<17:23,  5.07s/it]loss: 0.19858042895793915, grad_norm: 0.41137823462486267\n",
            "Training:   9% 21/226 [01:46<16:18,  4.77s/it]loss: 0.3185543119907379, grad_norm: 0.43238455057144165\n",
            "Training:  10% 22/226 [01:47<13:01,  3.83s/it]loss: 0.20576225221157074, grad_norm: 0.2644505798816681\n",
            "Training:  10% 23/226 [01:52<13:17,  3.93s/it]loss: 0.1265171319246292, grad_norm: 0.15336254239082336\n",
            "Training:  11% 24/226 [02:00<17:38,  5.24s/it]loss: 0.24577955901622772, grad_norm: 0.3244819939136505\n",
            "Training:  11% 25/226 [02:10<21:58,  6.56s/it]loss: 0.204293355345726, grad_norm: 0.21786190569400787\n",
            "Training:  12% 26/226 [02:14<19:37,  5.89s/it]loss: 0.17962194979190826, grad_norm: 0.1919766366481781\n",
            "Training:  12% 27/226 [02:18<18:03,  5.44s/it]loss: 0.20102789998054504, grad_norm: 0.297268271446228\n",
            "Training:  12% 28/226 [02:22<16:01,  4.85s/it]loss: 0.14666955173015594, grad_norm: 0.15276236832141876\n",
            "Training:  13% 29/226 [02:26<15:44,  4.79s/it]loss: 0.15308590233325958, grad_norm: 0.1627555638551712\n",
            "Training:  13% 30/226 [02:33<16:58,  5.20s/it]loss: 0.20944973826408386, grad_norm: 0.3413224220275879\n",
            "Training:  14% 31/226 [02:35<14:24,  4.44s/it]loss: 0.2416485995054245, grad_norm: 0.24009236693382263\n",
            "Training:  14% 32/226 [02:38<12:29,  3.86s/it]loss: 0.18840104341506958, grad_norm: 0.19667617976665497\n",
            "Training:  15% 33/226 [02:42<12:32,  3.90s/it]loss: 0.17612794041633606, grad_norm: 0.19162340462207794\n",
            "Training:  15% 34/226 [02:48<14:26,  4.52s/it]loss: 0.2164343297481537, grad_norm: 0.2370721399784088\n",
            "Training:  15% 35/226 [02:59<20:40,  6.49s/it]loss: 0.1488654464483261, grad_norm: 0.13311922550201416\n",
            "Training:  16% 36/226 [03:05<19:59,  6.31s/it]loss: 0.21387796103954315, grad_norm: 0.23875239491462708\n",
            "Training:  16% 37/226 [03:08<17:28,  5.55s/it]loss: 0.2374279797077179, grad_norm: 0.27127623558044434\n",
            "Training:  17% 38/226 [03:11<14:30,  4.63s/it]loss: 0.23473814129829407, grad_norm: 0.20900185406208038\n",
            "Training:  17% 39/226 [03:15<13:45,  4.42s/it]loss: 0.17853349447250366, grad_norm: 0.5374495387077332\n",
            "Training:  18% 40/226 [03:24<18:21,  5.92s/it]loss: 0.21492519974708557, grad_norm: 0.19287247955799103\n",
            "Training:  18% 41/226 [03:28<15:49,  5.13s/it]loss: 0.1648837924003601, grad_norm: 0.17672693729400635\n",
            "Training:  19% 42/226 [03:33<15:40,  5.11s/it]loss: 0.279448002576828, grad_norm: 0.2994326055049896\n",
            "Training:  19% 43/226 [03:35<13:32,  4.44s/it]loss: 0.13557909429073334, grad_norm: 0.1445336788892746\n",
            "Training:  19% 44/226 [03:41<14:29,  4.78s/it]loss: 0.15185131132602692, grad_norm: 0.12633030116558075\n",
            "Training:  20% 45/226 [03:58<25:38,  8.50s/it]loss: 0.1525944471359253, grad_norm: 0.1428164392709732\n",
            "Training:  20% 46/226 [04:05<24:07,  8.04s/it]loss: 0.20751558244228363, grad_norm: 0.2138623148202896\n",
            "Training:  21% 47/226 [04:11<21:46,  7.30s/it]loss: 0.22492019832134247, grad_norm: 0.5537257790565491\n",
            "Training:  21% 48/226 [04:14<18:08,  6.12s/it]loss: 0.23355749249458313, grad_norm: 0.22798776626586914\n",
            "Training:  22% 49/226 [04:17<14:51,  5.03s/it]loss: 0.3082975447177887, grad_norm: 0.41666126251220703\n",
            "Training:  22% 50/226 [04:18<11:59,  4.09s/it]loss: 0.1534557342529297, grad_norm: 0.14969070255756378\n",
            "Training:  23% 51/226 [04:22<11:35,  3.97s/it]loss: 0.16095179319381714, grad_norm: 0.22565674781799316\n",
            "Training:  23% 52/226 [04:26<11:24,  3.93s/it]loss: 0.23676154017448425, grad_norm: 0.24926333129405975\n",
            "Training:  23% 53/226 [04:29<10:21,  3.59s/it]loss: 0.2522827088832855, grad_norm: 0.2274857461452484\n",
            "Training:  24% 54/226 [04:32<09:57,  3.48s/it]loss: 0.18097785115242004, grad_norm: 0.178227037191391\n",
            "Training:  24% 55/226 [04:45<17:48,  6.25s/it]loss: 0.15983474254608154, grad_norm: 0.1802644431591034\n",
            "Training:  25% 56/226 [04:49<16:09,  5.70s/it]loss: 0.28185465931892395, grad_norm: 0.3234439790248871\n",
            "Training:  25% 57/226 [04:51<12:50,  4.56s/it]loss: 0.1545296013355255, grad_norm: 0.1598721593618393\n",
            "Training:  26% 58/226 [04:57<13:54,  4.97s/it]loss: 0.1487177014350891, grad_norm: 0.14939697086811066\n",
            "Training:  26% 59/226 [05:07<17:38,  6.34s/it]loss: 0.19492806494235992, grad_norm: 0.21245203912258148\n",
            "Training:  27% 60/226 [05:11<15:55,  5.76s/it]loss: 0.17626169323921204, grad_norm: 0.211574524641037\n",
            "Training:  27% 61/226 [05:15<14:45,  5.37s/it]loss: 0.12124422937631607, grad_norm: 0.1623581498861313\n",
            "Training:  27% 62/226 [05:25<17:55,  6.56s/it]loss: 0.20717309415340424, grad_norm: 0.21720850467681885\n",
            "Training:  28% 63/226 [05:29<15:47,  5.81s/it]loss: 0.17030341923236847, grad_norm: 0.14774470031261444\n",
            "Training:  28% 64/226 [05:36<17:12,  6.37s/it]loss: 0.2048679143190384, grad_norm: 0.21461021900177002\n",
            "Training:  29% 65/226 [05:47<20:20,  7.58s/it]loss: 0.2326377034187317, grad_norm: 0.18666885793209076\n",
            "Training:  29% 66/226 [05:49<16:12,  6.08s/it]loss: 0.13836444914340973, grad_norm: 0.1767682135105133\n",
            "Training:  30% 67/226 [05:55<15:51,  5.99s/it]loss: 0.1591397523880005, grad_norm: 0.1657497137784958\n",
            "Training:  30% 68/226 [06:00<14:38,  5.56s/it]loss: 0.2350977510213852, grad_norm: 0.20907600224018097\n",
            "Training:  31% 69/226 [06:06<14:45,  5.64s/it]loss: 0.2921831011772156, grad_norm: 1.2692511081695557\n",
            "Training:  31% 70/226 [06:08<12:04,  4.65s/it]loss: 0.1916799247264862, grad_norm: 0.19779445230960846\n",
            "Training:  31% 71/226 [06:12<11:36,  4.49s/it]loss: 0.19230440258979797, grad_norm: 0.21394288539886475\n",
            "Training:  32% 72/226 [06:19<13:11,  5.14s/it]loss: 0.18564961850643158, grad_norm: 0.17482668161392212\n",
            "Training:  32% 73/226 [06:23<12:10,  4.78s/it]loss: 0.2205055058002472, grad_norm: 0.23447923362255096\n",
            "Training:  33% 74/226 [06:27<11:50,  4.67s/it]loss: 0.17976824939250946, grad_norm: 0.21312689781188965\n",
            "Training:  33% 75/226 [06:38<16:40,  6.63s/it]loss: 0.23097942769527435, grad_norm: 0.30478066205978394\n",
            "Training:  34% 76/226 [06:42<14:26,  5.78s/it]loss: 0.16843438148498535, grad_norm: 0.17241011559963226\n",
            "Training:  34% 77/226 [06:47<13:48,  5.56s/it]loss: 0.23526531457901, grad_norm: 1.708052396774292\n",
            "Training:  35% 78/226 [06:50<11:59,  4.86s/it]loss: 0.23399046063423157, grad_norm: 0.2653128206729889\n",
            "Training:  35% 79/226 [06:54<11:07,  4.54s/it]loss: 0.17707954347133636, grad_norm: 0.2196495085954666\n",
            "Training:  35% 80/226 [07:00<11:57,  4.91s/it]loss: 0.16299968957901, grad_norm: 0.15369781851768494\n",
            "Training:  36% 81/226 [07:06<12:56,  5.36s/it]loss: 0.19328726828098297, grad_norm: 0.99676513671875\n",
            "Training:  36% 82/226 [07:14<14:31,  6.05s/it]loss: 0.17194142937660217, grad_norm: 0.450069397687912\n",
            "Training:  37% 83/226 [07:19<13:35,  5.70s/it]loss: 0.18919341266155243, grad_norm: 0.20178893208503723\n",
            "Training:  37% 84/226 [07:24<13:16,  5.61s/it]loss: 0.16975891590118408, grad_norm: 0.2781219184398651\n",
            "Training:  38% 85/226 [07:36<17:10,  7.31s/it]loss: 0.24594323337078094, grad_norm: 0.3031691610813141\n",
            "Training:  38% 86/226 [07:38<13:38,  5.85s/it]loss: 0.24054054915905, grad_norm: 0.2552715241909027\n",
            "Training:  38% 87/226 [07:41<11:39,  5.03s/it]loss: 0.21917936205863953, grad_norm: 0.22940775752067566\n",
            "Training:  39% 88/226 [07:46<11:40,  5.07s/it]loss: 0.14299815893173218, grad_norm: 0.1745215356349945\n",
            "Training:  39% 89/226 [07:52<12:13,  5.36s/it]loss: 0.20460814237594604, grad_norm: 0.2740583121776581\n",
            "Training:  40% 90/226 [07:57<12:00,  5.30s/it]loss: 0.22716106474399567, grad_norm: 1.2318899631500244\n",
            "Training:  40% 91/226 [08:01<10:37,  4.72s/it]loss: 0.16358715295791626, grad_norm: 0.15269146859645844\n",
            "Training:  41% 92/226 [08:06<10:50,  4.86s/it]loss: 0.19398629665374756, grad_norm: 0.21221715211868286\n",
            "Training:  41% 93/226 [08:11<11:02,  4.98s/it]loss: 0.17479132115840912, grad_norm: 0.1682468056678772\n",
            "Training:  42% 94/226 [08:16<11:03,  5.03s/it]loss: 0.1434859335422516, grad_norm: 0.2409227341413498\n",
            "Training:  42% 95/226 [08:31<17:09,  7.86s/it]loss: 0.20976440608501434, grad_norm: 0.27936992049217224\n",
            "Training:  42% 96/226 [08:34<14:02,  6.48s/it]loss: 0.15737496316432953, grad_norm: 0.1678026020526886\n",
            "Training:  43% 97/226 [08:41<14:24,  6.70s/it]loss: 0.2085500806570053, grad_norm: 0.20944863557815552\n",
            "Training:  43% 98/226 [08:46<13:00,  6.10s/it]loss: 0.16762064397335052, grad_norm: 0.20563660562038422\n",
            "Training:  44% 99/226 [08:51<12:14,  5.79s/it]loss: 0.1741832047700882, grad_norm: 0.18017083406448364\n",
            "Training:  44% 100/226 [08:58<12:42,  6.05s/it]loss: 0.17627258598804474, grad_norm: 0.5365023016929626\n",
            "Training:  45% 101/226 [09:05<13:06,  6.29s/it]loss: 0.15276388823986053, grad_norm: 0.1675509363412857\n",
            "Training:  45% 102/226 [09:09<12:00,  5.81s/it]loss: 0.18660128116607666, grad_norm: 0.1994098573923111\n",
            "Training:  46% 103/226 [09:13<10:37,  5.18s/it]loss: 0.21553772687911987, grad_norm: 0.29086050391197205\n",
            "Training:  46% 104/226 [09:16<09:12,  4.53s/it]loss: 0.21517574787139893, grad_norm: 0.748700737953186\n",
            "Training:  46% 105/226 [09:28<13:53,  6.88s/it]loss: 0.22742286324501038, grad_norm: 0.2948419153690338\n",
            "Training:  47% 106/226 [09:32<11:39,  5.83s/it]loss: 0.2523565888404846, grad_norm: 0.27727842330932617\n",
            "Training:  47% 107/226 [09:35<09:49,  4.95s/it]loss: 0.28176429867744446, grad_norm: 0.48366987705230713\n",
            "Training:  48% 108/226 [09:37<08:27,  4.30s/it]loss: 0.27891674637794495, grad_norm: 0.3620850443840027\n",
            "Training:  48% 109/226 [09:39<06:49,  3.50s/it]loss: 0.17986251413822174, grad_norm: 0.23092256486415863\n",
            "Training:  49% 110/226 [09:43<07:11,  3.72s/it]loss: 0.222654789686203, grad_norm: 0.4681489169597626\n",
            "Training:  49% 111/226 [09:46<06:41,  3.49s/it]loss: 0.1633572280406952, grad_norm: 0.16434769332408905\n",
            "Training:  50% 112/226 [09:53<08:34,  4.51s/it]loss: 0.20194299519062042, grad_norm: 0.2232508361339569\n",
            "Training:  50% 113/226 [09:57<08:15,  4.39s/it]loss: 0.137745663523674, grad_norm: 0.18465153872966766\n",
            "Training:  50% 114/226 [10:04<09:32,  5.11s/it]loss: 0.14034046232700348, grad_norm: 0.14678652584552765\n",
            "Training:  51% 115/226 [10:18<14:10,  7.66s/it]loss: 0.17133520543575287, grad_norm: 0.44292882084846497\n",
            "Training:  51% 116/226 [10:26<14:40,  8.01s/it]loss: 0.23119743168354034, grad_norm: 0.1913297176361084\n",
            "Training:  52% 117/226 [10:31<12:52,  7.08s/it]loss: 0.20518454909324646, grad_norm: 0.24260163307189941\n",
            "Training:  52% 118/226 [10:36<11:11,  6.22s/it]loss: 0.16987334191799164, grad_norm: 0.17877282202243805\n",
            "Training:  53% 119/226 [10:40<10:08,  5.69s/it]loss: 0.21437270939350128, grad_norm: 0.5274420380592346\n",
            "Training:  53% 120/226 [10:45<09:35,  5.43s/it]loss: 0.26975104212760925, grad_norm: 0.2692372798919678\n",
            "Training:  54% 121/226 [10:47<07:55,  4.53s/it]loss: 0.2192438691854477, grad_norm: 0.25935736298561096\n",
            "Training:  54% 122/226 [10:52<07:54,  4.57s/it]loss: 0.22647014260292053, grad_norm: 0.6141919493675232\n",
            "Training:  54% 123/226 [10:56<07:24,  4.32s/it]loss: 0.1630491465330124, grad_norm: 0.1830340027809143\n",
            "Training:  55% 124/226 [11:00<07:04,  4.17s/it]loss: 0.191119983792305, grad_norm: 0.2245045304298401\n",
            "Training:  55% 125/226 [11:11<10:33,  6.27s/it]loss: 0.19428510963916779, grad_norm: 0.2189904898405075\n",
            "Training:  56% 126/226 [11:14<09:02,  5.43s/it]loss: 0.1630207896232605, grad_norm: 0.16332074999809265\n",
            "Training:  56% 127/226 [11:19<08:25,  5.11s/it]loss: 0.17316758632659912, grad_norm: 0.2671724855899811\n",
            "Training:  57% 128/226 [11:23<08:00,  4.90s/it]loss: 0.3617120385169983, grad_norm: 0.33631113171577454\n",
            "Training:  57% 129/226 [11:25<06:28,  4.00s/it]loss: 0.17461994290351868, grad_norm: 0.2389390766620636\n",
            "Training:  58% 130/226 [11:31<07:37,  4.76s/it]loss: 0.16979199647903442, grad_norm: 0.1909094750881195\n",
            "Training:  58% 131/226 [11:37<07:46,  4.91s/it]loss: 0.22370396554470062, grad_norm: 0.21350274980068207\n",
            "Training:  58% 132/226 [11:42<07:52,  5.03s/it]loss: 0.2104414999485016, grad_norm: 0.2682167589664459\n",
            "Training:  59% 133/226 [11:47<07:47,  5.03s/it]loss: 0.13667912781238556, grad_norm: 0.12730920314788818\n",
            "Training:  59% 134/226 [11:57<10:05,  6.58s/it]loss: 0.18278490006923676, grad_norm: 0.17567913234233856\n",
            "Training:  60% 135/226 [12:13<13:58,  9.21s/it]loss: 0.18446402251720428, grad_norm: 0.20945647358894348\n",
            "Training:  60% 136/226 [12:18<12:00,  8.00s/it]loss: 0.29227977991104126, grad_norm: 0.3875967860221863\n",
            "Training:  61% 137/226 [12:19<09:05,  6.13s/it]loss: 0.2879039943218231, grad_norm: 0.3536423444747925\n",
            "Training:  61% 138/226 [12:21<07:04,  4.82s/it]loss: 0.1440397948026657, grad_norm: 0.1630622297525406\n",
            "Training:  62% 139/226 [12:27<07:34,  5.22s/it]loss: 0.19328339397907257, grad_norm: 0.2475019097328186\n",
            "Training:  62% 140/226 [12:32<07:05,  4.95s/it]loss: 0.20440484583377838, grad_norm: 1.496417760848999\n",
            "Training:  62% 141/226 [12:36<06:34,  4.64s/it]loss: 0.2473745346069336, grad_norm: 0.29662683606147766\n",
            "Training:  63% 142/226 [12:38<05:27,  3.90s/it]loss: 0.22055548429489136, grad_norm: 0.23126086592674255\n",
            "Training:  63% 143/226 [12:43<05:58,  4.31s/it]loss: 0.30868417024612427, grad_norm: 0.42425966262817383\n",
            "Training:  64% 144/226 [12:45<04:59,  3.65s/it]loss: 0.2060411423444748, grad_norm: 0.23019492626190186\n",
            "Training:  64% 145/226 [12:53<06:45,  5.01s/it]loss: 0.17238986492156982, grad_norm: 0.30584287643432617\n",
            "Training:  65% 146/226 [12:57<06:11,  4.65s/it]loss: 0.17097768187522888, grad_norm: 0.1697378307580948\n",
            "Training:  65% 147/226 [13:02<06:10,  4.69s/it]loss: 0.1458273082971573, grad_norm: 0.1805349737405777\n",
            "Training:  65% 148/226 [13:09<07:01,  5.40s/it]loss: 0.21628674864768982, grad_norm: 0.31017354130744934\n",
            "Training:  66% 149/226 [13:12<06:04,  4.73s/it]loss: 0.2232901155948639, grad_norm: 0.18299582600593567\n",
            "Training:  66% 150/226 [13:18<06:25,  5.08s/it]loss: 0.1850297451019287, grad_norm: 0.28671589493751526\n",
            "Training:  67% 151/226 [13:23<06:13,  4.98s/it]loss: 0.25519874691963196, grad_norm: 0.23231777548789978\n",
            "Training:  67% 152/226 [13:27<05:45,  4.67s/it]loss: 0.14068463444709778, grad_norm: 0.15746431052684784\n",
            "Training:  68% 153/226 [13:35<07:09,  5.89s/it]loss: 0.2739506661891937, grad_norm: 0.7860882878303528\n",
            "Training:  68% 154/226 [13:38<05:48,  4.84s/it]loss: 0.3600936233997345, grad_norm: 0.7343247532844543\n",
            "Training:  69% 155/226 [13:47<07:05,  6.00s/it]loss: 0.1736488938331604, grad_norm: 0.21970997750759125\n",
            "Training:  69% 156/226 [13:50<06:10,  5.29s/it]loss: 0.16185997426509857, grad_norm: 0.23417365550994873\n",
            "Training:  69% 157/226 [13:57<06:41,  5.82s/it]loss: 0.2179461270570755, grad_norm: 0.7431756854057312\n",
            "Training:  70% 158/226 [14:01<05:44,  5.06s/it]loss: 0.27896809577941895, grad_norm: 0.3962920308113098\n",
            "Training:  70% 159/226 [14:03<04:43,  4.24s/it]loss: 0.22858642041683197, grad_norm: 0.23871001601219177\n",
            "Training:  71% 160/226 [14:07<04:28,  4.06s/it]loss: 0.24225609004497528, grad_norm: 0.293951153755188\n",
            "Training:  71% 161/226 [14:09<03:50,  3.55s/it]loss: 0.19663245975971222, grad_norm: 0.21912619471549988\n",
            "Training:  72% 162/226 [14:13<04:04,  3.81s/it]loss: 0.1807456761598587, grad_norm: 0.25641030073165894\n",
            "Training:  72% 163/226 [14:18<04:09,  3.95s/it]loss: 0.16714408993721008, grad_norm: 0.1973097175359726\n",
            "Training:  73% 164/226 [14:24<04:43,  4.58s/it]loss: 0.17765973508358002, grad_norm: 0.24493880569934845\n",
            "Training:  73% 165/226 [14:37<07:11,  7.08s/it]loss: 0.14577960968017578, grad_norm: 0.21488532423973083\n",
            "Training:  73% 166/226 [14:44<07:07,  7.12s/it]loss: 0.303603857755661, grad_norm: 0.5092083215713501\n",
            "Training:  74% 167/226 [14:45<05:23,  5.49s/it]loss: 0.21239997446537018, grad_norm: 0.17131301760673523\n",
            "Training:  74% 168/226 [14:50<05:03,  5.24s/it]loss: 0.19034740328788757, grad_norm: 0.18258771300315857\n",
            "Training:  75% 169/226 [14:55<04:54,  5.16s/it]loss: 0.2301088124513626, grad_norm: 0.24483738839626312\n",
            "Training:  75% 170/226 [15:00<04:41,  5.03s/it]loss: 0.17256022989749908, grad_norm: 0.16605530679225922\n",
            "Training:  76% 171/226 [15:04<04:19,  4.72s/it]loss: 0.21746698021888733, grad_norm: 0.31864792108535767\n",
            "Training:  76% 172/226 [15:07<03:57,  4.40s/it]loss: 0.26059219241142273, grad_norm: 0.23539209365844727\n",
            "Training:  77% 173/226 [15:12<03:52,  4.40s/it]loss: 0.16598139703273773, grad_norm: 0.1639825403690338\n",
            "Training:  77% 174/226 [15:16<03:51,  4.46s/it]loss: 0.191839799284935, grad_norm: 0.2745762765407562\n",
            "Training:  77% 175/226 [15:25<04:49,  5.67s/it]loss: 0.21761570870876312, grad_norm: 0.5761594772338867\n",
            "Training:  78% 176/226 [15:29<04:20,  5.22s/it]loss: 0.32437455654144287, grad_norm: 0.3785112202167511\n",
            "Training:  78% 177/226 [15:31<03:23,  4.16s/it]loss: 0.20084750652313232, grad_norm: 0.19857783615589142\n",
            "Training:  79% 178/226 [15:34<03:06,  3.88s/it]loss: 0.22177335619926453, grad_norm: 0.2814738154411316\n",
            "Training:  79% 179/226 [15:37<02:43,  3.47s/it]loss: 0.2244558483362198, grad_norm: 0.24418364465236664\n",
            "Training:  80% 180/226 [15:42<03:09,  4.13s/it]loss: 0.19292472302913666, grad_norm: 0.5233298540115356\n",
            "Training:  80% 181/226 [15:46<03:06,  4.14s/it]loss: 0.20723658800125122, grad_norm: 0.2265070378780365\n",
            "Training:  81% 182/226 [15:51<03:12,  4.38s/it]loss: 0.2207036316394806, grad_norm: 0.2960413992404938\n",
            "Training:  81% 183/226 [15:54<02:49,  3.95s/it]loss: 0.1961165964603424, grad_norm: 0.277875155210495\n",
            "Training:  81% 184/226 [15:57<02:31,  3.62s/it]loss: 0.35833850502967834, grad_norm: 0.82846599817276\n",
            "Training:  82% 185/226 [16:00<02:18,  3.38s/it]loss: 0.19525684416294098, grad_norm: 0.2272748053073883\n",
            "Training:  82% 186/226 [16:03<02:12,  3.31s/it]loss: 0.1680084615945816, grad_norm: 0.21562661230564117\n",
            "Training:  83% 187/226 [16:08<02:33,  3.93s/it]loss: 0.2920891344547272, grad_norm: 0.49832409620285034\n",
            "Training:  83% 188/226 [16:11<02:09,  3.40s/it]loss: 0.21815268695354462, grad_norm: 0.315949410200119\n",
            "Training:  84% 189/226 [16:14<02:08,  3.48s/it]loss: 0.24480538070201874, grad_norm: 0.2510654330253601\n",
            "Training:  84% 190/226 [16:18<02:03,  3.44s/it]loss: 0.15621188282966614, grad_norm: 0.1940058469772339\n",
            "Training:  85% 191/226 [16:23<02:17,  3.92s/it]loss: 0.18666234612464905, grad_norm: 0.20402935147285461\n",
            "Training:  85% 192/226 [16:27<02:15,  3.98s/it]loss: 0.24710649251937866, grad_norm: 0.304404616355896\n",
            "Training:  85% 193/226 [16:30<02:07,  3.87s/it]loss: 0.15346576273441315, grad_norm: 0.16663864254951477\n",
            "Training:  86% 194/226 [16:36<02:24,  4.51s/it]loss: 0.2759847044944763, grad_norm: 0.27798715233802795\n",
            "Training:  86% 195/226 [16:45<02:56,  5.69s/it]loss: 0.15207527577877045, grad_norm: 0.19329224526882172\n",
            "Training:  87% 196/226 [16:52<03:04,  6.16s/it]loss: 0.2202162891626358, grad_norm: 0.26803484559059143\n",
            "Training:  87% 197/226 [16:56<02:35,  5.36s/it]loss: 0.22025641798973083, grad_norm: 0.2238573133945465\n",
            "Training:  88% 198/226 [16:58<02:07,  4.56s/it]loss: 0.15900912880897522, grad_norm: 0.16679269075393677\n",
            "Training:  88% 199/226 [17:06<02:27,  5.47s/it]loss: 0.2246965616941452, grad_norm: 0.48386919498443604\n",
            "Training:  88% 200/226 [17:10<02:10,  5.03s/it]loss: 0.27822640538215637, grad_norm: 0.31287214159965515\n",
            "Training:  89% 201/226 [17:12<01:44,  4.18s/it]loss: 0.2016550898551941, grad_norm: 0.22934679687023163\n",
            "Training:  89% 202/226 [17:15<01:31,  3.79s/it]loss: 0.2739179730415344, grad_norm: 0.2412467747926712\n",
            "Training:  90% 203/226 [17:17<01:18,  3.40s/it]loss: 0.23250558972358704, grad_norm: 0.2626359760761261\n",
            "Training:  90% 204/226 [17:20<01:11,  3.24s/it]loss: 0.3089887499809265, grad_norm: 0.34594446420669556\n",
            "Training:  91% 205/226 [17:28<01:35,  4.55s/it]loss: 0.11113464087247849, grad_norm: 0.14704661071300507\n",
            "Training:  91% 206/226 [17:38<02:01,  6.07s/it]loss: 0.20978787541389465, grad_norm: 0.2475091814994812\n",
            "Training:  92% 207/226 [17:41<01:38,  5.20s/it]loss: 0.21220079064369202, grad_norm: 0.21984493732452393\n",
            "Training:  92% 208/226 [17:44<01:25,  4.74s/it]loss: 0.20353209972381592, grad_norm: 0.18231408298015594\n",
            "Training:  92% 209/226 [17:49<01:17,  4.56s/it]loss: 0.20373612642288208, grad_norm: 0.2236524075269699\n",
            "Training:  93% 210/226 [17:54<01:18,  4.88s/it]loss: 0.15971294045448303, grad_norm: 0.1500321477651596\n",
            "Training:  93% 211/226 [17:59<01:11,  4.76s/it]loss: 0.1402755230665207, grad_norm: 1.2821251153945923\n",
            "Training:  94% 212/226 [18:05<01:13,  5.26s/it]loss: 0.25382789969444275, grad_norm: 0.26473844051361084\n",
            "Training:  94% 213/226 [18:08<01:00,  4.63s/it]loss: 0.27211281657218933, grad_norm: 0.3111608028411865\n",
            "Training:  95% 214/226 [18:11<00:49,  4.14s/it]loss: 0.2039448320865631, grad_norm: 1.0267465114593506\n",
            "Training:  95% 215/226 [18:21<01:04,  5.85s/it]loss: 0.1810498982667923, grad_norm: 0.3705613315105438\n",
            "Training:  96% 216/226 [18:27<00:58,  5.80s/it]loss: 0.14097025990486145, grad_norm: 0.21430280804634094\n",
            "Training:  96% 217/226 [18:33<00:53,  5.96s/it]loss: 0.16627183556556702, grad_norm: 0.37998947501182556\n",
            "Training:  96% 218/226 [18:39<00:47,  5.94s/it]loss: 0.19162620604038239, grad_norm: 0.32415124773979187\n",
            "Training:  97% 219/226 [18:43<00:37,  5.35s/it]loss: 0.210722878575325, grad_norm: 1.5753588676452637\n",
            "Training:  97% 220/226 [18:47<00:30,  5.05s/it]loss: 0.2320985347032547, grad_norm: 0.26587966084480286\n",
            "Training:  98% 221/226 [18:51<00:23,  4.68s/it]loss: 0.1533513069152832, grad_norm: 0.3120729625225067\n",
            "Training:  98% 222/226 [18:56<00:18,  4.71s/it]loss: 0.14175784587860107, grad_norm: 0.7185941338539124\n",
            "Training:  99% 223/226 [19:04<00:17,  5.82s/it]loss: 0.24655987322330475, grad_norm: 0.3531814515590668\n",
            "Training:  99% 224/226 [19:08<00:10,  5.19s/it]loss: 0.21005752682685852, grad_norm: 0.26257309317588806\n",
            "Training: 100% 225/226 [19:20<00:07,  7.37s/it]loss: 0.1744478940963745, grad_norm: 0.20894934237003326\n",
            "Training: 100% 226/226 [19:25<00:00,  5.16s/it]\n",
            "Validation loss: 0.6992669415473938\n",
            "Epoch: 7\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.1819552183151245, grad_norm: 0.9604895710945129\n",
            "Training:   0% 1/226 [00:05<19:14,  5.13s/it]loss: 0.3337692618370056, grad_norm: 0.4243347644805908\n",
            "Training:   1% 2/226 [00:07<12:43,  3.41s/it]loss: 0.1679726243019104, grad_norm: 0.3951717019081116\n",
            "Training:   1% 3/226 [00:14<18:44,  5.04s/it]loss: 0.2730158865451813, grad_norm: 0.35434702038764954\n",
            "Training:   2% 4/226 [00:16<14:17,  3.86s/it]loss: 0.17815415561199188, grad_norm: 0.23919007182121277\n",
            "Training:   2% 5/226 [00:21<15:45,  4.28s/it]loss: 0.20421870052814484, grad_norm: 0.3186311423778534\n",
            "Training:   3% 6/226 [00:27<18:27,  5.03s/it]loss: 0.23713544011116028, grad_norm: 2.8714020252227783\n",
            "Training:   3% 7/226 [00:31<16:29,  4.52s/it]loss: 0.3120831251144409, grad_norm: 1.9491145610809326\n",
            "Training:   4% 8/226 [00:33<13:56,  3.84s/it]loss: 0.24456913769245148, grad_norm: 0.26827913522720337\n",
            "Training:   4% 9/226 [00:39<15:32,  4.30s/it]loss: 0.2699767053127289, grad_norm: 0.33584606647491455\n",
            "Training:   4% 10/226 [00:42<14:40,  4.08s/it]loss: 0.23684170842170715, grad_norm: 0.33217474818229675\n",
            "Training:   5% 11/226 [00:47<15:21,  4.29s/it]loss: 0.20584958791732788, grad_norm: 0.28390640020370483\n",
            "Training:   5% 12/226 [00:54<18:43,  5.25s/it]loss: 0.22747160494327545, grad_norm: 0.3473312258720398\n",
            "Training:   6% 13/226 [00:59<17:42,  4.99s/it]loss: 0.28146424889564514, grad_norm: 0.41200530529022217\n",
            "Training:   6% 14/226 [01:01<15:12,  4.31s/it]loss: 0.20740251243114471, grad_norm: 0.29229193925857544\n",
            "Training:   7% 15/226 [01:08<17:04,  4.85s/it]loss: 0.2069465070962906, grad_norm: 0.25120338797569275\n",
            "Training:   7% 16/226 [01:13<17:44,  5.07s/it]loss: 0.15590687096118927, grad_norm: 0.17224721610546112\n",
            "Training:   8% 17/226 [01:20<19:15,  5.53s/it]loss: 0.2656775116920471, grad_norm: 0.27770066261291504\n",
            "Training:   8% 18/226 [01:23<16:33,  4.77s/it]loss: 0.2543022036552429, grad_norm: 0.5648072957992554\n",
            "Training:   8% 19/226 [01:33<22:05,  6.40s/it]loss: 0.2519995868206024, grad_norm: 0.38029077649116516\n",
            "Training:   9% 20/226 [01:36<18:25,  5.36s/it]loss: 0.19891305267810822, grad_norm: 0.2932010591030121\n",
            "Training:   9% 21/226 [01:40<17:29,  5.12s/it]loss: 0.2670938968658447, grad_norm: 0.7387202382087708\n",
            "Training:  10% 22/226 [01:45<16:42,  4.92s/it]loss: 0.14859411120414734, grad_norm: 0.15823809802532196\n",
            "Training:  10% 23/226 [01:54<20:49,  6.16s/it]loss: 0.2054547816514969, grad_norm: 0.2700265944004059\n",
            "Training:  11% 24/226 [01:59<19:31,  5.80s/it]loss: 0.1830504685640335, grad_norm: 0.2704794704914093\n",
            "Training:  11% 25/226 [02:03<17:27,  5.21s/it]loss: 0.20418760180473328, grad_norm: 0.24124394357204437\n",
            "Training:  12% 26/226 [02:08<16:56,  5.08s/it]loss: 0.24374976754188538, grad_norm: 0.277040034532547\n",
            "Training:  12% 27/226 [02:11<15:15,  4.60s/it]loss: 0.13572673499584198, grad_norm: 0.16463688015937805\n",
            "Training:  12% 28/226 [02:19<18:12,  5.52s/it]loss: 0.2510961890220642, grad_norm: 0.9702580571174622\n",
            "Training:  13% 29/226 [02:30<24:18,  7.40s/it]loss: 0.18969914317131042, grad_norm: 1.248643159866333\n",
            "Training:  13% 30/226 [02:35<21:30,  6.59s/it]loss: 0.2022453099489212, grad_norm: 0.23402750492095947\n",
            "Training:  14% 31/226 [02:38<17:57,  5.53s/it]loss: 0.2526477873325348, grad_norm: 0.3502497971057892\n",
            "Training:  14% 32/226 [02:41<15:40,  4.85s/it]loss: 0.2241378277540207, grad_norm: 0.275743305683136\n",
            "Training:  15% 33/226 [02:46<15:07,  4.70s/it]loss: 0.19883985817432404, grad_norm: 1.9699302911758423\n",
            "Training:  15% 34/226 [02:49<13:56,  4.35s/it]loss: 0.20765160024166107, grad_norm: 0.27240481972694397\n",
            "Training:  15% 35/226 [02:54<14:26,  4.53s/it]loss: 0.21519067883491516, grad_norm: 0.2952495217323303\n",
            "Training:  16% 36/226 [02:58<13:46,  4.35s/it]loss: 0.20883400738239288, grad_norm: 0.3084363043308258\n",
            "Training:  16% 37/226 [03:02<13:03,  4.15s/it]loss: 0.2740625739097595, grad_norm: 1.3637216091156006\n",
            "Training:  17% 38/226 [03:06<13:05,  4.18s/it]loss: 0.2275821417570114, grad_norm: 0.4469617009162903\n",
            "Training:  17% 39/226 [03:15<17:48,  5.71s/it]loss: 0.16362972557544708, grad_norm: 1.017808198928833\n",
            "Training:  18% 40/226 [03:22<18:14,  5.88s/it]loss: 0.2325466126203537, grad_norm: 0.38899359107017517\n",
            "Training:  18% 41/226 [03:25<15:45,  5.11s/it]loss: 0.2746330797672272, grad_norm: 0.4020270109176636\n",
            "Training:  19% 42/226 [03:30<15:06,  4.93s/it]loss: 0.19718602299690247, grad_norm: 0.47437605261802673\n",
            "Training:  19% 43/226 [03:39<19:22,  6.35s/it]loss: 0.12439382821321487, grad_norm: 0.1923474669456482\n",
            "Training:  19% 44/226 [03:51<23:45,  7.83s/it]loss: 0.31588155031204224, grad_norm: 1.5830078125\n",
            "Training:  20% 45/226 [03:53<18:54,  6.27s/it]loss: 0.19182994961738586, grad_norm: 0.1958102434873581\n",
            "Training:  20% 46/226 [03:58<17:54,  5.97s/it]loss: 0.33514901995658875, grad_norm: 0.8170613050460815\n",
            "Training:  21% 47/226 [04:02<15:19,  5.14s/it]loss: 0.2769564688205719, grad_norm: 0.42537355422973633\n",
            "Training:  21% 48/226 [04:06<14:19,  4.83s/it]loss: 0.2994067668914795, grad_norm: 0.545557975769043\n",
            "Training:  22% 49/226 [04:15<18:02,  6.12s/it]loss: 0.20410795509815216, grad_norm: 0.28064924478530884\n",
            "Training:  22% 50/226 [04:22<18:30,  6.31s/it]loss: 0.2835342288017273, grad_norm: 0.3832954466342926\n",
            "Training:  23% 51/226 [04:24<14:36,  5.01s/it]loss: 0.17841371893882751, grad_norm: 0.2516735792160034\n",
            "Training:  23% 52/226 [04:32<17:26,  6.02s/it]loss: 0.2302958071231842, grad_norm: 0.40924596786499023\n",
            "Training:  23% 53/226 [04:35<15:13,  5.28s/it]loss: 0.21982382237911224, grad_norm: 0.26700982451438904\n",
            "Training:  24% 54/226 [04:41<15:21,  5.36s/it]loss: 0.22436633706092834, grad_norm: 0.26804131269454956\n",
            "Training:  24% 55/226 [04:48<16:35,  5.82s/it]loss: 0.21597599983215332, grad_norm: 0.29526305198669434\n",
            "Training:  25% 56/226 [04:51<13:48,  4.88s/it]loss: 0.19629046320915222, grad_norm: 0.2953689694404602\n",
            "Training:  25% 57/226 [04:55<13:29,  4.79s/it]loss: 0.2554191052913666, grad_norm: 0.35230541229248047\n",
            "Training:  26% 58/226 [04:58<11:31,  4.12s/it]loss: 0.20016725361347198, grad_norm: 0.31023773550987244\n",
            "Training:  26% 59/226 [05:08<16:39,  5.99s/it]loss: 0.16482239961624146, grad_norm: 0.19617430865764618\n",
            "Training:  27% 60/226 [05:16<18:20,  6.63s/it]loss: 0.2279684990644455, grad_norm: 0.2780517339706421\n",
            "Training:  27% 61/226 [05:20<15:49,  5.75s/it]loss: 0.22334788739681244, grad_norm: 0.2507694661617279\n",
            "Training:  27% 62/226 [05:24<14:34,  5.33s/it]loss: 0.25135084986686707, grad_norm: 0.24679669737815857\n",
            "Training:  28% 63/226 [05:28<12:52,  4.74s/it]loss: 0.2727772891521454, grad_norm: 0.3337997496128082\n",
            "Training:  28% 64/226 [05:31<11:34,  4.29s/it]loss: 0.15681792795658112, grad_norm: 0.20305849611759186\n",
            "Training:  29% 65/226 [05:36<12:20,  4.60s/it]loss: 0.22308024764060974, grad_norm: 0.35554343461990356\n",
            "Training:  29% 66/226 [05:39<10:50,  4.07s/it]loss: 0.19429247081279755, grad_norm: 0.3355920612812042\n",
            "Training:  30% 67/226 [05:43<10:24,  3.93s/it]loss: 0.1601824015378952, grad_norm: 0.475170373916626\n",
            "Training:  30% 68/226 [05:47<10:43,  4.07s/it]loss: 0.25246086716651917, grad_norm: 0.26012179255485535\n",
            "Training:  31% 69/226 [05:57<15:30,  5.93s/it]loss: 0.1709473580121994, grad_norm: 0.18852770328521729\n",
            "Training:  31% 70/226 [06:02<14:11,  5.46s/it]loss: 0.14066538214683533, grad_norm: 0.15939223766326904\n",
            "Training:  31% 71/226 [06:08<14:45,  5.71s/it]loss: 0.19853796064853668, grad_norm: 0.21069572865962982\n",
            "Training:  32% 72/226 [06:14<14:35,  5.68s/it]loss: 0.1680375039577484, grad_norm: 0.20318731665611267\n",
            "Training:  32% 73/226 [06:19<13:55,  5.46s/it]loss: 0.20737899839878082, grad_norm: 0.20756474137306213\n",
            "Training:  33% 74/226 [06:24<14:03,  5.55s/it]loss: 0.22784066200256348, grad_norm: 0.3283361494541168\n",
            "Training:  33% 75/226 [06:29<13:19,  5.30s/it]loss: 0.26564544439315796, grad_norm: 0.54108726978302\n",
            "Training:  34% 76/226 [06:32<11:29,  4.60s/it]loss: 0.21613505482673645, grad_norm: 0.256938636302948\n",
            "Training:  34% 77/226 [06:35<10:17,  4.14s/it]loss: 0.13277263939380646, grad_norm: 0.14957928657531738\n",
            "Training:  35% 78/226 [06:42<12:11,  4.94s/it]loss: 0.1827063113451004, grad_norm: 0.33573365211486816\n",
            "Training:  35% 79/226 [06:53<16:45,  6.84s/it]loss: 0.18868793547153473, grad_norm: 0.25763317942619324\n",
            "Training:  35% 80/226 [06:58<15:06,  6.21s/it]loss: 0.20969067513942719, grad_norm: 0.3015117347240448\n",
            "Training:  36% 81/226 [07:02<13:48,  5.71s/it]loss: 0.20707222819328308, grad_norm: 0.24147923290729523\n",
            "Training:  36% 82/226 [07:06<12:27,  5.19s/it]loss: 0.14712278544902802, grad_norm: 0.15802879631519318\n",
            "Training:  37% 83/226 [07:11<11:57,  5.02s/it]loss: 0.20918159186840057, grad_norm: 0.29090356826782227\n",
            "Training:  37% 84/226 [07:15<10:59,  4.64s/it]loss: 0.21667973697185516, grad_norm: 0.2412368655204773\n",
            "Training:  38% 85/226 [07:18<09:50,  4.19s/it]loss: 0.2617972493171692, grad_norm: 0.34708672761917114\n",
            "Training:  38% 86/226 [07:21<08:55,  3.83s/it]loss: 0.15536801517009735, grad_norm: 0.18104062974452972\n",
            "Training:  38% 87/226 [07:27<10:21,  4.47s/it]loss: 0.18793752789497375, grad_norm: 0.24974967539310455\n",
            "Training:  39% 88/226 [07:31<09:50,  4.28s/it]loss: 0.2287905216217041, grad_norm: 0.32469049096107483\n",
            "Training:  39% 89/226 [07:40<13:04,  5.73s/it]loss: 0.2015857696533203, grad_norm: 0.18290135264396667\n",
            "Training:  40% 90/226 [07:46<13:10,  5.81s/it]loss: 0.2332393079996109, grad_norm: 0.2878389060497284\n",
            "Training:  40% 91/226 [07:49<11:07,  4.94s/it]loss: 0.1601797491312027, grad_norm: 0.17522640526294708\n",
            "Training:  41% 92/226 [07:53<10:48,  4.84s/it]loss: 0.2926028072834015, grad_norm: 0.3485504388809204\n",
            "Training:  41% 93/226 [07:57<09:50,  4.44s/it]loss: 0.21121026575565338, grad_norm: 0.2243954837322235\n",
            "Training:  42% 94/226 [08:01<09:19,  4.24s/it]loss: 0.16837918758392334, grad_norm: 0.16799287497997284\n",
            "Training:  42% 95/226 [08:05<09:08,  4.19s/it]loss: 0.36447492241859436, grad_norm: 0.38597479462623596\n",
            "Training:  42% 96/226 [08:06<07:28,  3.45s/it]loss: 0.11886485666036606, grad_norm: 0.1419467329978943\n",
            "Training:  43% 97/226 [08:15<10:28,  4.87s/it]loss: 0.2182835191488266, grad_norm: 0.2199971228837967\n",
            "Training:  43% 98/226 [08:20<10:53,  5.11s/it]loss: 0.20562954246997833, grad_norm: 0.20021608471870422\n",
            "Training:  44% 99/226 [08:31<14:20,  6.78s/it]loss: 0.2086624652147293, grad_norm: 0.16402530670166016\n",
            "Training:  44% 100/226 [08:38<14:16,  6.80s/it]loss: 0.13022296130657196, grad_norm: 0.11453685909509659\n",
            "Training:  45% 101/226 [08:45<14:11,  6.81s/it]loss: 0.24704501032829285, grad_norm: 0.24549855291843414\n",
            "Training:  45% 102/226 [08:47<11:34,  5.60s/it]loss: 0.19231952726840973, grad_norm: 0.22442467510700226\n",
            "Training:  46% 103/226 [08:51<10:04,  4.92s/it]loss: 0.20015519857406616, grad_norm: 0.18497516214847565\n",
            "Training:  46% 104/226 [08:55<09:23,  4.62s/it]loss: 0.15969476103782654, grad_norm: 0.1273534744977951\n",
            "Training:  46% 105/226 [08:59<09:07,  4.53s/it]loss: 0.15747493505477905, grad_norm: 0.15098705887794495\n",
            "Training:  47% 106/226 [09:04<09:27,  4.73s/it]loss: 0.1776067018508911, grad_norm: 0.15792226791381836\n",
            "Training:  47% 107/226 [09:11<10:24,  5.25s/it]loss: 0.15300428867340088, grad_norm: 0.14031779766082764\n",
            "Training:  48% 108/226 [09:17<10:44,  5.46s/it]loss: 0.13541731238365173, grad_norm: 0.15937067568302155\n",
            "Training:  48% 109/226 [09:29<14:35,  7.48s/it]loss: 0.24533887207508087, grad_norm: 0.20689839124679565\n",
            "Training:  49% 110/226 [09:31<11:36,  6.01s/it]loss: 0.1739007532596588, grad_norm: 0.9469533562660217\n",
            "Training:  49% 111/226 [09:35<10:23,  5.43s/it]loss: 0.13381551206111908, grad_norm: 0.28271883726119995\n",
            "Training:  50% 112/226 [09:41<10:18,  5.42s/it]loss: 0.11870641261339188, grad_norm: 0.26731324195861816\n",
            "Training:  50% 113/226 [09:50<12:25,  6.60s/it]loss: 0.21600228548049927, grad_norm: 0.26983219385147095\n",
            "Training:  50% 114/226 [09:54<10:59,  5.89s/it]loss: 0.14415571093559265, grad_norm: 0.19659362733364105\n",
            "Training:  51% 115/226 [10:01<11:12,  6.06s/it]loss: 0.16301791369915009, grad_norm: 0.19366635382175446\n",
            "Training:  51% 116/226 [10:06<10:52,  5.93s/it]loss: 0.16944827139377594, grad_norm: 0.17899399995803833\n",
            "Training:  52% 117/226 [10:11<10:13,  5.63s/it]loss: 0.21755151450634003, grad_norm: 0.202852264046669\n",
            "Training:  52% 118/226 [10:15<08:55,  4.96s/it]loss: 0.21523292362689972, grad_norm: 0.22458887100219727\n",
            "Training:  53% 119/226 [10:26<12:02,  6.76s/it]loss: 0.27592241764068604, grad_norm: 0.22881902754306793\n",
            "Training:  53% 120/226 [10:29<10:04,  5.70s/it]loss: 0.17325958609580994, grad_norm: 0.15181294083595276\n",
            "Training:  54% 121/226 [10:35<10:21,  5.92s/it]loss: 0.25922900438308716, grad_norm: 0.22758933901786804\n",
            "Training:  54% 122/226 [10:38<08:42,  5.03s/it]loss: 0.201095312833786, grad_norm: 0.22707222402095795\n",
            "Training:  54% 123/226 [10:44<08:50,  5.15s/it]loss: 0.2154727727174759, grad_norm: 0.427356094121933\n",
            "Training:  55% 124/226 [10:48<08:06,  4.77s/it]loss: 0.22715316712856293, grad_norm: 0.18559879064559937\n",
            "Training:  55% 125/226 [10:52<07:48,  4.64s/it]loss: 0.3105855882167816, grad_norm: 0.3371812105178833\n",
            "Training:  56% 126/226 [10:53<06:09,  3.70s/it]loss: 0.2879205346107483, grad_norm: 0.28606224060058594\n",
            "Training:  56% 127/226 [10:55<05:02,  3.05s/it]loss: 0.18158391118049622, grad_norm: 0.376138836145401\n",
            "Training:  57% 128/226 [10:59<05:41,  3.48s/it]loss: 0.15868103504180908, grad_norm: 0.14425811171531677\n",
            "Training:  57% 129/226 [11:07<07:31,  4.66s/it]loss: 0.19316056370735168, grad_norm: 0.1856199949979782\n",
            "Training:  58% 130/226 [11:10<06:50,  4.28s/it]loss: 0.18938829004764557, grad_norm: 0.23633044958114624\n",
            "Training:  58% 131/226 [11:14<06:32,  4.13s/it]loss: 0.15543411672115326, grad_norm: 0.14833968877792358\n",
            "Training:  58% 132/226 [11:20<07:16,  4.64s/it]loss: 0.3445303738117218, grad_norm: 0.49170395731925964\n",
            "Training:  59% 133/226 [11:22<05:52,  3.79s/it]loss: 0.15866436064243317, grad_norm: 0.20042279362678528\n",
            "Training:  59% 134/226 [11:27<06:33,  4.27s/it]loss: 0.22830277681350708, grad_norm: 0.2099049687385559\n",
            "Training:  60% 135/226 [11:32<06:32,  4.32s/it]loss: 0.1966383159160614, grad_norm: 0.2151367962360382\n",
            "Training:  60% 136/226 [11:35<06:18,  4.21s/it]loss: 0.15428808331489563, grad_norm: 0.2876858115196228\n",
            "Training:  61% 137/226 [11:42<07:05,  4.78s/it]loss: 0.21651428937911987, grad_norm: 0.2217046171426773\n",
            "Training:  61% 138/226 [11:45<06:24,  4.37s/it]loss: 0.2817327082157135, grad_norm: 0.26177066564559937\n",
            "Training:  62% 139/226 [11:54<08:16,  5.71s/it]loss: 0.29915115237236023, grad_norm: 0.3489251434803009\n",
            "Training:  62% 140/226 [11:56<06:34,  4.59s/it]loss: 0.26266786456108093, grad_norm: 0.32724544405937195\n",
            "Training:  62% 141/226 [11:59<05:43,  4.04s/it]loss: 0.17986081540584564, grad_norm: 0.19452627003192902\n",
            "Training:  63% 142/226 [12:04<06:18,  4.51s/it]loss: 0.11732662469148636, grad_norm: 0.13509953022003174\n",
            "Training:  63% 143/226 [12:13<08:08,  5.89s/it]loss: 0.1790732890367508, grad_norm: 0.45153433084487915\n",
            "Training:  64% 144/226 [12:19<07:59,  5.84s/it]loss: 0.1536097228527069, grad_norm: 0.18125638365745544\n",
            "Training:  64% 145/226 [12:23<07:18,  5.41s/it]loss: 0.17594321072101593, grad_norm: 0.20497514307498932\n",
            "Training:  65% 146/226 [12:29<07:27,  5.59s/it]loss: 0.21128544211387634, grad_norm: 0.19495128095149994\n",
            "Training:  65% 147/226 [12:34<06:54,  5.24s/it]loss: 0.14687535166740417, grad_norm: 0.14698566496372223\n",
            "Training:  65% 148/226 [12:39<06:34,  5.06s/it]loss: 0.18186800181865692, grad_norm: 0.15669068694114685\n",
            "Training:  66% 149/226 [12:51<09:27,  7.37s/it]loss: 0.2689758241176605, grad_norm: 0.35977014899253845\n",
            "Training:  66% 150/226 [12:55<07:47,  6.16s/it]loss: 0.20786206424236298, grad_norm: 0.44146570563316345\n",
            "Training:  67% 151/226 [12:59<07:02,  5.63s/it]loss: 0.21362657845020294, grad_norm: 0.373592346906662\n",
            "Training:  67% 152/226 [13:02<06:06,  4.95s/it]loss: 0.2008248269557953, grad_norm: 0.2259514033794403\n",
            "Training:  68% 153/226 [13:06<05:23,  4.43s/it]loss: 0.29810941219329834, grad_norm: 0.39698123931884766\n",
            "Training:  68% 154/226 [13:07<04:20,  3.61s/it]loss: 0.24392341077327728, grad_norm: 0.4004333019256592\n",
            "Training:  69% 155/226 [13:10<03:48,  3.22s/it]loss: 0.28228044509887695, grad_norm: 0.3095548152923584\n",
            "Training:  69% 156/226 [13:13<03:45,  3.21s/it]loss: 0.17288178205490112, grad_norm: 0.21653850376605988\n",
            "Training:  69% 157/226 [13:18<04:13,  3.68s/it]loss: 0.18882444500923157, grad_norm: 0.23787204921245575\n",
            "Training:  70% 158/226 [13:22<04:25,  3.90s/it]loss: 0.22732041776180267, grad_norm: 0.21163420379161835\n",
            "Training:  70% 159/226 [13:28<05:05,  4.56s/it]loss: 0.2045823633670807, grad_norm: 1.3569047451019287\n",
            "Training:  71% 160/226 [13:31<04:26,  4.04s/it]loss: 0.21071822941303253, grad_norm: 0.24223598837852478\n",
            "Training:  71% 161/226 [13:34<04:07,  3.81s/it]loss: 0.21450835466384888, grad_norm: 0.2648710012435913\n",
            "Training:  72% 162/226 [13:38<04:07,  3.87s/it]loss: 0.1964014768600464, grad_norm: 0.26895761489868164\n",
            "Training:  72% 163/226 [13:43<04:28,  4.26s/it]loss: 0.14244741201400757, grad_norm: 0.2740941643714905\n",
            "Training:  73% 164/226 [13:51<05:27,  5.29s/it]loss: 0.29119813442230225, grad_norm: 0.29326269030570984\n",
            "Training:  73% 165/226 [13:57<05:41,  5.60s/it]loss: 0.24471163749694824, grad_norm: 0.30236923694610596\n",
            "Training:  73% 166/226 [14:00<04:37,  4.62s/it]loss: 0.27894821763038635, grad_norm: 0.3933168053627014\n",
            "Training:  74% 167/226 [14:02<03:50,  3.91s/it]loss: 0.183473140001297, grad_norm: 0.2504788041114807\n",
            "Training:  74% 168/226 [14:07<03:59,  4.12s/it]loss: 0.17491739988327026, grad_norm: 0.20204269886016846\n",
            "Training:  75% 169/226 [14:17<05:48,  6.11s/it]loss: 0.17120777070522308, grad_norm: 0.30258816480636597\n",
            "Training:  75% 170/226 [14:22<05:14,  5.62s/it]loss: 0.18159152567386627, grad_norm: 0.1803896129131317\n",
            "Training:  76% 171/226 [14:26<04:41,  5.12s/it]loss: 0.1836291402578354, grad_norm: 0.2060680091381073\n",
            "Training:  76% 172/226 [14:31<04:42,  5.23s/it]loss: 0.21385137736797333, grad_norm: 0.17671063542366028\n",
            "Training:  77% 173/226 [14:37<04:39,  5.27s/it]loss: 0.2912813127040863, grad_norm: 0.32596713304519653\n",
            "Training:  77% 174/226 [14:40<04:06,  4.73s/it]loss: 0.16807028651237488, grad_norm: 0.1790042221546173\n",
            "Training:  77% 175/226 [14:45<04:08,  4.88s/it]loss: 0.22321584820747375, grad_norm: 0.2170889526605606\n",
            "Training:  78% 176/226 [14:48<03:31,  4.24s/it]loss: 0.18951788544654846, grad_norm: 0.17260253429412842\n",
            "Training:  78% 177/226 [14:52<03:29,  4.28s/it]loss: 0.1757446825504303, grad_norm: 0.1860680878162384\n",
            "Training:  79% 178/226 [14:59<03:56,  4.93s/it]loss: 0.2328837513923645, grad_norm: 0.2091471552848816\n",
            "Training:  79% 179/226 [15:08<04:58,  6.35s/it]loss: 0.16919706761837006, grad_norm: 0.14327608048915863\n",
            "Training:  80% 180/226 [15:13<04:24,  5.74s/it]loss: 0.16024667024612427, grad_norm: 0.1566677987575531\n",
            "Training:  80% 181/226 [15:21<04:46,  6.36s/it]loss: 0.1715734750032425, grad_norm: 0.16558007895946503\n",
            "Training:  81% 182/226 [15:24<04:00,  5.45s/it]loss: 0.13529841601848602, grad_norm: 0.10434133559465408\n",
            "Training:  81% 183/226 [15:33<04:42,  6.56s/it]loss: 0.19792647659778595, grad_norm: 0.151902973651886\n",
            "Training:  81% 184/226 [15:37<04:07,  5.88s/it]loss: 0.22485493123531342, grad_norm: 0.21361508965492249\n",
            "Training:  82% 185/226 [15:42<03:47,  5.55s/it]loss: 0.13765770196914673, grad_norm: 0.12936072051525116\n",
            "Training:  82% 186/226 [15:51<04:26,  6.67s/it]loss: 0.24544547498226166, grad_norm: 0.32314333319664\n",
            "Training:  83% 187/226 [15:54<03:29,  5.38s/it]loss: 0.28525903820991516, grad_norm: 0.26134032011032104\n",
            "Training:  83% 188/226 [15:57<02:55,  4.63s/it]loss: 0.21851898729801178, grad_norm: 0.19088472425937653\n",
            "Training:  84% 189/226 [16:06<03:42,  6.00s/it]loss: 0.1697578728199005, grad_norm: 0.16136062145233154\n",
            "Training:  84% 190/226 [16:11<03:23,  5.65s/it]loss: 0.16213028132915497, grad_norm: 0.20827117562294006\n",
            "Training:  85% 191/226 [16:15<03:07,  5.36s/it]loss: 0.24513502418994904, grad_norm: 0.2570303976535797\n",
            "Training:  85% 192/226 [16:18<02:34,  4.56s/it]loss: 0.18868659436702728, grad_norm: 0.15214744210243225\n",
            "Training:  85% 193/226 [16:23<02:33,  4.66s/it]loss: 0.25934141874313354, grad_norm: 0.27789506316185\n",
            "Training:  86% 194/226 [16:26<02:11,  4.12s/it]loss: 0.20637467503547668, grad_norm: 0.1638384610414505\n",
            "Training:  86% 195/226 [16:30<02:10,  4.20s/it]loss: 0.20723265409469604, grad_norm: 0.15710845589637756\n",
            "Training:  87% 196/226 [16:33<01:53,  3.80s/it]loss: 0.20323920249938965, grad_norm: 0.2376996874809265\n",
            "Training:  87% 197/226 [16:36<01:44,  3.60s/it]loss: 0.14637412130832672, grad_norm: 0.16492117941379547\n",
            "Training:  88% 198/226 [16:42<02:02,  4.36s/it]loss: 0.18348154425621033, grad_norm: 0.19433198869228363\n",
            "Training:  88% 199/226 [16:53<02:48,  6.24s/it]loss: 0.15814191102981567, grad_norm: 0.5752929449081421\n",
            "Training:  88% 200/226 [16:58<02:32,  5.86s/it]loss: 0.10562360286712646, grad_norm: 0.15840089321136475\n",
            "Training:  89% 201/226 [17:06<02:41,  6.44s/it]loss: 0.22235193848609924, grad_norm: 0.3178858160972595\n",
            "Training:  89% 202/226 [17:09<02:14,  5.59s/it]loss: 0.17497962713241577, grad_norm: 0.1549672931432724\n",
            "Training:  90% 203/226 [17:15<02:05,  5.48s/it]loss: 0.3233824074268341, grad_norm: 0.4106180667877197\n",
            "Training:  90% 204/226 [17:16<01:35,  4.36s/it]loss: 0.10670461505651474, grad_norm: 0.11544959992170334\n",
            "Training:  91% 205/226 [17:25<01:58,  5.63s/it]loss: 0.23503151535987854, grad_norm: 0.23024217784404755\n",
            "Training:  91% 206/226 [17:29<01:44,  5.22s/it]loss: 0.15514634549617767, grad_norm: 0.19147436320781708\n",
            "Training:  92% 207/226 [17:34<01:37,  5.13s/it]loss: 0.23579399287700653, grad_norm: 0.669891893863678\n",
            "Training:  92% 208/226 [17:37<01:18,  4.37s/it]loss: 0.1439591646194458, grad_norm: 0.1691938191652298\n",
            "Training:  92% 209/226 [17:50<02:01,  7.15s/it]loss: 0.19089210033416748, grad_norm: 0.19202925264835358\n",
            "Training:  93% 210/226 [17:54<01:37,  6.09s/it]loss: 0.17947758734226227, grad_norm: 0.33011171221733093\n",
            "Training:  93% 211/226 [17:59<01:25,  5.72s/it]loss: 0.20094507932662964, grad_norm: 0.25231602787971497\n",
            "Training:  94% 212/226 [18:03<01:14,  5.36s/it]loss: 0.23694546520709991, grad_norm: 0.23690637946128845\n",
            "Training:  94% 213/226 [18:07<01:04,  4.95s/it]loss: 0.1967799961566925, grad_norm: 0.197562113404274\n",
            "Training:  95% 214/226 [18:11<00:55,  4.63s/it]loss: 0.18155036866664886, grad_norm: 0.16693226993083954\n",
            "Training:  95% 215/226 [18:16<00:50,  4.62s/it]loss: 0.2704387307167053, grad_norm: 0.3235633075237274\n",
            "Training:  96% 216/226 [18:18<00:38,  3.89s/it]loss: 0.1653304100036621, grad_norm: 0.17677444219589233\n",
            "Training:  96% 217/226 [18:24<00:40,  4.47s/it]loss: 0.14606675505638123, grad_norm: 0.8980093002319336\n",
            "Training:  96% 218/226 [18:30<00:40,  5.06s/it]loss: 0.27470389008522034, grad_norm: 0.325244665145874\n",
            "Training:  97% 219/226 [18:38<00:40,  5.74s/it]loss: 0.2543646991252899, grad_norm: 0.2801675796508789\n",
            "Training:  97% 220/226 [18:40<00:29,  4.87s/it]loss: 0.21781618893146515, grad_norm: 0.239421546459198\n",
            "Training:  98% 221/226 [18:45<00:23,  4.78s/it]loss: 0.17422448098659515, grad_norm: 0.3015597462654114\n",
            "Training:  98% 222/226 [18:49<00:17,  4.41s/it]loss: 0.27058514952659607, grad_norm: 0.3573864698410034\n",
            "Training:  99% 223/226 [18:51<00:11,  3.96s/it]loss: 0.25129434466362, grad_norm: 0.33254745602607727\n",
            "Training:  99% 224/226 [18:57<00:08,  4.32s/it]loss: 0.22045791149139404, grad_norm: 0.20684672892093658\n",
            "Training: 100% 225/226 [19:00<00:03,  3.91s/it]loss: 0.2141379415988922, grad_norm: 0.2284633070230484\n",
            "Training: 100% 226/226 [19:03<00:00,  5.06s/it]\n",
            "Validation loss: 0.7597374773025513\n",
            "Epoch: 8\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.14735472202301025, grad_norm: 0.18628783524036407\n",
            "Training:   0% 1/226 [00:06<24:18,  6.48s/it]loss: 0.162009596824646, grad_norm: 0.1606660783290863\n",
            "Training:   1% 2/226 [00:11<21:47,  5.84s/it]loss: 0.33966004848480225, grad_norm: 0.38569092750549316\n",
            "Training:   1% 3/226 [00:14<16:28,  4.43s/it]loss: 0.21046443283557892, grad_norm: 0.17280322313308716\n",
            "Training:   2% 4/226 [00:18<15:45,  4.26s/it]loss: 0.16999441385269165, grad_norm: 0.36688345670700073\n",
            "Training:   2% 5/226 [00:24<18:12,  4.94s/it]loss: 0.23273280262947083, grad_norm: 0.2041717767715454\n",
            "Training:   3% 6/226 [00:29<17:34,  4.79s/it]loss: 0.21847966313362122, grad_norm: 0.18964022397994995\n",
            "Training:   3% 7/226 [00:32<15:09,  4.15s/it]loss: 0.2345985621213913, grad_norm: 1.3706737756729126\n",
            "Training:   4% 8/226 [00:36<15:23,  4.24s/it]loss: 0.15741582214832306, grad_norm: 0.14024782180786133\n",
            "Training:   4% 9/226 [00:41<15:49,  4.38s/it]loss: 0.172649547457695, grad_norm: 0.7243035435676575\n",
            "Training:   4% 10/226 [00:46<16:15,  4.52s/it]loss: 0.2360595464706421, grad_norm: 0.2979050874710083\n",
            "Training:   5% 11/226 [00:50<15:56,  4.45s/it]loss: 0.13786612451076508, grad_norm: 0.13268804550170898\n",
            "Training:   5% 12/226 [00:59<20:34,  5.77s/it]loss: 0.3730373978614807, grad_norm: 0.7776580452919006\n",
            "Training:   6% 13/226 [01:01<17:08,  4.83s/it]loss: 0.17224480211734772, grad_norm: 0.1733456552028656\n",
            "Training:   6% 14/226 [01:07<18:17,  5.18s/it]loss: 0.19203074276447296, grad_norm: 0.20876707136631012\n",
            "Training:   7% 15/226 [01:12<17:20,  4.93s/it]loss: 0.16800248622894287, grad_norm: 0.24483153223991394\n",
            "Training:   7% 16/226 [01:16<16:54,  4.83s/it]loss: 0.18725644052028656, grad_norm: 0.24408432841300964\n",
            "Training:   8% 17/226 [01:21<16:21,  4.70s/it]loss: 0.19729571044445038, grad_norm: 0.19051896035671234\n",
            "Training:   8% 18/226 [01:27<17:37,  5.09s/it]loss: 0.23452459275722504, grad_norm: 0.31181401014328003\n",
            "Training:   8% 19/226 [01:30<15:38,  4.53s/it]loss: 0.23956800997257233, grad_norm: 0.2607387900352478\n",
            "Training:   9% 20/226 [01:33<14:20,  4.18s/it]loss: 0.25175854563713074, grad_norm: 0.24147994816303253\n",
            "Training:   9% 21/226 [01:37<14:15,  4.18s/it]loss: 0.24979546666145325, grad_norm: 0.28980889916419983\n",
            "Training:  10% 22/226 [01:40<12:48,  3.77s/it]loss: 0.21255424618721008, grad_norm: 0.31471559405326843\n",
            "Training:  10% 23/226 [01:50<18:58,  5.61s/it]loss: 0.19325178861618042, grad_norm: 0.16710878908634186\n",
            "Training:  11% 24/226 [01:55<17:56,  5.33s/it]loss: 0.355398952960968, grad_norm: 0.4761091470718384\n",
            "Training:  11% 25/226 [01:57<14:29,  4.32s/it]loss: 0.1980714499950409, grad_norm: 0.20474962890148163\n",
            "Training:  12% 26/226 [02:03<16:27,  4.94s/it]loss: 0.24238832294940948, grad_norm: 0.2603108286857605\n",
            "Training:  12% 27/226 [02:06<14:18,  4.31s/it]loss: 0.19677619636058807, grad_norm: 0.20569296181201935\n",
            "Training:  12% 28/226 [02:12<16:01,  4.86s/it]loss: 0.147374227643013, grad_norm: 0.19784556329250336\n",
            "Training:  13% 29/226 [02:16<15:23,  4.69s/it]loss: 0.21170753240585327, grad_norm: 0.19883134961128235\n",
            "Training:  13% 30/226 [02:21<15:39,  4.79s/it]loss: 0.21518529951572418, grad_norm: 0.22367294132709503\n",
            "Training:  14% 31/226 [02:25<13:54,  4.28s/it]loss: 0.19734638929367065, grad_norm: 0.18681707978248596\n",
            "Training:  14% 32/226 [02:27<12:31,  3.87s/it]loss: 0.17232027649879456, grad_norm: 0.17176976799964905\n",
            "Training:  15% 33/226 [02:39<20:05,  6.24s/it]loss: 0.19799594581127167, grad_norm: 0.27226972579956055\n",
            "Training:  15% 34/226 [02:43<17:32,  5.48s/it]loss: 0.1965094953775406, grad_norm: 0.1605100929737091\n",
            "Training:  15% 35/226 [02:48<17:01,  5.35s/it]loss: 0.15505965054035187, grad_norm: 0.15386484563350677\n",
            "Training:  16% 36/226 [02:53<16:32,  5.22s/it]loss: 0.3045044541358948, grad_norm: 0.3281170427799225\n",
            "Training:  16% 37/226 [02:55<13:07,  4.17s/it]loss: 0.20952406525611877, grad_norm: 0.17946691811084747\n",
            "Training:  17% 38/226 [02:58<12:35,  4.02s/it]loss: 0.12914203107357025, grad_norm: 0.1295100897550583\n",
            "Training:  17% 39/226 [03:06<16:23,  5.26s/it]loss: 0.1888425499200821, grad_norm: 0.21633650362491608\n",
            "Training:  18% 40/226 [03:11<15:24,  4.97s/it]loss: 0.14610280096530914, grad_norm: 0.26435035467147827\n",
            "Training:  18% 41/226 [03:17<16:31,  5.36s/it]loss: 0.20866434276103973, grad_norm: 0.21343152225017548\n",
            "Training:  19% 42/226 [03:20<14:38,  4.78s/it]loss: 0.16056841611862183, grad_norm: 0.19033724069595337\n",
            "Training:  19% 43/226 [03:26<15:46,  5.17s/it]loss: 0.17315036058425903, grad_norm: 0.2189488261938095\n",
            "Training:  19% 44/226 [03:30<13:54,  4.59s/it]loss: 0.13281069695949554, grad_norm: 0.1340697556734085\n",
            "Training:  20% 45/226 [03:36<15:39,  5.19s/it]loss: 0.2293390929698944, grad_norm: 0.18273068964481354\n",
            "Training:  20% 46/226 [03:40<14:21,  4.79s/it]loss: 0.19279970228672028, grad_norm: 0.15040841698646545\n",
            "Training:  21% 47/226 [03:45<14:46,  4.95s/it]loss: 0.1669587939977646, grad_norm: 0.12386427074670792\n",
            "Training:  21% 48/226 [03:51<15:07,  5.10s/it]loss: 0.16910788416862488, grad_norm: 0.8286719918251038\n",
            "Training:  22% 49/226 [03:55<14:30,  4.92s/it]loss: 0.2343784123659134, grad_norm: 0.24983695149421692\n",
            "Training:  22% 50/226 [03:58<12:27,  4.24s/it]loss: 0.177682563662529, grad_norm: 0.18582499027252197\n",
            "Training:  23% 51/226 [04:04<13:36,  4.67s/it]loss: 0.2373843491077423, grad_norm: 0.2963572144508362\n",
            "Training:  23% 52/226 [04:06<11:44,  4.05s/it]loss: 0.2185211330652237, grad_norm: 0.29275399446487427\n",
            "Training:  23% 53/226 [04:12<12:44,  4.42s/it]loss: 0.1125492975115776, grad_norm: 0.10375028103590012\n",
            "Training:  24% 54/226 [04:22<17:33,  6.13s/it]loss: 0.17767532169818878, grad_norm: 0.15593628585338593\n",
            "Training:  24% 55/226 [04:29<18:09,  6.37s/it]loss: 0.1864645928144455, grad_norm: 0.22990505397319794\n",
            "Training:  25% 56/226 [04:34<16:51,  5.95s/it]loss: 0.17406421899795532, grad_norm: 0.18357761204242706\n",
            "Training:  25% 57/226 [04:39<16:25,  5.83s/it]loss: 0.1452140063047409, grad_norm: 0.13367855548858643\n",
            "Training:  26% 58/226 [04:49<19:23,  6.92s/it]loss: 0.16631504893302917, grad_norm: 0.1260555237531662\n",
            "Training:  26% 59/226 [04:58<20:51,  7.49s/it]loss: 0.19855056703090668, grad_norm: 0.19560521841049194\n",
            "Training:  27% 60/226 [05:02<18:18,  6.62s/it]loss: 0.12390940636396408, grad_norm: 0.1399453580379486\n",
            "Training:  27% 61/226 [05:07<16:53,  6.14s/it]loss: 0.175270676612854, grad_norm: 0.1564067006111145\n",
            "Training:  27% 62/226 [05:13<16:33,  6.06s/it]loss: 0.17357216775417328, grad_norm: 0.13248558342456818\n",
            "Training:  28% 63/226 [05:25<21:40,  7.98s/it]loss: 0.19558940827846527, grad_norm: 0.21556252241134644\n",
            "Training:  28% 64/226 [05:28<17:20,  6.42s/it]loss: 0.18234135210514069, grad_norm: 0.2622712254524231\n",
            "Training:  29% 65/226 [05:32<15:17,  5.70s/it]loss: 0.18082593381404877, grad_norm: 0.1241345927119255\n",
            "Training:  29% 66/226 [05:38<15:04,  5.66s/it]loss: 0.20312510430812836, grad_norm: 0.25035861134529114\n",
            "Training:  30% 67/226 [05:43<14:24,  5.44s/it]loss: 0.22796601057052612, grad_norm: 0.21535919606685638\n",
            "Training:  30% 68/226 [05:46<12:50,  4.88s/it]loss: 0.23968593776226044, grad_norm: 0.22972702980041504\n",
            "Training:  31% 69/226 [05:49<11:03,  4.22s/it]loss: 0.1736755669116974, grad_norm: 0.14716754853725433\n",
            "Training:  31% 70/226 [05:53<11:02,  4.25s/it]loss: 0.20274023711681366, grad_norm: 0.1280849725008011\n",
            "Training:  31% 71/226 [05:58<11:39,  4.52s/it]loss: 0.25074246525764465, grad_norm: 0.2244431972503662\n",
            "Training:  32% 72/226 [06:01<10:26,  4.07s/it]loss: 0.16159528493881226, grad_norm: 0.13931986689567566\n",
            "Training:  32% 73/226 [06:13<15:51,  6.22s/it]loss: 0.16812093555927277, grad_norm: 0.166653111577034\n",
            "Training:  33% 74/226 [06:17<14:33,  5.75s/it]loss: 0.14515192806720734, grad_norm: 0.1811201423406601\n",
            "Training:  33% 75/226 [06:22<13:37,  5.41s/it]loss: 0.22816886007785797, grad_norm: 0.2203415483236313\n",
            "Training:  34% 76/226 [06:25<11:29,  4.60s/it]loss: 0.14004845917224884, grad_norm: 0.11737651377916336\n",
            "Training:  34% 77/226 [06:31<12:59,  5.23s/it]loss: 0.2573530972003937, grad_norm: 0.2123699188232422\n",
            "Training:  35% 78/226 [06:35<11:35,  4.70s/it]loss: 0.21003255248069763, grad_norm: 0.1854429394006729\n",
            "Training:  35% 79/226 [06:38<10:27,  4.27s/it]loss: 0.16744844615459442, grad_norm: 0.14447878301143646\n",
            "Training:  35% 80/226 [06:43<10:56,  4.50s/it]loss: 0.23766891658306122, grad_norm: 0.25405314564704895\n",
            "Training:  36% 81/226 [06:46<09:51,  4.08s/it]loss: 0.33239609003067017, grad_norm: 0.29318660497665405\n",
            "Training:  36% 82/226 [06:48<08:03,  3.35s/it]loss: 0.2182379513978958, grad_norm: 0.15841612219810486\n",
            "Training:  37% 83/226 [06:55<10:30,  4.41s/it]loss: 0.13593120872974396, grad_norm: 0.11692092567682266\n",
            "Training:  37% 84/226 [07:02<12:25,  5.25s/it]loss: 0.13011915981769562, grad_norm: 0.15052033960819244\n",
            "Training:  38% 85/226 [07:10<13:57,  5.94s/it]loss: 0.2016688883304596, grad_norm: 0.15988510847091675\n",
            "Training:  38% 86/226 [07:13<11:51,  5.08s/it]loss: 0.1850694864988327, grad_norm: 0.15764769911766052\n",
            "Training:  38% 87/226 [07:17<11:21,  4.90s/it]loss: 0.2571156919002533, grad_norm: 0.2654865086078644\n",
            "Training:  39% 88/226 [07:20<09:41,  4.21s/it]loss: 0.268037885427475, grad_norm: 0.3042948842048645\n",
            "Training:  39% 89/226 [07:22<08:17,  3.63s/it]loss: 0.1478256732225418, grad_norm: 0.12053024023771286\n",
            "Training:  40% 90/226 [07:29<10:13,  4.51s/it]loss: 0.2551192343235016, grad_norm: 0.18549630045890808\n",
            "Training:  40% 91/226 [07:32<09:27,  4.20s/it]loss: 0.22411198914051056, grad_norm: 0.17500178515911102\n",
            "Training:  41% 92/226 [07:35<08:35,  3.85s/it]loss: 0.2161528617143631, grad_norm: 0.24757550656795502\n",
            "Training:  41% 93/226 [07:45<12:28,  5.62s/it]loss: 0.13320164382457733, grad_norm: 0.1538650095462799\n",
            "Training:  42% 94/226 [07:53<14:18,  6.51s/it]loss: 0.22181259095668793, grad_norm: 0.5527533888816833\n",
            "Training:  42% 95/226 [07:56<11:45,  5.38s/it]loss: 0.2277052253484726, grad_norm: 0.19334527850151062\n",
            "Training:  42% 96/226 [07:59<09:53,  4.56s/it]loss: 0.22811269760131836, grad_norm: 0.20265431702136993\n",
            "Training:  43% 97/226 [08:01<08:34,  3.99s/it]loss: 0.21036823093891144, grad_norm: 0.22682860493659973\n",
            "Training:  43% 98/226 [08:04<07:25,  3.48s/it]loss: 0.20942212641239166, grad_norm: 2.277940511703491\n",
            "Training:  44% 99/226 [08:08<07:53,  3.73s/it]loss: 0.12921826541423798, grad_norm: 0.4867410659790039\n",
            "Training:  44% 100/226 [08:16<10:14,  4.88s/it]loss: 0.17018578946590424, grad_norm: 0.17267243564128876\n",
            "Training:  45% 101/226 [08:21<10:39,  5.12s/it]loss: 0.23350389301776886, grad_norm: 0.27963507175445557\n",
            "Training:  45% 102/226 [08:25<09:34,  4.63s/it]loss: 0.15547531843185425, grad_norm: 0.15379391610622406\n",
            "Training:  46% 103/226 [08:36<13:35,  6.63s/it]loss: 0.15733297169208527, grad_norm: 0.16634012758731842\n",
            "Training:  46% 104/226 [08:42<13:13,  6.50s/it]loss: 0.2781033515930176, grad_norm: 0.32243672013282776\n",
            "Training:  46% 105/226 [08:45<11:00,  5.46s/it]loss: 0.20633438229560852, grad_norm: 0.20417188107967377\n",
            "Training:  47% 106/226 [08:50<10:39,  5.33s/it]loss: 0.13026542961597443, grad_norm: 0.1161370575428009\n",
            "Training:  47% 107/226 [08:58<11:45,  5.93s/it]loss: 0.2147415578365326, grad_norm: 0.5050982236862183\n",
            "Training:  48% 108/226 [09:00<09:43,  4.94s/it]loss: 0.19413208961486816, grad_norm: 0.19900844991207123\n",
            "Training:  48% 109/226 [09:04<09:03,  4.65s/it]loss: 0.27132412791252136, grad_norm: 0.30695146322250366\n",
            "Training:  49% 110/226 [09:07<07:36,  3.94s/it]loss: 0.22779184579849243, grad_norm: 0.2306070774793625\n",
            "Training:  49% 111/226 [09:10<07:08,  3.73s/it]loss: 0.1866929531097412, grad_norm: 0.7037283182144165\n",
            "Training:  50% 112/226 [09:14<07:04,  3.73s/it]loss: 0.20141036808490753, grad_norm: 0.19139933586120605\n",
            "Training:  50% 113/226 [09:23<10:26,  5.54s/it]loss: 0.16589826345443726, grad_norm: 0.17624253034591675\n",
            "Training:  50% 114/226 [09:29<10:31,  5.64s/it]loss: 0.22979743778705597, grad_norm: 1.0399397611618042\n",
            "Training:  51% 115/226 [09:33<09:41,  5.24s/it]loss: 0.19736480712890625, grad_norm: 0.22897370159626007\n",
            "Training:  51% 116/226 [09:37<08:52,  4.84s/it]loss: 0.12469220906496048, grad_norm: 0.13562840223312378\n",
            "Training:  52% 117/226 [09:44<09:51,  5.42s/it]loss: 0.21311335265636444, grad_norm: 0.22445912659168243\n",
            "Training:  52% 118/226 [09:49<09:16,  5.15s/it]loss: 0.1660497784614563, grad_norm: 0.19279341399669647\n",
            "Training:  53% 119/226 [09:54<09:05,  5.10s/it]loss: 0.1971178501844406, grad_norm: 0.20035801827907562\n",
            "Training:  53% 120/226 [09:57<08:18,  4.71s/it]loss: 0.22194330394268036, grad_norm: 0.9173229932785034\n",
            "Training:  54% 121/226 [10:00<07:20,  4.20s/it]loss: 0.19670133292675018, grad_norm: 0.2146206796169281\n",
            "Training:  54% 122/226 [10:04<06:48,  3.93s/it]loss: 0.20752350986003876, grad_norm: 0.26982536911964417\n",
            "Training:  54% 123/226 [10:17<11:26,  6.66s/it]loss: 0.17407189309597015, grad_norm: 0.22487643361091614\n",
            "Training:  55% 124/226 [10:22<10:49,  6.36s/it]loss: 0.19982226192951202, grad_norm: 0.21462686359882355\n",
            "Training:  55% 125/226 [10:28<10:09,  6.03s/it]loss: 0.23478107154369354, grad_norm: 0.2811122536659241\n",
            "Training:  56% 126/226 [10:30<08:14,  4.94s/it]loss: 0.21839077770709991, grad_norm: 0.22286280989646912\n",
            "Training:  56% 127/226 [10:35<08:06,  4.91s/it]loss: 0.19255737960338593, grad_norm: 0.7817643880844116\n",
            "Training:  57% 128/226 [10:39<07:36,  4.66s/it]loss: 0.19547945261001587, grad_norm: 0.2878511846065521\n",
            "Training:  57% 129/226 [10:44<07:43,  4.77s/it]loss: 0.16491766273975372, grad_norm: 0.15469466149806976\n",
            "Training:  58% 130/226 [10:51<08:29,  5.30s/it]loss: 0.2452130913734436, grad_norm: 0.3603813052177429\n",
            "Training:  58% 131/226 [10:53<07:13,  4.56s/it]loss: 0.22116047143936157, grad_norm: 0.22472043335437775\n",
            "Training:  58% 132/226 [10:58<06:57,  4.44s/it]loss: 0.17469727993011475, grad_norm: 0.1923062950372696\n",
            "Training:  59% 133/226 [11:10<10:34,  6.83s/it]loss: 0.21302783489227295, grad_norm: 0.23599010705947876\n",
            "Training:  59% 134/226 [11:13<08:34,  5.60s/it]loss: 0.17876413464546204, grad_norm: 0.18817022442817688\n",
            "Training:  60% 135/226 [11:17<07:54,  5.21s/it]loss: 0.26252034306526184, grad_norm: 0.6421107053756714\n",
            "Training:  60% 136/226 [11:20<06:48,  4.53s/it]loss: 0.19746018946170807, grad_norm: 0.1838090419769287\n",
            "Training:  61% 137/226 [11:24<06:37,  4.47s/it]loss: 0.15630464255809784, grad_norm: 0.14036166667938232\n",
            "Training:  61% 138/226 [11:34<08:38,  5.90s/it]loss: 0.1639527976512909, grad_norm: 0.17630314826965332\n",
            "Training:  62% 139/226 [11:38<07:53,  5.44s/it]loss: 0.17692996561527252, grad_norm: 0.26200687885284424\n",
            "Training:  62% 140/226 [11:47<09:12,  6.42s/it]loss: 0.19762514531612396, grad_norm: 0.20171432197093964\n",
            "Training:  62% 141/226 [11:51<08:02,  5.68s/it]loss: 0.19334931671619415, grad_norm: 0.17160265147686005\n",
            "Training:  63% 142/226 [11:54<07:06,  5.07s/it]loss: 0.23193112015724182, grad_norm: 0.21051067113876343\n",
            "Training:  63% 143/226 [12:02<07:57,  5.75s/it]loss: 0.1312844604253769, grad_norm: 0.3312815725803375\n",
            "Training:  64% 144/226 [12:09<08:38,  6.33s/it]loss: 0.17915841937065125, grad_norm: 0.1346091628074646\n",
            "Training:  64% 145/226 [12:19<09:45,  7.22s/it]loss: 0.16127073764801025, grad_norm: 0.24087093770503998\n",
            "Training:  65% 146/226 [12:23<08:30,  6.38s/it]loss: 0.15178053081035614, grad_norm: 0.166090726852417\n",
            "Training:  65% 147/226 [12:30<08:41,  6.60s/it]loss: 0.2740336060523987, grad_norm: 0.2833690941333771\n",
            "Training:  65% 148/226 [12:33<07:07,  5.48s/it]loss: 0.18580059707164764, grad_norm: 0.18657660484313965\n",
            "Training:  66% 149/226 [12:38<06:42,  5.22s/it]loss: 0.23582161962985992, grad_norm: 0.16661962866783142\n",
            "Training:  66% 150/226 [12:42<06:17,  4.97s/it]loss: 0.163400799036026, grad_norm: 0.17085310816764832\n",
            "Training:  67% 151/226 [12:48<06:45,  5.41s/it]loss: 0.1674688160419464, grad_norm: 0.2774238884449005\n",
            "Training:  67% 152/226 [12:56<07:21,  5.96s/it]loss: 0.2854755222797394, grad_norm: 0.38116469979286194\n",
            "Training:  68% 153/226 [13:05<08:21,  6.87s/it]loss: 0.30194488167762756, grad_norm: 0.46612903475761414\n",
            "Training:  68% 154/226 [13:06<06:21,  5.30s/it]loss: 0.16405577957630157, grad_norm: 0.14891600608825684\n",
            "Training:  69% 155/226 [13:12<06:19,  5.34s/it]loss: 0.15826579928398132, grad_norm: 0.15796370804309845\n",
            "Training:  69% 156/226 [13:16<06:02,  5.18s/it]loss: 0.1803676038980484, grad_norm: 2.486424684524536\n",
            "Training:  69% 157/226 [13:22<06:06,  5.31s/it]loss: 0.21472948789596558, grad_norm: 0.2765933871269226\n",
            "Training:  70% 158/226 [13:25<05:13,  4.61s/it]loss: 0.19580018520355225, grad_norm: 0.21348018944263458\n",
            "Training:  70% 159/226 [13:28<04:42,  4.21s/it]loss: 0.2780857980251312, grad_norm: 0.5524923205375671\n",
            "Training:  71% 160/226 [13:32<04:18,  3.92s/it]loss: 0.1984185129404068, grad_norm: 0.19216004014015198\n",
            "Training:  71% 161/226 [13:35<04:07,  3.80s/it]loss: 0.18919764459133148, grad_norm: 0.18926101922988892\n",
            "Training:  72% 162/226 [13:39<04:13,  3.96s/it]loss: 0.2068503051996231, grad_norm: 0.16661937534809113\n",
            "Training:  72% 163/226 [13:46<04:57,  4.72s/it]loss: 0.3021751642227173, grad_norm: 0.312315970659256\n",
            "Training:  73% 164/226 [13:48<04:10,  4.04s/it]loss: 0.11170650273561478, grad_norm: 0.15606151521205902\n",
            "Training:  73% 165/226 [13:58<05:44,  5.65s/it]loss: 0.2660830020904541, grad_norm: 0.3284352123737335\n",
            "Training:  73% 166/226 [14:00<04:33,  4.56s/it]loss: 0.20907364785671234, grad_norm: 0.2223387360572815\n",
            "Training:  74% 167/226 [14:03<03:58,  4.04s/it]loss: 0.21387650072574615, grad_norm: 0.2427399605512619\n",
            "Training:  74% 168/226 [14:07<04:05,  4.24s/it]loss: 0.16064776480197906, grad_norm: 0.146791473031044\n",
            "Training:  75% 169/226 [14:14<04:40,  4.92s/it]loss: 0.2717073857784271, grad_norm: 0.3057185709476471\n",
            "Training:  75% 170/226 [14:16<03:51,  4.14s/it]loss: 0.18614864349365234, grad_norm: 0.37256789207458496\n",
            "Training:  76% 171/226 [14:20<03:41,  4.03s/it]loss: 0.26019179821014404, grad_norm: 0.30562710762023926\n",
            "Training:  76% 172/226 [14:23<03:16,  3.63s/it]loss: 0.20494356751441956, grad_norm: 0.18474675714969635\n",
            "Training:  77% 173/226 [14:29<03:47,  4.30s/it]loss: 0.17977440357208252, grad_norm: 0.17651541531085968\n",
            "Training:  77% 174/226 [14:33<03:41,  4.25s/it]loss: 0.23834483325481415, grad_norm: 0.2261258065700531\n",
            "Training:  77% 175/226 [14:36<03:28,  4.09s/it]loss: 0.14315909147262573, grad_norm: 0.14938116073608398\n",
            "Training:  78% 176/226 [14:44<04:19,  5.18s/it]loss: 0.19257676601409912, grad_norm: 0.17592157423496246\n",
            "Training:  78% 177/226 [14:50<04:29,  5.50s/it]loss: 0.19835373759269714, grad_norm: 0.19241659343242645\n",
            "Training:  79% 178/226 [14:55<04:05,  5.12s/it]loss: 0.1412104070186615, grad_norm: 0.1738976389169693\n",
            "Training:  79% 179/226 [15:02<04:31,  5.78s/it]loss: 0.17607814073562622, grad_norm: 0.1639874428510666\n",
            "Training:  80% 180/226 [15:05<03:53,  5.07s/it]loss: 0.12224996089935303, grad_norm: 0.18721747398376465\n",
            "Training:  80% 181/226 [15:14<04:38,  6.18s/it]loss: 0.1903313547372818, grad_norm: 0.36555221676826477\n",
            "Training:  81% 182/226 [15:17<03:50,  5.24s/it]loss: 0.15569528937339783, grad_norm: 0.1350683569908142\n",
            "Training:  81% 183/226 [15:25<04:18,  6.00s/it]loss: 0.2018924206495285, grad_norm: 0.1482701301574707\n",
            "Training:  81% 184/226 [15:28<03:34,  5.10s/it]loss: 0.16812092065811157, grad_norm: 0.16001223027706146\n",
            "Training:  82% 185/226 [15:32<03:22,  4.93s/it]loss: 0.2040637582540512, grad_norm: 0.16779735684394836\n",
            "Training:  82% 186/226 [15:37<03:17,  4.95s/it]loss: 0.2252640724182129, grad_norm: 0.1770016998052597\n",
            "Training:  83% 187/226 [15:42<03:05,  4.75s/it]loss: 0.23231546580791473, grad_norm: 0.2044512778520584\n",
            "Training:  83% 188/226 [15:46<02:57,  4.68s/it]loss: 0.19620142877101898, grad_norm: 0.1963844895362854\n",
            "Training:  84% 189/226 [15:50<02:47,  4.52s/it]loss: 0.1891999989748001, grad_norm: 0.22148682177066803\n",
            "Training:  84% 190/226 [15:54<02:28,  4.13s/it]loss: 0.21786759793758392, grad_norm: 0.17987707257270813\n",
            "Training:  85% 191/226 [15:57<02:13,  3.82s/it]loss: 0.15061114728450775, grad_norm: 0.30313462018966675\n",
            "Training:  85% 192/226 [16:02<02:27,  4.34s/it]loss: 0.24673128128051758, grad_norm: 0.28877899050712585\n",
            "Training:  85% 193/226 [16:07<02:24,  4.38s/it]loss: 0.16849978268146515, grad_norm: 0.1164187490940094\n",
            "Training:  86% 194/226 [16:12<02:25,  4.53s/it]loss: 0.16565825045108795, grad_norm: 0.2286350578069687\n",
            "Training:  86% 195/226 [16:16<02:19,  4.51s/it]loss: 0.15659773349761963, grad_norm: 0.12706488370895386\n",
            "Training:  87% 196/226 [16:23<02:39,  5.32s/it]loss: 0.1624947339296341, grad_norm: 0.13078691065311432\n",
            "Training:  87% 197/226 [16:28<02:31,  5.23s/it]loss: 0.12305714190006256, grad_norm: 0.12744787335395813\n",
            "Training:  88% 198/226 [16:34<02:26,  5.25s/it]loss: 0.2236100137233734, grad_norm: 0.30593547224998474\n",
            "Training:  88% 199/226 [16:37<02:07,  4.72s/it]loss: 0.2535998821258545, grad_norm: 0.2743697166442871\n",
            "Training:  88% 200/226 [16:40<01:47,  4.14s/it]loss: 0.1393709033727646, grad_norm: 0.131039097905159\n",
            "Training:  89% 201/226 [16:46<01:55,  4.63s/it]loss: 0.19957534968852997, grad_norm: 0.162257120013237\n",
            "Training:  89% 202/226 [16:50<01:49,  4.58s/it]loss: 0.1562008410692215, grad_norm: 0.15664885938167572\n",
            "Training:  90% 203/226 [16:57<02:01,  5.28s/it]loss: 0.2623085379600525, grad_norm: 0.22390961647033691\n",
            "Training:  90% 204/226 [17:00<01:38,  4.47s/it]loss: 0.1713307499885559, grad_norm: 0.14983688294887543\n",
            "Training:  91% 205/226 [17:03<01:29,  4.27s/it]loss: 0.22392071783542633, grad_norm: 0.7448490262031555\n",
            "Training:  91% 206/226 [17:07<01:19,  3.96s/it]loss: 0.2022913545370102, grad_norm: 0.2194676399230957\n",
            "Training:  92% 207/226 [17:10<01:09,  3.65s/it]loss: 0.20993562042713165, grad_norm: 0.2044871598482132\n",
            "Training:  92% 208/226 [17:14<01:09,  3.89s/it]loss: 0.18753498792648315, grad_norm: 0.2522098422050476\n",
            "Training:  92% 209/226 [17:20<01:16,  4.52s/it]loss: 0.20656023919582367, grad_norm: 0.16211502254009247\n",
            "Training:  93% 210/226 [17:25<01:13,  4.60s/it]loss: 0.21384461224079132, grad_norm: 0.22201190888881683\n",
            "Training:  93% 211/226 [17:28<01:03,  4.24s/it]loss: 0.1317349076271057, grad_norm: 0.1374340057373047\n",
            "Training:  94% 212/226 [17:34<01:05,  4.71s/it]loss: 0.17301753163337708, grad_norm: 0.3834353983402252\n",
            "Training:  94% 213/226 [17:45<01:27,  6.73s/it]loss: 0.21878036856651306, grad_norm: 0.21177345514297485\n",
            "Training:  95% 214/226 [17:50<01:12,  6.06s/it]loss: 0.21695327758789062, grad_norm: 0.20922566950321198\n",
            "Training:  95% 215/226 [17:53<00:56,  5.12s/it]loss: 0.2274492383003235, grad_norm: 0.28995203971862793\n",
            "Training:  96% 216/226 [17:56<00:45,  4.53s/it]loss: 0.20350879430770874, grad_norm: 0.1698351800441742\n",
            "Training:  96% 217/226 [18:00<00:39,  4.39s/it]loss: 0.23598623275756836, grad_norm: 0.19656893610954285\n",
            "Training:  96% 218/226 [18:03<00:31,  3.93s/it]loss: 0.20247004926204681, grad_norm: 0.16965176165103912\n",
            "Training:  97% 219/226 [18:08<00:30,  4.29s/it]loss: 0.18195347487926483, grad_norm: 0.21005573868751526\n",
            "Training:  97% 220/226 [18:14<00:27,  4.65s/it]loss: 0.1771611124277115, grad_norm: 0.13038837909698486\n",
            "Training:  98% 221/226 [18:19<00:24,  4.88s/it]loss: 0.1550397425889969, grad_norm: 0.13422898948192596\n",
            "Training:  98% 222/226 [18:23<00:18,  4.60s/it]loss: 0.26537182927131653, grad_norm: 0.4729086756706238\n",
            "Training:  99% 223/226 [18:29<00:14,  4.91s/it]loss: 0.1570705622434616, grad_norm: 0.11574694514274597\n",
            "Training:  99% 224/226 [18:34<00:10,  5.00s/it]loss: 0.1750045269727707, grad_norm: 0.1412619650363922\n",
            "Training: 100% 225/226 [18:38<00:04,  4.77s/it]loss: 0.20029307901859283, grad_norm: 0.3052392899990082\n",
            "Training: 100% 226/226 [18:42<00:00,  4.97s/it]\n",
            "Validation loss: 0.7312560296058654\n",
            "Epoch: 9\n",
            "Training:   0% 0/226 [00:00<?, ?it/s]loss: 0.2239883840084076, grad_norm: 0.20033104717731476\n",
            "Training:   0% 1/226 [00:03<12:42,  3.39s/it]loss: 0.17181085050106049, grad_norm: 0.16375352442264557\n",
            "Training:   1% 2/226 [00:07<14:46,  3.96s/it]loss: 0.17933911085128784, grad_norm: 0.16091884672641754\n",
            "Training:   1% 3/226 [00:11<15:04,  4.06s/it]loss: 0.20766015350818634, grad_norm: 0.18710336089134216\n",
            "Training:   2% 4/226 [00:15<14:06,  3.81s/it]loss: 0.1958175152540207, grad_norm: 0.1677107810974121\n",
            "Training:   2% 5/226 [00:20<15:11,  4.12s/it]loss: 0.2209566831588745, grad_norm: 0.2010616809129715\n",
            "Training:   3% 6/226 [00:23<14:39,  4.00s/it]loss: 0.17290502786636353, grad_norm: 0.179551899433136\n",
            "Training:   3% 7/226 [00:28<15:12,  4.17s/it]loss: 0.20395098626613617, grad_norm: 0.18561860918998718\n",
            "Training:   4% 8/226 [00:31<14:15,  3.93s/it]loss: 0.11870840936899185, grad_norm: 0.09450608491897583\n",
            "Training:   4% 9/226 [00:39<18:37,  5.15s/it]loss: 0.255826860666275, grad_norm: 0.22734872996807098\n",
            "Training:   4% 10/226 [00:42<16:06,  4.47s/it]loss: 0.22574084997177124, grad_norm: 0.18381080031394958\n",
            "Training:   5% 11/226 [00:46<15:08,  4.23s/it]loss: 0.1380518674850464, grad_norm: 0.12363407015800476\n",
            "Training:   5% 12/226 [00:51<16:12,  4.54s/it]loss: 0.16294419765472412, grad_norm: 0.14419128000736237\n",
            "Training:   6% 13/226 [00:55<15:51,  4.47s/it]loss: 0.2049110382795334, grad_norm: 0.16630378365516663\n",
            "Training:   6% 14/226 [01:00<15:47,  4.47s/it]loss: 0.1731274574995041, grad_norm: 0.14165611565113068\n",
            "Training:   7% 15/226 [01:05<16:14,  4.62s/it]loss: 0.20488177239894867, grad_norm: 0.18985827267169952\n",
            "Training:   7% 16/226 [01:08<14:50,  4.24s/it]loss: 0.1872016340494156, grad_norm: 0.19266082346439362\n",
            "Training:   8% 17/226 [01:18<21:00,  6.03s/it]loss: 0.23819343745708466, grad_norm: 0.19650998711585999\n",
            "Training:   8% 18/226 [01:21<17:12,  4.96s/it]loss: 0.24663116037845612, grad_norm: 0.22646285593509674\n",
            "Training:   8% 19/226 [01:24<14:56,  4.33s/it]loss: 0.24163870513439178, grad_norm: 0.1658421903848648\n",
            "Training:   9% 20/226 [01:27<13:28,  3.93s/it]loss: 0.19177381694316864, grad_norm: 0.12732774019241333\n",
            "Training:   9% 21/226 [01:33<16:17,  4.77s/it]loss: 0.15868303179740906, grad_norm: 0.11614714562892914\n",
            "Training:  10% 22/226 [01:39<17:03,  5.02s/it]loss: 0.22741717100143433, grad_norm: 0.3172338008880615\n",
            "Training:  10% 23/226 [01:42<14:54,  4.41s/it]loss: 0.22384577989578247, grad_norm: 0.20592133700847626\n",
            "Training:  11% 24/226 [01:45<13:06,  3.89s/it]loss: 0.21421882510185242, grad_norm: 0.1686435043811798\n",
            "Training:  11% 25/226 [01:48<12:23,  3.70s/it]loss: 0.22955764830112457, grad_norm: 0.1773356795310974\n",
            "Training:  12% 26/226 [01:51<12:10,  3.65s/it]loss: 0.13748343288898468, grad_norm: 0.14483414590358734\n",
            "Training:  12% 27/226 [01:57<14:11,  4.28s/it]loss: 0.18745574355125427, grad_norm: 0.2437828779220581\n",
            "Training:  12% 28/226 [02:00<13:07,  3.98s/it]loss: 0.14470717310905457, grad_norm: 0.11295808851718903\n",
            "Training:  13% 29/226 [02:08<16:28,  5.02s/it]loss: 0.162779301404953, grad_norm: 0.1263500303030014\n",
            "Training:  13% 30/226 [02:12<15:55,  4.88s/it]loss: 0.18760791420936584, grad_norm: 0.1431429535150528\n",
            "Training:  14% 31/226 [02:17<15:34,  4.79s/it]loss: 0.15035562217235565, grad_norm: 0.11205526441335678\n",
            "Training:  14% 32/226 [02:23<16:46,  5.19s/it]loss: 0.2390478551387787, grad_norm: 0.21157750487327576\n",
            "Training:  15% 33/226 [02:26<14:19,  4.45s/it]loss: 0.16185539960861206, grad_norm: 0.16596926748752594\n",
            "Training:  15% 34/226 [02:31<15:08,  4.73s/it]loss: 0.213634192943573, grad_norm: 0.13217204809188843\n",
            "Training:  15% 35/226 [02:36<15:03,  4.73s/it]loss: 0.13702766597270966, grad_norm: 0.15658332407474518\n",
            "Training:  16% 36/226 [02:42<16:35,  5.24s/it]loss: 0.2525072395801544, grad_norm: 0.17254768311977386\n",
            "Training:  16% 37/226 [02:53<21:09,  6.72s/it]loss: 0.11356263607740402, grad_norm: 0.10396672040224075\n",
            "Training:  17% 38/226 [03:02<24:06,  7.70s/it]loss: 0.2050991952419281, grad_norm: 0.15424224734306335\n",
            "Training:  17% 39/226 [03:07<21:16,  6.83s/it]loss: 0.17549312114715576, grad_norm: 0.09843552857637405\n",
            "Training:  18% 40/226 [03:13<20:25,  6.59s/it]loss: 0.20143522322177887, grad_norm: 0.14545704424381256\n",
            "Training:  18% 41/226 [03:16<17:07,  5.55s/it]loss: 0.2121296226978302, grad_norm: 0.17008697986602783\n",
            "Training:  19% 42/226 [03:20<15:38,  5.10s/it]loss: 0.1589621603488922, grad_norm: 0.13601873815059662\n",
            "Training:  19% 43/226 [03:25<14:50,  4.87s/it]loss: 0.19241437315940857, grad_norm: 0.23055382072925568\n",
            "Training:  19% 44/226 [03:27<12:46,  4.21s/it]loss: 0.18718238174915314, grad_norm: 0.26491332054138184\n",
            "Training:  20% 45/226 [03:31<12:09,  4.03s/it]loss: 0.14030399918556213, grad_norm: 0.09928373992443085\n",
            "Training:  20% 46/226 [03:36<12:36,  4.20s/it]loss: 0.19904369115829468, grad_norm: 0.15740294754505157\n",
            "Training:  21% 47/226 [03:46<17:48,  5.97s/it]loss: 0.1904347836971283, grad_norm: 0.14344951510429382\n",
            "Training:  21% 48/226 [03:51<16:51,  5.69s/it]loss: 0.19280362129211426, grad_norm: 0.13512061536312103\n",
            "Training:  22% 49/226 [03:55<15:46,  5.34s/it]loss: 0.13369417190551758, grad_norm: 0.11500948667526245\n",
            "Training:  22% 50/226 [04:02<16:38,  5.67s/it]loss: 0.2022823542356491, grad_norm: 0.16138224303722382\n",
            "Training:  23% 51/226 [04:06<15:01,  5.15s/it]loss: 0.11767174303531647, grad_norm: 0.21225261688232422\n",
            "Training:  23% 52/226 [04:12<16:04,  5.54s/it]loss: 0.17658241093158722, grad_norm: 0.1389414370059967\n",
            "Training:  23% 53/226 [04:17<15:39,  5.43s/it]loss: 0.17426319420337677, grad_norm: 0.12433289736509323\n",
            "Training:  24% 54/226 [04:22<14:33,  5.08s/it]loss: 0.13766568899154663, grad_norm: 0.12787452340126038\n",
            "Training:  24% 55/226 [04:27<14:23,  5.05s/it]loss: 0.23282210528850555, grad_norm: 0.24129672348499298\n",
            "Training:  25% 56/226 [04:29<12:02,  4.25s/it]loss: 0.1826549619436264, grad_norm: 0.1276540905237198\n",
            "Training:  25% 57/226 [04:41<18:40,  6.63s/it]loss: 0.19910357892513275, grad_norm: 0.16474378108978271\n",
            "Training:  26% 58/226 [04:45<15:55,  5.69s/it]loss: 0.1740344762802124, grad_norm: 0.12416046857833862\n",
            "Training:  26% 59/226 [04:49<14:37,  5.26s/it]loss: 0.12783360481262207, grad_norm: 0.10302387923002243\n",
            "Training:  27% 60/226 [04:56<16:12,  5.86s/it]loss: 0.16565649211406708, grad_norm: 0.11762034147977829\n",
            "Training:  27% 61/226 [05:02<15:41,  5.71s/it]loss: 0.1709521859884262, grad_norm: 0.1732298731803894\n",
            "Training:  27% 62/226 [05:07<15:11,  5.56s/it]loss: 0.18636000156402588, grad_norm: 0.15055742859840393\n",
            "Training:  28% 63/226 [05:12<14:30,  5.34s/it]loss: 0.26472941040992737, grad_norm: 0.2482765018939972\n",
            "Training:  28% 64/226 [05:14<11:42,  4.34s/it]loss: 0.13981987535953522, grad_norm: 0.11878656595945358\n",
            "Training:  29% 65/226 [05:19<12:44,  4.75s/it]loss: 0.2808779776096344, grad_norm: 0.21136240661144257\n",
            "Training:  29% 66/226 [05:21<10:36,  3.98s/it]loss: 0.18891315162181854, grad_norm: 0.1618739664554596\n",
            "Training:  30% 67/226 [05:30<14:17,  5.39s/it]loss: 0.18018309772014618, grad_norm: 0.13914218544960022\n",
            "Training:  30% 68/226 [05:34<13:02,  4.95s/it]loss: 0.21401561796665192, grad_norm: 0.2186438888311386\n",
            "Training:  31% 69/226 [05:37<11:01,  4.21s/it]loss: 0.20778997242450714, grad_norm: 0.16622139513492584\n",
            "Training:  31% 70/226 [05:41<10:45,  4.14s/it]loss: 0.19595690071582794, grad_norm: 0.1979677379131317\n",
            "Training:  31% 71/226 [05:44<10:18,  3.99s/it]loss: 0.1927785575389862, grad_norm: 0.1748988926410675\n",
            "Training:  32% 72/226 [05:48<10:10,  3.97s/it]loss: 0.13812921941280365, grad_norm: 0.11249398440122604\n",
            "Training:  32% 73/226 [05:52<10:22,  4.07s/it]loss: 0.266783744096756, grad_norm: 0.22491176426410675\n",
            "Training:  33% 74/226 [05:55<09:24,  3.71s/it]loss: 0.19832858443260193, grad_norm: 0.1651720255613327\n",
            "Training:  33% 75/226 [05:59<09:15,  3.68s/it]loss: 0.17276522517204285, grad_norm: 0.14693494141101837\n",
            "Training:  34% 76/226 [06:02<09:08,  3.66s/it]loss: 0.1724032163619995, grad_norm: 0.12786155939102173\n",
            "Training:  34% 77/226 [06:10<11:47,  4.75s/it]loss: 0.1690831184387207, grad_norm: 0.13040973246097565\n",
            "Training:  35% 78/226 [06:14<11:25,  4.63s/it]loss: 0.2106541097164154, grad_norm: 0.14945733547210693\n",
            "Training:  35% 79/226 [06:17<10:23,  4.24s/it]loss: 0.22353698313236237, grad_norm: 0.17328326404094696\n",
            "Training:  35% 80/226 [06:22<10:49,  4.45s/it]loss: 0.17931881546974182, grad_norm: 0.11822044849395752\n",
            "Training:  36% 81/226 [06:27<11:03,  4.58s/it]loss: 0.23849321901798248, grad_norm: 0.227015882730484\n",
            "Training:  36% 82/226 [06:30<09:23,  3.91s/it]loss: 0.11506042629480362, grad_norm: 0.09553615748882294\n",
            "Training:  37% 83/226 [06:37<12:02,  5.05s/it]loss: 0.19343198835849762, grad_norm: 0.193757563829422\n",
            "Training:  37% 84/226 [06:42<11:38,  4.92s/it]loss: 0.1719529628753662, grad_norm: 0.16325408220291138\n",
            "Training:  38% 85/226 [06:47<11:44,  5.00s/it]loss: 0.22139324247837067, grad_norm: 0.19185982644557953\n",
            "Training:  38% 86/226 [06:51<11:09,  4.78s/it]loss: 0.28117984533309937, grad_norm: 0.28703704476356506\n",
            "Training:  38% 87/226 [06:56<10:43,  4.63s/it]loss: 0.18381960690021515, grad_norm: 0.11656828224658966\n",
            "Training:  39% 88/226 [07:01<11:02,  4.80s/it]loss: 0.10842307657003403, grad_norm: 0.08352891355752945\n",
            "Training:  39% 89/226 [07:10<14:09,  6.20s/it]loss: 0.1733800619840622, grad_norm: 0.13260601460933685\n",
            "Training:  40% 90/226 [07:17<14:17,  6.31s/it]loss: 0.1500316858291626, grad_norm: 0.1512814313173294\n",
            "Training:  40% 91/226 [07:23<13:49,  6.14s/it]loss: 0.15481047332286835, grad_norm: 0.12887369096279144\n",
            "Training:  41% 92/226 [07:28<13:14,  5.93s/it]loss: 0.1755339652299881, grad_norm: 0.14086738228797913\n",
            "Training:  41% 93/226 [07:35<13:39,  6.16s/it]loss: 0.24188579618930817, grad_norm: 0.31399303674697876\n",
            "Training:  42% 94/226 [07:38<11:24,  5.19s/it]loss: 0.17990311980247498, grad_norm: 0.18353068828582764\n",
            "Training:  42% 95/226 [07:42<10:34,  4.85s/it]loss: 0.15328200161457062, grad_norm: 0.12343338876962662\n",
            "Training:  42% 96/226 [07:49<12:10,  5.62s/it]loss: 0.3047352433204651, grad_norm: 0.2698346972465515\n",
            "Training:  43% 97/226 [07:56<12:38,  5.88s/it]loss: 0.25538307428359985, grad_norm: 0.21278885006904602\n",
            "Training:  43% 98/226 [07:58<10:12,  4.78s/it]loss: 0.14686010777950287, grad_norm: 0.12895508110523224\n",
            "Training:  44% 99/226 [08:03<10:11,  4.82s/it]loss: 0.21421203017234802, grad_norm: 0.20931802690029144\n",
            "Training:  44% 100/226 [08:06<09:08,  4.35s/it]loss: 0.17108318209648132, grad_norm: 0.24139049649238586\n",
            "Training:  45% 101/226 [08:09<08:17,  3.98s/it]loss: 0.15712222456932068, grad_norm: 0.12442401051521301\n",
            "Training:  45% 102/226 [08:15<09:19,  4.51s/it]loss: 0.16745290160179138, grad_norm: 0.1960199773311615\n",
            "Training:  46% 103/226 [08:19<08:41,  4.24s/it]loss: 0.14989222586154938, grad_norm: 0.1548299789428711\n",
            "Training:  46% 104/226 [08:24<09:04,  4.46s/it]loss: 0.2657191753387451, grad_norm: 0.2443239390850067\n",
            "Training:  46% 105/226 [08:26<07:45,  3.85s/it]loss: 0.09834276884794235, grad_norm: 0.20281299948692322\n",
            "Training:  47% 106/226 [08:35<11:02,  5.52s/it]loss: 0.23451201617717743, grad_norm: 0.23546729981899261\n",
            "Training:  47% 107/226 [08:45<13:19,  6.72s/it]loss: 0.1547563076019287, grad_norm: 0.10938213765621185\n",
            "Training:  48% 108/226 [08:50<12:26,  6.32s/it]loss: 0.2282692939043045, grad_norm: 0.16223002970218658\n",
            "Training:  48% 109/226 [08:54<11:02,  5.66s/it]loss: 0.11400104314088821, grad_norm: 0.07873184978961945\n",
            "Training:  49% 110/226 [09:03<12:47,  6.61s/it]loss: 0.16363172233104706, grad_norm: 0.10940252244472504\n",
            "Training:  49% 111/226 [09:10<12:40,  6.62s/it]loss: 0.11212342232465744, grad_norm: 0.09312146157026291\n",
            "Training:  50% 112/226 [09:17<12:52,  6.78s/it]loss: 0.186025008559227, grad_norm: 0.14255426824092865\n",
            "Training:  50% 113/226 [09:21<11:15,  5.97s/it]loss: 0.20861737430095673, grad_norm: 0.16081364452838898\n",
            "Training:  50% 114/226 [09:25<09:46,  5.24s/it]loss: 0.16425472497940063, grad_norm: 0.12545301020145416\n",
            "Training:  51% 115/226 [09:29<09:10,  4.96s/it]loss: 0.17430426180362701, grad_norm: 0.1370086818933487\n",
            "Training:  51% 116/226 [09:34<09:12,  5.02s/it]loss: 0.1696065217256546, grad_norm: 0.13767151534557343\n",
            "Training:  52% 117/226 [09:44<11:59,  6.60s/it]loss: 0.14271365106105804, grad_norm: 0.1376013159751892\n",
            "Training:  52% 118/226 [09:50<11:06,  6.17s/it]loss: 0.1396186202764511, grad_norm: 0.10491020232439041\n",
            "Training:  53% 119/226 [09:57<11:31,  6.46s/it]loss: 0.15184704959392548, grad_norm: 0.10328185558319092\n",
            "Training:  53% 120/226 [10:04<12:00,  6.79s/it]loss: 0.21020519733428955, grad_norm: 0.13983088731765747\n",
            "Training:  54% 121/226 [10:07<10:00,  5.72s/it]loss: 0.2034798115491867, grad_norm: 0.1815890520811081\n",
            "Training:  54% 122/226 [10:11<08:46,  5.06s/it]loss: 0.21827831864356995, grad_norm: 0.1494535207748413\n",
            "Training:  54% 123/226 [10:14<07:44,  4.51s/it]loss: 0.24838942289352417, grad_norm: 0.2276725322008133\n",
            "Training:  55% 124/226 [10:17<06:46,  3.98s/it]loss: 0.18629242479801178, grad_norm: 0.14825861155986786\n",
            "Training:  55% 125/226 [10:21<06:38,  3.95s/it]loss: 0.15234671533107758, grad_norm: 0.14138534665107727\n",
            "Training:  56% 126/226 [10:25<06:31,  3.91s/it]loss: 0.1578487753868103, grad_norm: 0.11239403486251831\n",
            "Training:  56% 127/226 [10:37<10:31,  6.38s/it]loss: 0.3337359130382538, grad_norm: 0.35557448863983154\n",
            "Training:  57% 128/226 [10:39<08:07,  4.97s/it]loss: 0.18589447438716888, grad_norm: 0.14512774348258972\n",
            "Training:  57% 129/226 [10:42<07:18,  4.52s/it]loss: 0.14528138935565948, grad_norm: 0.11873870342969894\n",
            "Training:  58% 130/226 [10:47<07:23,  4.62s/it]loss: 0.16939562559127808, grad_norm: 0.1343703269958496\n",
            "Training:  58% 131/226 [10:50<06:34,  4.15s/it]loss: 0.173043355345726, grad_norm: 0.13927103579044342\n",
            "Training:  58% 132/226 [10:54<06:43,  4.29s/it]loss: 0.3338761627674103, grad_norm: 0.39820441603660583\n",
            "Training:  59% 133/226 [10:56<05:27,  3.52s/it]loss: 0.17920951545238495, grad_norm: 0.140008345246315\n",
            "Training:  59% 134/226 [11:01<05:56,  3.87s/it]loss: 0.12775462865829468, grad_norm: 0.1052374541759491\n",
            "Training:  60% 135/226 [11:09<07:59,  5.27s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZLo-3ExUwlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}